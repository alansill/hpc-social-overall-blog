{
    "version": "https://jsonfeed.org/version/1",
    "title": "hpc.social - Aggregated Personal Blog",
    "home_page_url": "https://hpc.social/personal-blog/",
    "feed_url": "https://hpc.social/personal-blog/feed.json",
    "description": "Shared personal experiences and stories",
    "icon": "https://hpc.social/personal-blog/assets/images/apple-touch-icon.png",
    "favicon": "https://hpc.social/personal-blog/assets/images/favicon.png",
    "expired": false,
    
    "author":  {
        "name": "hpc.social",
        "url": null,
        "avatar": null
    },
    
"items": [
    
        {
            "id": "https://hpc.social/personal-blog/2022/adam-s-weekly-ish-update-2022-12-20/",
            "title": "Adam’s weekly (-ish) update, 2022-12-20",
            "summary": null,
            "content_text": "What&#8217;s newThe past few weeks have been on the intense side at work, so I completely lost track of the blog and haven&#8217;t had a chance to write much in that time. However, I&#8217;m now on a holiday break, and finally have time to sit down at a keyboard to write more than code and Slack messages.One of the highlights of the past few weeks was a trip to San Jose, and the NVIDIA headquarters. I changed teams at work back in July, transferring from a group that was closely integrated with product management, to a more straightforward engineering team which designs and builds new high-performance computing systems. This was the first chance I&#8217;ve had to meet up with other members of my new team in person, and it was a really wonderful experience to be in the same physical space as folks who were previously just images on my screen. I love working remotely, but it&#8217;s also great to be able to stand in front of a white board with someone and brainstorm, or get coffee and just have a chat with a coworker outside of a video call with an agenda.(Plus, we were all careful and managed to avoid catching COVID from each other! Which was a win on its own.)Now, for the next two weeks I&#8217;m off work, and planning to take some time to relax and spend time on projects that are harder to focus on during busy work weeks. Expect (maybe) less about computers in my blog and social feeds, and more about D&amp;D, baking, and tasty cocktails.What I&#8217;m reading, watching, and listening toI&#8217;ve been a bit too scattered to focus on actual books the past few weeks, but I did find time for a few interesting articles and podcasts. In particular,&#8220;Why Roman Egypt was such a strange province&#8221;, from Bret Devereaux: As usual from Devereaux, an accessible but extremely detailed discussion of why so much of what we know about the Roman empire is from Egyptian records, but why that also might not be representative of the broader empire.&#8220;Emoji as incident resolution tools&#8221;, from Will Gallego: A fun discussion of how using emoji as part of a team&#8217;s communication can add nuance and shared understanding during incident management, along with a discussion of the disadvantages and costs associated with the practice.&#8220;What does modern software architecture look like in 2022?&#8221;, from Bartosz Mikulski: A nice  article which discusses how service-oriented software architecture can often include an explicit expectation of change. For example, the architecture might include notes on an ongoing deprecation of a library, or might signpost the need to factor a new microservice out when overall system load gets high enough.The Brady Heywood podcast: Found via the Oxide and Friends podcast, the Brady Heywood podcast is a series on engineering disasters and their consequences from a forensic engineering firm. It&#8217;s mostly not being updated any more (with the podcasters moving on to a separate series on complexity science), but it has a deep back catalog of good episodes, and includes thoughtful discussions of human factors, safety engineering, and how organizational pressures become manifest in engineering artifacts.Recent recipesSmitten Kitchen&#8217;s Homemade Irish Cream: This is a recipe I make every year, and I often give away small bottles of it as holiday gifts. It&#8217;s really ridiculously tasty, much better than Baileys  or similar, and good either on its own or in hot chocolate.Smitten Kitchen&#8217;s Fairytale of New York: This is a really tasty whiskey cocktail, and the star of the show is a &#8220;winter warmth syrup&#8221; that substitutes in for simple syrup. The syrup is simply very tasty, and turns what&#8217;s effectively an OId Fashioned variant into a lovely holiday cocktail.Sparkling gingerbread from Yossy Arefi&#8217;s Snaking Cakes: This recipe takes a little more prep than most of Arefi&#8217;s &#8220;snacking cakes&#8221;, as it includes ginger three ways (ground, fresh, and crystallized), but it&#8217;s worth the few minutes of extra work.Pet photosI&#8217;m pretty sure these two want me to turn the fireplace on.Just Percy bullying the dog by stealing his bed.",
            "content_html": "<h2>What&#8217;s new</h2><p>The past few weeks have been on the intense side at work, so I completely lost track of the blog and haven&#8217;t had a chance to write much in that time. However, I&#8217;m now on a holiday break, and finally have time to sit down at a keyboard to write more than code and Slack messages.</p><p><span id=\"more-289\"></span></p><p>One of the highlights of the past few weeks was a trip to San Jose, and the NVIDIA headquarters. I changed teams at work back in July, transferring from a group that was closely integrated with product management, to a more straightforward engineering team which <a href=\"https://blogs.nvidia.com/blog/2020/08/14/making-selene-pandemic-ai/\">designs and builds new high-performance computing systems</a>. </p><p>This was the first chance I&#8217;ve had to meet up with other members of my new team in person, and it was a really wonderful experience to be in the same physical space as folks who were previously just images on my screen. I love working remotely, but it&#8217;s also great to be able to stand in front of a white board with someone and brainstorm, or get coffee and just have a chat with a coworker outside of a video call with an agenda.</p><p>(Plus, we were all careful and managed to avoid catching COVID from each other! Which was a win on its own.)</p><p>Now, for the next two weeks I&#8217;m off work, and planning to take some time to relax and spend time on projects that are harder to focus on during busy work weeks. Expect (maybe) less about computers in my blog and social feeds, and more about D&amp;D, baking, and tasty cocktails.</p><h2>What I&#8217;m reading, watching, and listening to</h2><p>I&#8217;ve been a bit too scattered to focus on actual books the past few weeks, but I did find time for a few interesting articles and podcasts. In particular,</p><ul><li><a href=\"https://acoup.blog/2022/12/02/collections-why-roman-egypt-was-such-a-strange-province/\">&#8220;Why Roman Egypt was such a strange province&#8221;</a>, from Bret Devereaux: As usual from Devereaux, an accessible but extremely detailed discussion of why so much of what we know about the Roman empire is from Egyptian records, but why that also might not be representative of the broader empire.</li><li><a href=\"https://willgallego.com/2022/12/18/emoji-as-incident-resolution-tools/\">&#8220;Emoji as incident resolution tools&#8221;</a>, from Will Gallego: A fun discussion of how using emoji as part of a team&#8217;s communication can add nuance and shared understanding during incident management, along with a discussion of the disadvantages and costs associated with the practice.</li><li><a href=\"https://www.mikulskibartosz.name/modern-software-architecture-in-2022/\">&#8220;What does modern software architecture look like in 2022?&#8221;</a>, from Bartosz Mikulski: A nice  article which discusses how service-oriented software architecture can often include an explicit expectation of change. For example, the architecture might include notes on an ongoing deprecation of a library, or might signpost the need to factor a new microservice out when overall system load gets high enough.</li><li><a href=\"https://www.bradyheywood.com.au/podcasts/\">The Brady Heywood podcast</a>: Found via the <a href=\"https://oxide.computer/podcasts/oxide-and-friends/1137359\">Oxide and Friends podcast</a>, the Brady Heywood podcast is a series on engineering disasters and their consequences from a forensic engineering firm. It&#8217;s mostly not being updated any more (with the podcasters moving on to a separate series on complexity science), but it has a deep back catalog of good episodes, and includes thoughtful discussions of human factors, safety engineering, and how organizational pressures become manifest in engineering artifacts.</li></ul><h2>Recent recipes</h2><ul><li><a href=\"https://smittenkitchen.com/2016/12/homemade-irish-cream/\">Smitten Kitchen&#8217;s Homemade Irish Cream</a>: This is a recipe I make every year, and I often give away small bottles of it as holiday gifts. It&#8217;s really ridiculously tasty, much better than Baileys  or similar, and good either on its own or in hot chocolate.</li><li><a href=\"https://smittenkitchen.com/2014/12/fairytale-of-new-york/\">Smitten Kitchen&#8217;s Fairytale of New York</a>: This is a really tasty whiskey cocktail, and the star of the show is a &#8220;winter warmth syrup&#8221; that substitutes in for simple syrup. The syrup is simply very tasty, and turns what&#8217;s effectively an OId Fashioned variant into a lovely holiday cocktail.</li><li>Sparkling gingerbread from <a href=\"http://www.apt2bbakingco.com/snacking-cakes\">Yossy Arefi&#8217;s Snaking Cakes</a>: This recipe takes a little more prep than most of Arefi&#8217;s &#8220;snacking cakes&#8221;, as it includes ginger three ways (ground, fresh, and crystallized), but it&#8217;s worth the few minutes of extra work.</li></ul><h2>Pet photos</h2><figure class=\"wp-block-image size-large is-resized\"><img alt=\"A white calico cat and a gray tabby cat lounging on a large brown pet bed in front of a gas fireplace.\" class=\"wp-image-295\" height=\"512\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_7207-768x1024.jpeg\" width=\"384\" /><figcaption class=\"wp-element-caption\">I&#8217;m pretty sure these two want me to turn the fireplace on.</figcaption></figure><figure class=\"wp-block-image size-large is-resized\"><img alt=\"A gray tabby cat lounges on a dog bed, while a golden doodle lays on the floor nearby and looks forlornly at the bed.\" class=\"wp-image-294\" height=\"512\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_1725-1024x1024.jpeg\" width=\"512\" /><figcaption class=\"wp-element-caption\">Just Percy bullying the dog by stealing his bed.</figcaption></figure>",
            "url": "https://hpc.social/personal-blog/2022/adam-s-weekly-ish-update-2022-12-20/",
            
            
            
            
            
            "date_published": "2022-12-20T18:14:52-07:00",
            "date_modified": "2022-12-20T18:14:52-07:00",
            
                "author": "Thinking Out Loud"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/adam-s-weekly-update-2022-12-04/",
            "title": "Adam’s weekly update, 2022-12-04",
            "summary": null,
            "content_text": "What&#8217;s newThis week was really intense from a work perspective. Not &#8220;bad intense&#8221;, but the kind of week where every day was spent with such a level of focus, that at 5 PM or so I found myself staring off into space and forgetting words. I think I got some good things accomplished, but my brain also felt like mush by the time the weekend came.This week I&#8217;m traveling to San Jose for work (I just checked into my hotel a little while ago!), so I fully expect this week to also be eaten by work. So I don&#8217;t promise anything terribly interesting for next week&#8217;s post&#8230;However, I did take advantage of a Sunday in San Jose to visit the Computer History Museum in Mountain View! I try to visit the museum every few years, and while a lot of the exhibits are the same, enough things change that I always get something new from the visit. Also, I&#8217;ve been doing a lot of reading about hardware development and the history thereof lately, so it was interesting to examine the museum through that new lens.I may write more about my visit later this week &#8212; it definitely sparked some thoughts &#8212; but in the mean time, here are a few photos I took while wandering around the museum.The Babbage Difference Engine, and other mechanical computers, have always fascinated me.Can&#8217;t visit the museum without visiting the Cray-1.I would have loved to have seen a CM-1 in operation, with its red LEDs showing the operation of its many single-bit CPUs.Having recently read Charles Petzold&#8217;s &#8220;Code&#8221;, I was struck by how closely the front panel of the Altair 8800 resembles the fictional front panel of the computer that Petzold constructs from logic gates up.The CHM Learning Lab now includes a back room with a couple of Dell PowerEdge R710 servers, complete with instructions for how to disassemble and reassemble them. Anyone who wants can wander in and take them apart. It was great fun watching a 5-year-old kid pulling components out of one of these&#8230; As well as feeling a little weird, as I think I&#8217;ve run these in production!What I&#8217;m readingI don&#8217;t have a ton to share this week &#8212; honestly, the whole week feels like a blur &#8212; but here are two books that I recommend.The Red Scholar&#8217;s Wake, by Aliette de Bodard: As the blurb says, &#8220;Lesbian space pirates!&#8221; Also, a really wonderful novella about building a new relationship amidst grief, power differentials, politics, and space battles. I think I basically recommend everything that de Bodard writes, but especially this. And it basically stands alone! So you can read this first, without going back to the other stories in the same world.Dealers of Lightning: XEROX PARC and the Dawn of the Computer Age, by Michael Hiltzik: I&#8217;ve just started this, but it&#8217;s already a really interesting snapshot of a key period in the development of the personal computer.Recent recipesSmitten Kitchen&#8217;s Unfussy Sugar Cookies: These cookies did, indeed, prove to be both tasty and easy to make. If you just want some easy cookies to snack on, I absolutely recommend this recipe.Pet photos",
            "content_html": "<h2>What&#8217;s new</h2><p>This week was really intense from a work perspective. Not &#8220;bad intense&#8221;, but the kind of week where every day was spent with such a level of focus, that at 5 PM or so I found myself staring off into space and forgetting words. I think I got some good things accomplished, but my brain also felt like mush by the time the weekend came.</p><p><span id=\"more-268\"></span></p><p>This week I&#8217;m traveling to San Jose for work (I just checked into my hotel a little while ago!), so I fully expect this week to also be eaten by work. So I don&#8217;t promise anything terribly interesting for next week&#8217;s post&#8230;</p><p>However, I did take advantage of a Sunday in San Jose to visit the <a href=\"https://computerhistory.org/\">Computer History Museum</a> in Mountain View! I try to visit the museum every few years, and while a lot of the exhibits are the same, enough things change that I always get something new from the visit. Also, I&#8217;ve been doing a lot of reading about hardware development and the history thereof lately, so it was interesting to examine the museum through that new lens.</p><p>I may write more about my visit later this week &#8212; it definitely sparked some thoughts &#8212; but in the mean time, here are a few photos I took while wandering around the museum.</p><figure class=\"wp-block-image size-large is-resized\"><img alt=\"A mechanical computer built mostly of brass, with various numerical dials. A small placard labels this as a replica of the Babbage Difference Engine No. 1 Demonstration Piece.\" class=\"wp-image-282\" height=\"800\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_6894-768x1024.jpg\" width=\"600\" /><figcaption class=\"wp-element-caption\">The Babbage Difference Engine, and other mechanical computers, have always fascinated me.</figcaption></figure><figure class=\"wp-block-image size-large is-resized\"><img alt=\"The Cray-1, a round computer with its own built-in seating attached.\" class=\"wp-image-283\" height=\"446\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_6965-1024x768.jpg\" width=\"595\" /><figcaption class=\"wp-element-caption\">Can&#8217;t visit the museum without visiting the Cray-1.</figcaption></figure><figure class=\"wp-block-image size-large is-resized\"><img alt=\"The Connection Machine 1, a large black cube divided in eight sections.\" class=\"wp-image-284\" height=\"768\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_6973-768x1024.jpg\" width=\"576\" /><figcaption class=\"wp-element-caption\">I would have loved to have seen a CM-1 in operation, with its red LEDs showing the operation of its many single-bit CPUs.</figcaption></figure><figure class=\"wp-block-image size-large is-resized\"><img alt=\"The front panel of an Altair 8800 computer, with an array of LEDs and switches controlling the state of individual bits.\" class=\"wp-image-285\" height=\"449\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_7037-1024x768.jpg\" width=\"598\" /><figcaption class=\"wp-element-caption\">Having recently read Charles Petzold&#8217;s &#8220;Code&#8221;, I was struck by how closely the front panel of the Altair 8800 resembles the fictional front panel of the computer that Petzold constructs from logic gates up.</figcaption></figure><figure class=\"wp-block-image size-large is-resized\"><img alt=\"A Dell PowerEdge R710 lays on a white plastic table, top cover off, surrounded by instructions on how to disassemble it.\" class=\"wp-image-286\" height=\"467\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_7073-1024x768.jpg\" width=\"623\" /><figcaption class=\"wp-element-caption\">The CHM Learning Lab now includes a back room with a couple of Dell PowerEdge R710 servers, complete with instructions for how to disassemble and reassemble them. Anyone who wants can wander in and take them apart. It was great fun watching a 5-year-old kid pulling components out of one of these&#8230; As well as feeling a little weird, as I think I&#8217;ve run these in production!</figcaption></figure><h2>What I&#8217;m reading</h2><p>I don&#8217;t have a ton to share this week &#8212; honestly, the whole week feels like a blur &#8212; but here are two books that I recommend.</p><ul><li><a href=\"https://www.aliettedebodard.com/bibliography/novels/the-universe-of-xuya/the-red-scholars-wake/\">The Red Scholar&#8217;s Wake, by Aliette de Bodard</a>: As the blurb says, &#8220;Lesbian space pirates!&#8221; Also, a really wonderful novella about building a new relationship amidst grief, power differentials, politics, and space battles. I think I basically recommend everything that de Bodard writes, but especially this. And it basically stands alone! So you can read this first, without going back to the other stories in the same world.</li><li><a href=\"https://www.harpercollins.com/products/dealers-of-lightning-michael-a-hiltzik?variant=40824247779362\">Dealers of Lightning: XEROX PARC and the Dawn of the Computer Age, by Michael Hiltzik</a>: I&#8217;ve just started this, but it&#8217;s already a really interesting snapshot of a key period in the development of the personal computer.</li></ul><h2>Recent recipes</h2><ul><li><a href=\"https://smittenkitchen.com/2019/12/unfussy-sugar-cookies/\">Smitten Kitchen&#8217;s Unfussy Sugar Cookies</a>: These cookies did, indeed, prove to be both tasty and easy to make. If you just want some easy cookies to snack on, I absolutely recommend this recipe.</li></ul><h2>Pet photos</h2><figure class=\"wp-block-image size-large is-resized\"><img alt=\"Phyrne the calico cat stares down into the camera from a stairway\" class=\"wp-image-279\" height=\"414\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_6881-768x1024.jpg\" width=\"310\" /></figure><figure class=\"wp-block-image size-large is-resized\"><img alt=\"Close-up on the face of Percy the gray tabby cat\" class=\"wp-image-280\" height=\"420\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_6879-768x1024.jpg\" width=\"314\" /></figure><figure class=\"wp-block-image size-large is-resized\"><img alt=\"Benny the golden doodle curled up on a dog bed\" class=\"wp-image-281\" height=\"238\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_6876-1024x768.jpg\" width=\"317\" /></figure>",
            "url": "https://hpc.social/personal-blog/2022/adam-s-weekly-update-2022-12-04/",
            
            
            
            
            
            "date_published": "2022-12-05T05:49:35-07:00",
            "date_modified": "2022-12-05T05:49:35-07:00",
            
                "author": "Thinking Out Loud"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/an-initial-look-at-deep-learning-io-performance/",
            "title": "An Initial Look at Deep Learning IO Performance",
            "summary": null,
            "content_text": "AbstractThis blog post describes an investigation of IO behavior of TensorFlow and PyTorch during resnet50 training running on Lambda Lab’s 8x V100 GPU instances.  Both ephemeral local NVMe storage and network attached persistent storage was tested.  The local NVMe storage was fast enough to achieve a throughput rate required to hit synthetic test targets.  The network attached persistent storage may not be able to fully saturate 8 V100 GPUs during training, though can achieve nearly the same level of performance as the local storage so long as TFRecords are utilized.  Further, there are specific behaviors and bottlenecks in TensorFlow and PyTorch that can reduce training performance when using real data from ImageNet.AcknowledgementsThank you to Michael Balaban at Lambda Labs for providing access to their GPU cloud for this testing.  Thank you to Chuan Li for the creation of his TensorFlow benchmarking tools.  Thank you also to Andrej Karpathy, Toby Boyd, Yanan Cao, Sanjoy Das, Thomas Joerg, and Justin Lebar for their excellent blog posts on deep learning and XLA performance that helped inform this article.  I hope that this post will be useful for others as your work and writing was useful for me.Introduction  …just because you can formulate your problem as RL doesn’t mean you should. If you insist on using the technology without understanding how it works you are likely to fail.          Andrej Karpathy, A Recipe for Training Neural Networks, 2019That was the phrase that stuck in my head when I first started this project.   What project you may ask?  I want to understand how deep learning experiments utilize fast storage devices.  Not just any experiments either: real ones, preferably big.  That’s how I happened upon Andrej Karpathy’s blog.  He is the former Sr. Director of AI at Tesla and knows a thing or two about training big neural networks.  I’ve spent the last decade working on Ceph and have worked on distributed systems and distributed storage for nearly 2 decades at this point.  But training neural nets?  The closest I’ve come was back in the early 2000s when I tried to build a tool to predict video game framerates.  I scraped benchmark numbers from review websites and built M5 decision trees based on hardware and video card settings.  It sort of worked, but was terribly overtrained on a small (~4000 sample) dataset.  Training with petabytes of data to teach an AI how to responsibly drive a car?  I can already feel a bit of imposter syndrome setting in.Thankfully my goal is comparatively modest.  I don’t need to build a cutting edge classifier or explore the intricacies of manually implementing back-propagation.  I simply want to understand the IO patterns that are involved when training big datasets with fast GPUs so I can help researchers speed up their work.  Up until now, my ability to do this was fairly limited.  At the day job I’ve had access to a small group of nodes with extremely modest GPUs.  I set up runs with MLPerf but the datasets (WMT G-E and CoCo) easily fit into memory. Other than a short burst of read traffic at the very beginning of training there was very little IO.  Recently I had the opportunity to meet Michael Balaban, Co-Founder of Lambda Labs.  I told him what I wanted to do and he gave me access to Lambda’s GPU cloud and beta persistent storage to give it a try.  I was able to grab one of Lambda’s 8x Tesla V100 instances (These things are incredibly popular so it’s best to grab one early in the morning!).  Not all of Lambda’s instance types currently have access to the persistent storage but the V100 instances in the Texas zone do.  Once secured, I got to work.TensorFlow - SyntheticBefore even attempting to run tests with real data, I realized I needed a baseline to start with.  Luckily, Chuan Li, Lambda’s Chief Scientific Officer, wrote a tool for running TensorFlow benchmarks and made it available on github here. One of the advantages of Lambda’s cloud is that they’ve already bundled up many popular tools for running deep-learning workloads into one package called Lambda Stack which comes pre-installed when you start an instance.  This made it fast to get started, though I did run into one issue.  Lambda Stack comes standard with TensorFlow 2, but Chuan Li’s tool relies on a TensorFlow benchmark submodule that is designed to work with TensorFlow 1.  Luckily, the parent repository was unofficially updated to work with Tensorflow 2 (with a warning that it is no longer being maintained).  A quick “git checkout master” in the “benchmarks” submodule directory got everything working.  Chuan Li’s tool makes it simple to run tests with several preconfigured templates already included.  I chose the fp16 resnet50 configuration as it should be fast at processing images and is fairly standard.TF_XLA_FLAGS=--tf_xla_auto_jit=2 ./batch_benchmark.sh X X 1 100 2 config/config_resnet50_replicated_fp16_train_synUsing the invocation provided in the benchmark README.md file, I was able to quickly run benchmarks with synthetic data on up to 8 V100 GPUs in the node.  At one point I got stuck, hitting what appeared at first to be an unexplainable 25% performance loss. I reran the tests multiple times and even monitored GPU clockspeeds/temperatures in nvidia-smi with no luck.  Ultimately I discovered my error.  In the slow cases, I had inadvertently left out the “TF_XLA_FLAGS=–tf_xla_auto_jit=2” environment variable.  It turns out that setting this allows Tensorflow compile and execute functions with XLA (Accelerated Linear Algebra) support which is a pretty big win for these tests.At this point I decided that I needed to understand how Chuan Li’s tool works.  It turns out that he is using the same base tf_cnn_benchmarks.py benchmark code that companies like Nvidia and Dell also use for benchmarking their GPU solutions.  I spent some time running it directly with Dell’s settings from their deep learning overview here.  Unfortunately those tests had mixed results, even after various tweaks.  While researching the XLA issues I mentioned earlier however, I made an even better discovery on the TensorFlow website.  I found an excellent blog post with performance data written by some of the core Tensorflow developers.  It’s now 4 years old, but still appears to be quite valid.  The tuning options used were both simpler and resulted in higher performance versus other configurations that I’ve come across.Training with synthetic data in Lambda’s cloud resulted in similar performance to what the Tensorflow developer’s reported.  In fact, using their own settings yielded slightly faster results when running on Lambda’s 8xV100 instance!  It was incredibly encouraging to me that even in Lambda’s cloud environment with virtual machine instances I could achieve performance that was as fast or faster than what the Tensorflow developers were reporting.Choosing a Real Data Set  The first step to training a neural net is to not touch any neural net code at all and instead begin by thoroughly inspecting your data.          Andrej Karpathy, A Recipe for Training Neural Networks, 2019Having convinced myself that I had Tensorflow operating reasonably efficiently in synthetic tests, it was time to start thinking about what dataset to use for “real” training.  The largest and most obvious choice is ImageNet.  ImageNet is composed of over 1.2 million categorized images that form a roughly 160GB training dataset.  It is also the largest dataset I could find that was publicly accessible. Downloading it isn’t so easy however. The only version that I could access is the ImageNet Object Localization Challenge dataset hosted on kaggle.After finally figuring out how to download the data, it was time to follow Andrej’s advice and try to learn something about it.  While ImageNet is curated and annotated, it has many images of different sizes, dimensions, and pixel counts.  Images also come from many sources with different levels of quality.  Through the power of stack-exchange I was able to find a bash one-liner script to generate a histogram of image sizes:find . -type f -print0 | xargs -0 ls -l | awk '{size[int(log($5)/log(2))]++}END{for (i in size) printf(\"%10d %3d\\n\", 2^i, size[i])}' | sort -nRoughly 80% of the images are in the 64KB or 128KB size bins. Almost all of the remaining images are smaller.  That gives us a pretty good idea of what kind of IOs to expect during classification.  Or at least…it does for frameworks that read those images directly.  In Tensorflow’s case, there’s an alternative format called TFRecord.  TFRecords are basically collections of image data sequentially laid out in much larger files.  Instead of iterating over thousands or millions of individual image files, TFRecords allow Tensorflow to instead stream fewer, larger files that each house multiple images.  It’s a one time cost to pre-process the data so Tensorflow has less work to do during training.  After I downloaded the ImageNet data I took a shot at converting the ImageNet LOC data into TensorFlow records.  Luckily, the TensorFlow tpu github repository already has a tool that can do this.  I had to manipulate the dataset slightly, but ultimately this process worked (at least for the training data):pip install gcloud google-cloud-storagepip install protobuf==3.20.1mkdir ~/data/ImageNetFooln -s ~/data/ImageNet/ILSVRC/Data/CLS-LOC/train ~/data/ImageNetFoo/trainln -s ~/data/ImageNet/ILSVRC/Data/CLS-LOC/val ~/data/ImageNetFoo/valln -s ~/data/ImageNet/ILSVRC/Data/CLS-LOC/test ~/data/ImageNetFoo/testln -s ~/data/ImageNet/LOC_synset_mapping.txt ~/data/ImageNetFoo/synset_labels.txtpython imagenet_to_gcs.py --raw_data_dir=/home/ubuntu/data/ImageNetFoo --local_scratch_dir=/home/ubuntu/ExaltedOrbs/ImageNet/tf_records --nogcs_uploadPerhaps I should say that this worked so long as the original dataset was located on the local NVMe drive.  The persistent storage didn’t fare as well.  Attempting to decompress ImageNet on the persistent storage resulted in blowing past the max number of open files allowed with errors like:OSError: [Errno 24] Too many open files.Unfortunately this couldn’t be fixed on the instance.  It appeared to be passed through from the host and the persistent storage was completely unusable until the instance was rebooted.  Recently I spoke to one of Lambda’s engineers and they are working on a fix. (It may already be implemented by the time you read this!)  I also want to note that the persistent storage is still in beta so issues like this are not entirely unexpected.  Having said that, before hitting the error it was significantly slower extracting ImageNet on the persistent storage vs on the local NVMe storage.  It’s probably best to extract ImageNet locally and then write the large TFRecords to the persistent storage during the conversion process.  Luckily extracting ImageNet to local storage was fine, and storing the original archive and the resulting TFRecords on the persistent storage worked perfectly fine as well.FIO - Baseline IO ResultsNext, I turned my attention to running baseline tests on Lambda’s local and persistent storage using fio.  Fio is a highly configurable and well respected benchmark in the storage community and perfect for generating baseline results.  I decided to use a dataset size that is roughly similar to ImageNet (200GB), the libaio engine in fio with direct IO, and an appropriately high IO depth to let the NVMe drives stretch their legs a bit.Throughput with the local NVMe drive(s) is surprisingly good.  The persistent storage is slower, but still might be fast enough at a little over 1GB/s for large reads.  16K IOPS was somewhat slower in both cases.  I chose 16K so that I could quickly compare to tests I ran in my Ceph QEMU/KVM performance blog post here.  Without getting into the details, I suspect there’s still some room for improved IOPS with Lambda’s setup.  Luckily though, converting into TFRecords should make Tensorflow throughput bound instead of latency bound.  What about PyTorch or other tools that want to read images directly though?  Fio gives us the ability to simulate it by using its ‘bssplit’ feature.  We can take the size ranges and percentiles generated when examining ImageNet and give fio a similar distribution:fio --ioengine=libaio --direct=1 --bssplit=2K/1:4K/2:8K/4:16K/8:32K/13:64K/38:128K/33:256K/1 --iodepth=128 --rw=randread --norandommap --size=200G --numjobs=1 --runtime=300 --time_based --name=fooThis isn’t exactly right as we are not reading data spread across millions of files, but it should provide something of an upper bound on what to expect.  It looks like the persistent storage can do approximately 10K reads/second at a throughput rate of around 750MB/s.  The local storage is about 3-4 times faster.  Local storage should be fast enough to support the kind of images/second throughput rates we want to hit in Tensorflow on 8 V100 GPUs, but the jury is still out for the persistent storage.Tensorflow - ImageNetRunning benchmarks with real data rather than synthetic data is fairly straightforward in Tensorflow.  You simply append data_dir and data_name flags to the CLI invocation to let it know where the TFRecords are located:sync; echo 3 | sudo tee /proc/sys/vm/drop_cachespython ./tf_cnn_benchmarks.py --batch_size=256 --num_batches=100 --model=resnet50 --optimizer=momentum --variable_update=replicated --all_reduce_spec=nccl --use_fp16=True --nodistortions --gradient_repacking=2 --compute_lr_on_cpu=True --single_l2_loss_op=True --xla_compile=True --num_gpus=8 --loss_type_to_report=base_loss --data_dir=/home/ubuntu/ImageNet-TF/train --data_name=imagenetOuch.  Much lower performance with the ImageNet data vs synthetic!  This is especially unfortunate given that 4 years ago the Tensorflow developers reported much better results.  I spent some time reading and experimenting with different settings.  Ultimately the one setting that made a substantial difference was “datasets_num_private_threads”.  In the Tensorflow benchmark source code, this setting is described as: “[The] number of threads for a private threadpool created for all datasets computation.”  I’ll go into more detail what these threads are doing in a bit. For now, let’s see how increasing the number of threads affects the results:Increasing the number of private threads has a dramatic effect on performance, though I was unable to fully match the performance achieved in the synthetic tests on either the local or persistent storage.  The local storage fared better at high thread counts gradually topping out at around 8600 images/second.  At high private thread counts the persistent storage topped out between 7000-8000 images/second with a higher degree of variability between runs.  I suspect that in this case the persistent storage has likely hit its (per instance) limit.In addition to having a dramatic effect on performance, changing the private thread count also had a large effect on the CPU consumption of the TensorFlow process.  CPU usage increases almost linearly with additional private threads up to around 30 cores.  What exactly are these private threads doing?  To answer that question, I utilized two tools that I often deploy when diagnosing CPU usage in Ceph.  When testing with a lower number of private threads, I used linux’s perf tool to look at where cycles are being consumed when the private threads are fully saturated.  At higher levels of private threads, I used my wallclock profiler uwpmp to look at how private threads spend their time when increasing the thread count no longer improves performance.In the first case with perf, we can get a good view of the work that these private threads are doing:--77.31%--tensorflow::ThreadPoolDevice::Compute          |                    |--51.19%--0x7f511a00c7d8          |          |                    |           --51.18%--tensorflow::jpeg::Uncompress          |--14.48%--tensorflow::ResizeBilinearOp&lt;Eigen::ThreadPoolDevice, unsigned char&gt;::Compute          |--5.47%--tensorflow::CastOpBase::Compute          |--2.66%--tensorflow::ReverseV2Op&lt;Eigen::ThreadPoolDevice, unsigned char, int&gt;::ComputeThe majority of the cycles consumed is in jpeg decompression and resize operations, along with a smattering of other stuff.  What happens if we look at a case with a higher private thread count but now look at wallclock time instead of cycles?  I ended up having some trouble getting the profiler to work properly and consistently get clean callgraphs, but I was able to get at least one run in that revealed some interesting information.  First, I saw time spent in the same functions that perf told us we were spending cycles in:+ 100.00% Eigen::ThreadPoolTempl&lt;tensorflow::thread::EigenEnvironment&gt;::WorkerLoop(int) + 99.90% ??? |+ 97.30% ??? ||+ 92.40% ??? |||+ 77.10% _PyEval_EvalFrameDefault ||||+ 47.20% ??? |||||+ 38.10% tensorflow::jpeg::Uncompress(void const*, int, tensorflow::jpeg::UncompressFlags const&amp;, long*, std::function&lt;unsigned char* (int, int, int)&gt;) ||||+ 12.20% tensorflow::ResizeBilinearOp&lt;Eigen::ThreadPoolDevice, unsigned char&gt;::Compute(tensorflow::OpKernelContext*) ||||+ 4.40% tensorflow::CastOpBase::Compute(tensorflow::OpKernelContext*) ||||+ 1.70% tensorflow::ReverseV2Op&lt;Eigen::ThreadPoolDevice, unsigned char, int&gt;::Compute(tensorflow::OpKernelContext*)But the wallclock profile also exposed that there may be contention in multiple areas in the private threads around some of the nsync synchronization primitives being used: |||||||    |  + 4.50% nsync::nsync_mu_semaphore_p(nsync::nsync_semaphore_s_*) |||||||    |   + 4.50% syscallThis almost always appeared nested deep inside:tensorflow::BFCAllocator::AllocateRaw(unsigned long, unsigned long, tensorflow::AllocationAttributes const&amp;)Sadly I was missing a number of debug symbols and don’t 100% trust the wallclock trace.  For now I’ll just say that the private threads are doing a significant amount of work decompressing and manipulating the image data to keep the GPUs fed.  I suspect that with newer and faster GPUs the image retrieval pipeline could become an even bigger issue when training with real image data.  The mystery for me is how The TensorFlow developers achieved such good results 4 years ago without using dedicated private threads at all.  Perhaps they had a significantly faster jpeg decompression mechanism that I am unaware of?PyTorch - ImageNetAfter running Tensorflow, I also ran some benchmarks in PyTorch using Nvidia’s “DeepLearningExamples” github repo.  First, I installed the prereqs and setup the repository:pip install 'git+https://github.com/NVIDIA/dllogger'pip install --extra-index-url https://developer.download.nvidia.com/compute/redist --upgrade nvidia-dali-cuda110git clone https://github.com/NVIDIA/DeepLearningExamplesThen, prepared ImageNet for usage in PyTorch:cd ~/data/ImageNet/ILSVRC/Data/CLS-LOC/valwget -qO- https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh | bashAnd finally ran a test:cd DeepLearningExamples/PyTorch/Classification/ConvNetssync; echo 3 | sudo tee /proc/sys/vm/drop_cachespython ./multiproc.py --nproc_per_node 1 ./main.py --arch resnet50 --label-smoothing 0.1 --run-epoch 1 --amp --static-loss-scale 256 --workspace /home/ubuntu/data/ImageNet-Scratch /home/ubuntu/data/ImageNet-Orig/ILSVRC/Data/CLS-LOC/There are a couple of differences here versus the TensorFlow tests.  First, I’m using the raw ImageNet archive instead of a preprocessed TFRecord dataset, so the read behavior is different.  Because I was unable to extract or copy the raw ImageNet archive onto the persistent storage, I’m also only testing the local NVMe drive.  Finally, I didn’t see any specific examples for running with fp16 in nVidia’s documentation, so I’m using amp (automatic mixed precision) which may be slightly slower.Given the number of differences it’s tough to draw direct comparisons with Tensorflow.  Amp is one difference, but it’s quite possible that there are tuning options that could improve performance here that I don’t know about.  I did notice that PyTorch, like Tensorflow, is using quite a bit of CPU to keep the GPUs working.  I suspect that there are ways to tweak the IO pipeline that could improve performance.  For now though, let’s compare the IO patterns on the local NVMe drive during the Tensorflow and PyTorch runs.  I was hoping to be able to use blktrace to do this, but unfortunately was unable to get any data from the virtual devices in the instance.  I was able to collect more general statistics using collectl however.Disk Read Statistics During PyTorch 8 GPU run:            Time      Name      KBytes      Merged      IOs      Size      Wait      QLen      SvcTim                  00:29:18      vda      761136      0      6746      113      58      431      0              00:29:19      vda      752172      0      6648      113      112      810      0              00:29:20      vda      747824      0      6595      113      84      604      0              00:29:21      vda      735964      0      6583      112      73      551      0              00:29:22      vda      695636      0      6237      112      102      760      0      Disk Read Statistics During TensorFlow 8 GPU run:            Time      Name      KBytes      Merged      IOs      Size      Wait      QLen      SvcTim                  00:38:45      vda      1081324      0      8440      128      0      7      0              00:38:46      vda      927512      0      7241      128      0      7      0              00:38:47      vda      913512      0      7130      128      0      7      0              00:38:48      vda      1047444      0      8186      128      0      6      0              00:38:49      vda      968776      0      7560      128      0      6      0      When just looking at the IO sizes, both runs appear similar, but that doesn’t tell the whole story.  It is likely that Tensorflow is doing much larger reads that are broken up into contiguous 128KB chunks by the block layer based on the underlying device’s max_sectors_kb setting.  The tells here are the very low queue length and wait times for the TensorFlow run versus the PyTorch run.  In both case the device service times are low (0), but in the TensorFlow case IOs are still backing up in the device queue.Interestingly, it appears that it may be possible to use nVidia’s DALI (Data Loading Library) package to read TFRecords into PyTorch.  I didn’t have time to attempt it, but potentially that could have a big effect on IO behavior and performance as well.ConclusionAs I’ve been writing this post, I realize just how complicated it is to understand the performance characteristics of training of neural networks.  Even as we talk about metrics like images/second, the options that are used (batch size for instance) can also affect convergence.  It’s very difficult to come up with a common methodology that is always better than others.  I wonder if another metric, like reaching a desired level of convergence, would be better in the end.  Having said that, I am glad for having done this exercise as I learned some valuable things:      Pre-processing data into a format like TFRecords on fast local storage is a big win from an IO perspective.  It lets storage systems that have slow metadata performance succeed so long as they have enough sequential read throughput to keep the machine learning framework busy.  This is a big win for many distributed file systems that may have substandard metadata performance (and even the good ones may still benefit).        To train on a dataset like ImageNet, you need somewhere around 1-1.3GB/s of raw disk throughput to keep 8 V100 GPUs busy when training in fp16.  For amp or fp32 the requirements are likely lower since the GPUs can’t work quite as fast.  With modern GPUs that are faster than the V100, the disk throughput requirements could be significantly higher.        Lambda’s local NVMe storage is likely fast enough to saturate 8 GPUs, even newer ones, so long as the rest of the IO path can keep up.  The persistent storage appears to become a bottleneck with sufficient GPUs and TensorFlow private threads, though can still function fairly well so long as TFRecords are used.  A concern going forward is how to ensure that the data pipeline in TensorFlow and PyTorch are fast enough to keep the GPUs fed.  The Tensorflow benchmark required a large number of private threads and showed potential evidence of contention at high thread counts.  PyTorch did not appear to natively support TFRecords, but NVidia DALI or other 3rd party code might help improve the IO path.        If it’s necessary to train directly with images rather than TFRecords, it may not make sense to host them on shared file systems.  It appears that Tensorflow and possibly PyTorch give users the ability to specify a separate training data and work directory.  If all operations against the training data are reads, it may be better to host datasets on read-only block device snapshots. For instance with Ceph, perhaps you could create a read/write RBD volume where you put a certain dataset, take a snapshot, and then map that snapshot as read only on multiple instances that all need access to the same image set.        Even with a training set as large as ImageNet, Lambda’s instances have so much memory that eventually the entire dataset becomes cached.  It was necessary to sync and drop caches before each test and keep tests short enough that they didn’t re-read the same data from buffer cache.  I was able to watch as long running tests eventually stopped performing reads and got faster as time went on.  This could make apples-to-apples comparison between different storage vendors difficult if not carefully controlled.        I’m almost certainly missing additional tweaks that can help speed up both Tensorflow and PyTorch.  This post shouldn’t be seen as the be-all/end-all for how to achieve high performance with these frameworks, but I hope it may at least help showcase some of the areas that are valuable to investigate when trying to train with real data and achieve high performance.  This wraps up my initial work looking at Deep Learning IO behavior.  I hope that next time I can come armed with a bit more knowledge about the internals of how PyTorch and Tensorflow work, focus a bit more on the quality of the training, find even larger datasets to work with, and maybe actually accomplish something useful rather than just play with ImageNet.Thanks for reading!",
            "content_html": "<h2 id=\"abstract\">Abstract</h2><p>This blog post describes an investigation of IO behavior of TensorFlow and PyTorch during resnet50 training running on Lambda Lab’s 8x V100 GPU instances.  Both ephemeral local NVMe storage and network attached persistent storage was tested.  The local NVMe storage was fast enough to achieve a throughput rate required to hit synthetic test targets.  The network attached persistent storage may not be able to fully saturate 8 V100 GPUs during training, though can achieve nearly the same level of performance as the local storage so long as TFRecords are utilized.  Further, there are specific behaviors and bottlenecks in TensorFlow and PyTorch that can reduce training performance when using real data from ImageNet.</p><h2 id=\"acknowledgements\">Acknowledgements</h2><p>Thank you to Michael Balaban at Lambda Labs for providing access to their GPU cloud for this testing.  Thank you to Chuan Li for the creation of his TensorFlow benchmarking tools.  Thank you also to Andrej Karpathy, Toby Boyd, Yanan Cao, Sanjoy Das, Thomas Joerg, and Justin Lebar for their excellent blog posts on deep learning and XLA performance that helped inform this article.  I hope that this post will be useful for others as your work and writing was useful for me.</p><h2 id=\"introduction\">Introduction</h2><blockquote>  <p><em>…just because you can formulate your problem as RL doesn’t mean you should. If you insist on using the technology without understanding how it works you are likely to fail.</em></p>  <p>        Andrej Karpathy, <a href=\"https://karpathy.github.io/2019/04/25/recipe/\">A Recipe for Training Neural Networks</a>, 2019</p></blockquote><p>That was the phrase that stuck in my head when I first started this project.   What project you may ask?  I want to understand how deep learning experiments utilize fast storage devices.  Not just any experiments either: <em>real</em> ones, preferably big.  That’s how I happened upon Andrej Karpathy’s blog.  He is the former Sr. Director of AI at Tesla and knows a thing or two about training big neural networks.  I’ve spent the last decade working on Ceph and have worked on distributed systems and distributed storage for nearly 2 decades at this point.  But training neural nets?  The closest I’ve come was back in the early 2000s when I tried to build a tool to predict video game framerates.  I scraped benchmark numbers from review websites and built M5 decision trees based on hardware and video card settings.  It sort of worked, but was terribly overtrained on a small (~4000 sample) dataset.  Training with petabytes of data to teach an AI how to responsibly drive a car?  I can already feel a bit of imposter syndrome setting in.</p><p>Thankfully my goal is comparatively modest.  I don’t need to build a cutting edge classifier or explore the intricacies of manually implementing back-propagation.  I simply want to understand the IO patterns that are involved when training big datasets with fast GPUs so I can help researchers speed up their work.  Up until now, my ability to do this was fairly limited.  At the day job I’ve had access to a small group of nodes with extremely modest GPUs.  I set up runs with MLPerf but the datasets (WMT G-E and CoCo) easily fit into memory. Other than a short burst of read traffic at the very beginning of training there was very little IO.  Recently I had the opportunity to meet Michael Balaban, Co-Founder of <a href=\"https://lambdalabs.com/\">Lambda Labs</a>.  I told him what I wanted to do and he gave me access to Lambda’s GPU cloud and beta persistent storage to give it a try.  I was able to grab one of Lambda’s 8x Tesla V100 instances (These things are incredibly popular so it’s best to grab one early in the morning!).  Not all of Lambda’s instance types currently have access to the persistent storage but the V100 instances in the Texas zone do.  Once secured, I got to work.</p><h2 id=\"tensorflow---synthetic\">TensorFlow - Synthetic</h2><p>Before even attempting to run tests with real data, I realized I needed a baseline to start with.  Luckily, Chuan Li, Lambda’s Chief Scientific Officer, wrote a tool for running TensorFlow benchmarks and made it available on github <a href=\"https://github.com/lambdal/lambda-tensorflow-benchmark\">here</a>. One of the advantages of Lambda’s cloud is that they’ve already bundled up many popular tools for running deep-learning workloads into one package called <a href=\"https://lambdalabs.com/lambda-stack-deep-learning-software\">Lambda Stack</a> which comes pre-installed when you start an instance.  This made it fast to get started, though I did run into one issue.  Lambda Stack comes standard with TensorFlow 2, but Chuan Li’s tool relies on a TensorFlow benchmark submodule that is designed to work with TensorFlow 1.  Luckily, the parent repository was unofficially updated to work with Tensorflow 2 (with a warning that it is no longer being maintained).  A quick “git checkout master” in the “benchmarks” submodule directory got everything working.  Chuan Li’s tool makes it simple to run tests with several preconfigured templates already included.  I chose the fp16 resnet50 configuration as it should be fast at processing images and is fairly standard.</p><pre><code>TF_XLA_FLAGS=--tf_xla_auto_jit=2 ./batch_benchmark.sh X X 1 100 2 config/config_resnet50_replicated_fp16_train_syn</code></pre><p>Using the invocation provided in the benchmark README.md file, I was able to quickly run benchmarks with synthetic data on up to 8 V100 GPUs in the node.  At one point I got stuck, hitting what appeared at first to be an unexplainable 25% performance loss. I reran the tests multiple times and even monitored GPU clockspeeds/temperatures in nvidia-smi with no luck.  Ultimately I discovered my error.  In the slow cases, I had inadvertently left out the “TF_XLA_FLAGS=–tf_xla_auto_jit=2” environment variable.  It turns out that setting this allows Tensorflow compile and execute functions with XLA (Accelerated Linear Algebra) support which is a pretty big win for these tests.</p><p><img alt=\"\" src=\"https://markhpc.github.io/images/2022-11-28-Lambda/Tensorflow_-_ResNet50_Synthetic_Training_fp16.svg\" /></p><p>At this point I decided that I needed to understand how Chuan Li’s tool works.  It turns out that he is using the same base tf_cnn_benchmarks.py benchmark code that companies like Nvidia and Dell also use for benchmarking their GPU solutions.  I spent some time running it directly with Dell’s settings from their deep learning overview <a href=\"https://infohub.delltechnologies.com/l/high-speed-object-storage-for-deep-learning/overview-3284\">here</a>.  Unfortunately those tests had mixed results, even after various tweaks.  While researching the XLA issues I mentioned earlier however, I made an even better <a href=\"https://blog.tensorflow.org/2018/11/pushing-limits-of-gpu-performance-with-xla.html\">discovery</a> on the TensorFlow website.  I found an excellent blog post with performance data written by some of the core Tensorflow developers.  It’s now 4 years old, but still appears to be quite valid.  The tuning options used were both simpler and resulted in higher performance versus other configurations that I’ve come across.</p><p><img alt=\"\" src=\"https://markhpc.github.io/images/2022-11-28-Lambda/Tensorflow_-_ResNet50_Synthetic_Training_fp16_blog_compare.svg\" /></p><p>Training with synthetic data in Lambda’s cloud resulted in similar performance to what the Tensorflow developer’s reported.  In fact, using their own settings yielded slightly faster results when running on Lambda’s 8xV100 instance!  It was incredibly encouraging to me that even in Lambda’s cloud environment with virtual machine instances I could achieve performance that was as fast or faster than what the Tensorflow developers were reporting.</p><h1 id=\"choosing-a-real-data-set\">Choosing a Real Data Set</h1><blockquote>  <p><em>The first step to training a neural net is to not touch any neural net code at all and instead begin by thoroughly inspecting your data.</em></p>  <p>        Andrej Karpathy, <a href=\"https://karpathy.github.io/2019/04/25/recipe/\">A Recipe for Training Neural Networks</a>, 2019</p></blockquote><p>Having convinced myself that I had Tensorflow operating reasonably efficiently in synthetic tests, it was time to start thinking about what dataset to use for “real” training.  The largest and most obvious choice is ImageNet.  ImageNet is composed of over 1.2 million categorized images that form a roughly 160GB training dataset.  It is also the largest dataset I could find that was publicly accessible. Downloading it isn’t so easy however. The only version that I could access is the ImageNet Object Localization Challenge dataset hosted on <a href=\"https://www.kaggle.com/c/imagenet-object-localization-challenge\">kaggle</a>.</p><p>After finally figuring out how to download the data, it was time to follow Andrej’s advice and try to learn something about it.  While ImageNet is curated and annotated, it has many images of different sizes, dimensions, and pixel counts.  Images also come from many sources with different levels of quality.  Through the power of stack-exchange I was able to find a bash one-liner script to generate a histogram of image sizes:</p><pre><code>find . -type f -print0 | xargs -0 ls -l | awk '{size[int(log($5)/log(2))]++}END{for (i in size) printf(\"%10d %3d\\n\", 2^i, size[i])}' | sort -n</code></pre><p><img alt=\"\" src=\"https://markhpc.github.io/images/2022-11-28-Lambda/ImageNet_-_Image_Distribution_by_Approximate_Size.svg\" /></p><p>Roughly 80% of the images are in the 64KB or 128KB size bins. Almost all of the remaining images are smaller.  That gives us a pretty good idea of what kind of IOs to expect during classification.  Or at least…it does for frameworks that read those images directly.  In Tensorflow’s case, there’s an alternative format called TFRecord.  TFRecords are basically collections of image data sequentially laid out in much larger files.  Instead of iterating over thousands or millions of individual image files, TFRecords allow Tensorflow to instead stream fewer, larger files that each house multiple images.  It’s a one time cost to pre-process the data so Tensorflow has less work to do during training.  After I downloaded the ImageNet data I took a shot at converting the ImageNet LOC data into TensorFlow records.  Luckily, the TensorFlow tpu github repository already has a <a href=\"https://github.com/tensorflow/tpu/blob/master/tools/datasets/README.md\">tool</a> that can do this.  I had to manipulate the dataset slightly, but ultimately this process worked (at least for the training data):</p><pre><code>pip install gcloud google-cloud-storagepip install protobuf==3.20.1mkdir ~/data/ImageNetFooln -s ~/data/ImageNet/ILSVRC/Data/CLS-LOC/train ~/data/ImageNetFoo/trainln -s ~/data/ImageNet/ILSVRC/Data/CLS-LOC/val ~/data/ImageNetFoo/valln -s ~/data/ImageNet/ILSVRC/Data/CLS-LOC/test ~/data/ImageNetFoo/testln -s ~/data/ImageNet/LOC_synset_mapping.txt ~/data/ImageNetFoo/synset_labels.txtpython imagenet_to_gcs.py --raw_data_dir=/home/ubuntu/data/ImageNetFoo --local_scratch_dir=/home/ubuntu/ExaltedOrbs/ImageNet/tf_records --nogcs_upload</code></pre><p>Perhaps I should say that this worked so long as the original dataset was located on the local NVMe drive.  The persistent storage didn’t fare as well.  Attempting to decompress ImageNet on the persistent storage resulted in blowing past the max number of open files allowed with errors like:</p><pre><code>OSError: [Errno 24] Too many open files.</code></pre><p>Unfortunately this couldn’t be fixed on the instance.  It appeared to be passed through from the host and the persistent storage was completely unusable until the instance was rebooted.  Recently I spoke to one of Lambda’s engineers and they are working on a fix. (It may already be implemented by the time you read this!)  I also want to note that the persistent storage is still in beta so issues like this are not entirely unexpected.  Having said that, before hitting the error it was significantly slower extracting ImageNet on the persistent storage vs on the local NVMe storage.  It’s probably best to extract ImageNet locally and then write the large TFRecords to the persistent storage during the conversion process.  Luckily extracting ImageNet to local storage was fine, and storing the original archive and the resulting TFRecords on the persistent storage worked perfectly fine as well.</p><h2 id=\"fio---baseline-io-results\">FIO - Baseline IO Results</h2><p>Next, I turned my attention to running baseline tests on Lambda’s local and persistent storage using fio.  Fio is a highly configurable and well respected benchmark in the storage community and perfect for generating baseline results.  I decided to use a dataset size that is roughly similar to ImageNet (200GB), the libaio engine in fio with direct IO, and an appropriately high IO depth to let the NVMe drives stretch their legs a bit.</p><p><img alt=\"\" src=\"https://markhpc.github.io/images/2022-11-28-Lambda/Lambda_Labs_8xv100_Storage.svg\" /></p><p>Throughput with the local NVMe drive(s) is surprisingly good.  The persistent storage is slower, but still might be fast enough at a little over 1GB/s for large reads.  16K IOPS was somewhat slower in both cases.  I chose 16K so that I could quickly compare to tests I ran in my Ceph QEMU/KVM performance blog post <a href=\"https://ceph.io/en/news/blog/2022/qemu-kvm-tuning/\">here</a>.  Without getting into the details, I suspect there’s still some room for improved IOPS with Lambda’s setup.  Luckily though, converting into TFRecords should make Tensorflow throughput bound instead of latency bound.  What about PyTorch or other tools that want to read images directly though?  Fio gives us the ability to simulate it by using its ‘bssplit’ feature.  We can take the size ranges and percentiles generated when examining ImageNet and give fio a similar distribution:</p><pre><code>fio --ioengine=libaio --direct=1 --bssplit=2K/1:4K/2:8K/4:16K/8:32K/13:64K/38:128K/33:256K/1 --iodepth=128 --rw=randread --norandommap --size=200G --numjobs=1 --runtime=300 --time_based --name=foo</code></pre><p><img alt=\"\" src=\"https://markhpc.github.io/images/2022-11-28-Lambda/Lambda_Labs_8xV100_Storage_Reads_Second_Bssplit.svg\" /></p><p>This isn’t exactly right as we are not reading data spread across millions of files, but it should provide something of an upper bound on what to expect.  It looks like the persistent storage can do approximately 10K reads/second at a throughput rate of around 750MB/s.  The local storage is about 3-4 times faster.  Local storage should be fast enough to support the kind of images/second throughput rates we want to hit in Tensorflow on 8 V100 GPUs, but the jury is still out for the persistent storage.</p><h2 id=\"tensorflow---imagenet\">Tensorflow - ImageNet</h2><p>Running benchmarks with real data rather than synthetic data is fairly straightforward in Tensorflow.  You simply append data_dir and data_name flags to the CLI invocation to let it know where the TFRecords are located:</p><pre><code>sync; echo 3 | sudo tee /proc/sys/vm/drop_cachespython ./tf_cnn_benchmarks.py --batch_size=256 --num_batches=100 --model=resnet50 --optimizer=momentum --variable_update=replicated --all_reduce_spec=nccl --use_fp16=True --nodistortions --gradient_repacking=2 --compute_lr_on_cpu=True --single_l2_loss_op=True --xla_compile=True --num_gpus=8 --loss_type_to_report=base_loss --data_dir=/home/ubuntu/ImageNet-TF/train --data_name=imagenet</code></pre><p><img alt=\"\" src=\"https://markhpc.github.io/images/2022-11-28-Lambda/Tensorflow_-_ResNet50_Real_Training_First_Attempt_fp16.svg\" /></p><p>Ouch.  Much lower performance with the ImageNet data vs synthetic!  This is especially unfortunate given that 4 years ago the Tensorflow developers reported much better results.  I spent some time reading and experimenting with different settings.  Ultimately the one setting that made a substantial difference was “datasets_num_private_threads”.  In the Tensorflow benchmark source code, this setting is described as: “[The] number of threads for a private threadpool created for all datasets computation.”  I’ll go into more detail what these threads are doing in a bit. For now, let’s see how increasing the number of threads affects the results:</p><p><img alt=\"\" src=\"https://markhpc.github.io/images/2022-11-28-Lambda/Tensorflow_-_ResNet50_ImageNet_Training_fp16_private_threads.svg\" /></p><p>Increasing the number of private threads has a dramatic effect on performance, though I was unable to fully match the performance achieved in the synthetic tests on either the local or persistent storage.  The local storage fared better at high thread counts gradually topping out at around 8600 images/second.  At high private thread counts the persistent storage topped out between 7000-8000 images/second with a higher degree of variability between runs.  I suspect that in this case the persistent storage has likely hit its (per instance) limit.</p><p>In addition to having a dramatic effect on performance, changing the private thread count also had a large effect on the CPU consumption of the TensorFlow process.  CPU usage increases almost linearly with additional private threads up to around 30 cores.  What exactly are these private threads doing?  To answer that question, I utilized two tools that I often deploy when diagnosing CPU usage in Ceph.  When testing with a lower number of private threads, I used linux’s perf tool to look at where cycles are being consumed when the private threads are fully saturated.  At higher levels of private threads, I used my wallclock profiler <a href=\"https://github.com/markhpc/uwpmp\">uwpmp</a> to look at how private threads spend their time when increasing the thread count no longer improves performance.</p><p>In the first case with perf, we can get a good view of the work that these private threads are doing:</p><pre><code>--77.31%--tensorflow::ThreadPoolDevice::Compute          |                    |--51.19%--0x7f511a00c7d8          |          |                    |           --51.18%--tensorflow::jpeg::Uncompress          |--14.48%--tensorflow::ResizeBilinearOp&lt;Eigen::ThreadPoolDevice, unsigned char&gt;::Compute          |--5.47%--tensorflow::CastOpBase::Compute          |--2.66%--tensorflow::ReverseV2Op&lt;Eigen::ThreadPoolDevice, unsigned char, int&gt;::Compute</code></pre><p>The majority of the cycles consumed is in jpeg decompression and resize operations, along with a smattering of other stuff.  What happens if we look at a case with a higher private thread count but now look at wallclock time instead of cycles?  I ended up having some trouble getting the profiler to work properly and consistently get clean callgraphs, but I was able to get at least one run in that revealed some interesting information.  First, I saw time spent in the same functions that perf told us we were spending cycles in:</p><pre><code>+ 100.00% Eigen::ThreadPoolTempl&lt;tensorflow::thread::EigenEnvironment&gt;::WorkerLoop(int) + 99.90% ??? |+ 97.30% ??? ||+ 92.40% ??? |||+ 77.10% _PyEval_EvalFrameDefault ||||+ 47.20% ??? |||||+ 38.10% tensorflow::jpeg::Uncompress(void const*, int, tensorflow::jpeg::UncompressFlags const&amp;, long*, std::function&lt;unsigned char* (int, int, int)&gt;) ||||+ 12.20% tensorflow::ResizeBilinearOp&lt;Eigen::ThreadPoolDevice, unsigned char&gt;::Compute(tensorflow::OpKernelContext*) ||||+ 4.40% tensorflow::CastOpBase::Compute(tensorflow::OpKernelContext*) ||||+ 1.70% tensorflow::ReverseV2Op&lt;Eigen::ThreadPoolDevice, unsigned char, int&gt;::Compute(tensorflow::OpKernelContext*)</code></pre><p>But the wallclock profile also exposed that there may be contention in multiple areas in the private threads around some of the nsync synchronization primitives being used:</p><pre><code> |||||||    |  + 4.50% nsync::nsync_mu_semaphore_p(nsync::nsync_semaphore_s_*) |||||||    |   + 4.50% syscall</code></pre><p>This almost always appeared nested deep inside:</p><pre><code>tensorflow::BFCAllocator::AllocateRaw(unsigned long, unsigned long, tensorflow::AllocationAttributes const&amp;)</code></pre><p>Sadly I was missing a number of debug symbols and don’t 100% trust the wallclock trace.  For now I’ll just say that the private threads are doing a significant amount of work decompressing and manipulating the image data to keep the GPUs fed.  I suspect that with newer and faster GPUs the image retrieval pipeline could become an even bigger issue when training with real image data.  The mystery for me is how The TensorFlow developers achieved such good results 4 years ago without using dedicated private threads at all.  Perhaps they had a significantly faster jpeg decompression mechanism that I am unaware of?</p><h2 id=\"pytorch---imagenet\">PyTorch - ImageNet</h2><p>After running Tensorflow, I also ran some benchmarks in PyTorch using Nvidia’s “DeepLearningExamples” github <a href=\"https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Classification/ConvNets/resnet50v1.5\">repo</a>.  First, I installed the prereqs and setup the repository:</p><pre><code>pip install 'git+https://github.com/NVIDIA/dllogger'pip install --extra-index-url https://developer.download.nvidia.com/compute/redist --upgrade nvidia-dali-cuda110git clone https://github.com/NVIDIA/DeepLearningExamples</code></pre><p>Then, prepared ImageNet for usage in PyTorch:</p><pre><code>cd ~/data/ImageNet/ILSVRC/Data/CLS-LOC/valwget -qO- https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh | bash</code></pre><p>And finally ran a test:</p><pre><code>cd DeepLearningExamples/PyTorch/Classification/ConvNetssync; echo 3 | sudo tee /proc/sys/vm/drop_cachespython ./multiproc.py --nproc_per_node 1 ./main.py --arch resnet50 --label-smoothing 0.1 --run-epoch 1 --amp --static-loss-scale 256 --workspace /home/ubuntu/data/ImageNet-Scratch /home/ubuntu/data/ImageNet-Orig/ILSVRC/Data/CLS-LOC/</code></pre><p>There are a couple of differences here versus the TensorFlow tests.  First, I’m using the raw ImageNet archive instead of a preprocessed TFRecord dataset, so the read behavior is different.  Because I was unable to extract or copy the raw ImageNet archive onto the persistent storage, I’m also only testing the local NVMe drive.  Finally, I didn’t see any specific examples for running with fp16 in nVidia’s documentation, so I’m using amp (automatic mixed precision) which may be slightly slower.</p><p><img alt=\"\" src=\"https://markhpc.github.io/images/2022-11-28-Lambda/Pytorch_-_ResNet50v15_ImageNet_Training_AMP.svg\" /></p><p>Given the number of differences it’s tough to draw direct comparisons with Tensorflow.  Amp is one difference, but it’s quite possible that there are tuning options that could improve performance here that I don’t know about.  I did notice that PyTorch, like Tensorflow, is using quite a bit of CPU to keep the GPUs working.  I suspect that there are ways to tweak the IO pipeline that could improve performance.  For now though, let’s compare the IO patterns on the local NVMe drive during the Tensorflow and PyTorch runs.  I was hoping to be able to use blktrace to do this, but unfortunately was unable to get any data from the virtual devices in the instance.  I was able to collect more general statistics using collectl however.</p><h5 id=\"disk-read-statistics-during-pytorch-8-gpu-run\">Disk Read Statistics During PyTorch 8 GPU run:</h5><table>  <thead>    <tr>      <th>Time</th>      <th>Name</th>      <th>KBytes</th>      <th>Merged</th>      <th>IOs</th>      <th>Size</th>      <th>Wait</th>      <th>QLen</th>      <th>SvcTim</th>    </tr>  </thead>  <tbody>    <tr>      <td>00:29:18</td>      <td>vda</td>      <td>761136</td>      <td>0</td>      <td>6746</td>      <td>113</td>      <td>58</td>      <td>431</td>      <td>0</td>    </tr>    <tr>      <td>00:29:19</td>      <td>vda</td>      <td>752172</td>      <td>0</td>      <td>6648</td>      <td>113</td>      <td>112</td>      <td>810</td>      <td>0</td>    </tr>    <tr>      <td>00:29:20</td>      <td>vda</td>      <td>747824</td>      <td>0</td>      <td>6595</td>      <td>113</td>      <td>84</td>      <td>604</td>      <td>0</td>    </tr>    <tr>      <td>00:29:21</td>      <td>vda</td>      <td>735964</td>      <td>0</td>      <td>6583</td>      <td>112</td>      <td>73</td>      <td>551</td>      <td>0</td>    </tr>    <tr>      <td>00:29:22</td>      <td>vda</td>      <td>695636</td>      <td>0</td>      <td>6237</td>      <td>112</td>      <td>102</td>      <td>760</td>      <td>0</td>    </tr>  </tbody></table><h5 id=\"disk-read-statistics-during-tensorflow-8-gpu-run\">Disk Read Statistics During TensorFlow 8 GPU run:</h5><table>  <thead>    <tr>      <th>Time</th>      <th>Name</th>      <th>KBytes</th>      <th>Merged</th>      <th>IOs</th>      <th>Size</th>      <th>Wait</th>      <th>QLen</th>      <th>SvcTim</th>    </tr>  </thead>  <tbody>    <tr>      <td>00:38:45</td>      <td>vda</td>      <td>1081324</td>      <td>0</td>      <td>8440</td>      <td>128</td>      <td>0</td>      <td>7</td>      <td>0</td>    </tr>    <tr>      <td>00:38:46</td>      <td>vda</td>      <td>927512</td>      <td>0</td>      <td>7241</td>      <td>128</td>      <td>0</td>      <td>7</td>      <td>0</td>    </tr>    <tr>      <td>00:38:47</td>      <td>vda</td>      <td>913512</td>      <td>0</td>      <td>7130</td>      <td>128</td>      <td>0</td>      <td>7</td>      <td>0</td>    </tr>    <tr>      <td>00:38:48</td>      <td>vda</td>      <td>1047444</td>      <td>0</td>      <td>8186</td>      <td>128</td>      <td>0</td>      <td>6</td>      <td>0</td>    </tr>    <tr>      <td>00:38:49</td>      <td>vda</td>      <td>968776</td>      <td>0</td>      <td>7560</td>      <td>128</td>      <td>0</td>      <td>6</td>      <td>0</td>    </tr>  </tbody></table><p><br />When just looking at the IO sizes, both runs appear similar, but that doesn’t tell the whole story.  It is likely that Tensorflow is doing much larger reads that are broken up into contiguous 128KB chunks by the block layer based on the underlying device’s max_sectors_kb setting.  The tells here are the very low queue length and wait times for the TensorFlow run versus the PyTorch run.  In both case the device service times are low (0), but in the TensorFlow case IOs are still backing up in the device queue.</p><p>Interestingly, it appears that it may be possible to use nVidia’s DALI (Data Loading Library) package to <a href=\"https://docs.nvidia.com/deeplearning/dali/archives/dali_170/user-guide/docs/examples/frameworks/pytorch/pytorch-various-readers.html\">read TFRecords into PyTorch</a>.  I didn’t have time to attempt it, but potentially that could have a big effect on IO behavior and performance as well.</p><h2 id=\"conclusion\">Conclusion</h2><p>As I’ve been writing this post, I realize just how complicated it is to understand the performance characteristics of training of neural networks.  Even as we talk about metrics like images/second, the options that are used (batch size for instance) can also affect convergence.  It’s very difficult to come up with a common methodology that is always better than others.  I wonder if another metric, like reaching a desired level of convergence, would be better in the end.  Having said that, I am glad for having done this exercise as I learned some valuable things:</p><ol>  <li>    <p>Pre-processing data into a format like TFRecords on fast local storage is a big win from an IO perspective.  It lets storage systems that have slow metadata performance succeed so long as they have enough sequential read throughput to keep the machine learning framework busy.  This is a big win for many distributed file systems that may have substandard metadata performance (and even the good ones may still benefit).</p>  </li>  <li>    <p>To train on a dataset like ImageNet, you need somewhere around 1-1.3GB/s of raw disk throughput to keep 8 V100 GPUs busy when training in fp16.  For amp or fp32 the requirements are likely lower since the GPUs can’t work quite as fast.  With modern GPUs that are faster than the V100, the disk throughput requirements could be significantly higher.</p>  </li>  <li>    <p>Lambda’s local NVMe storage is likely fast enough to saturate 8 GPUs, even newer ones, so long as the rest of the IO path can keep up.  The persistent storage appears to become a bottleneck with sufficient GPUs and TensorFlow private threads, though can still function fairly well so long as TFRecords are used.  A concern going forward is how to ensure that the data pipeline in TensorFlow and PyTorch are fast enough to keep the GPUs fed.  The Tensorflow benchmark required a large number of private threads and showed potential evidence of contention at high thread counts.  PyTorch did not appear to natively support TFRecords, but NVidia DALI or other 3rd party code might help improve the IO path.</p>  </li>  <li>    <p>If it’s necessary to train directly with images rather than TFRecords, it may not make sense to host them on shared file systems.  It appears that Tensorflow and possibly PyTorch give users the ability to specify a separate training data and work directory.  If all operations against the training data are reads, it may be better to host datasets on read-only block device snapshots. For instance with Ceph, perhaps you could create a read/write RBD volume where you put a certain dataset, take a snapshot, and then map that snapshot as read only on multiple instances that all need access to the same image set.</p>  </li>  <li>    <p>Even with a training set as large as ImageNet, Lambda’s instances have so much memory that eventually the entire dataset becomes cached.  It was necessary to sync and drop caches before each test and keep tests short enough that they didn’t re-read the same data from buffer cache.  I was able to watch as long running tests eventually stopped performing reads and got faster as time went on.  This could make apples-to-apples comparison between different storage vendors difficult if not carefully controlled.</p>  </li>  <li>    <p>I’m almost certainly missing additional tweaks that can help speed up both Tensorflow and PyTorch.  This post shouldn’t be seen as the be-all/end-all for how to achieve high performance with these frameworks, but I hope it may at least help showcase some of the areas that are valuable to investigate when trying to train with real data and achieve high performance.</p>  </li></ol><p>This wraps up my initial work looking at Deep Learning IO behavior.  I hope that next time I can come armed with a bit more knowledge about the internals of how PyTorch and Tensorflow work, focus a bit more on the quality of the training, find even larger datasets to work with, and maybe actually accomplish something useful rather than just play with ImageNet.</p><p>Thanks for reading!</p>",
            "url": "https://hpc.social/personal-blog/2022/an-initial-look-at-deep-learning-io-performance/",
            
            
            
            
            
            "date_published": "2022-11-28T00:00:00-07:00",
            "date_modified": "2022-11-28T00:00:00-07:00",
            
                "author": "Mark Nelson's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/adam-s-weekly-update-2022-11-27/",
            "title": "Adam’s weekly update, 2022-11-27",
            "summary": null,
            "content_text": "What&#8217;s newThe first thing that&#8217;s new is&#8230; this post! I&#8217;m going to try to do at least a weekly post on the blog now, just a general update and some links. This will hopefully help me get back into the habit of writing on the blog regularly, and maybe inspire me to write a bit more in general.I was off work this week for the Thanksgiving holiday, and traveled Michigan to visit my parents and my brother&#8217;s family. My mom has been struggling with some pretty major health issues this year, so it was really wonderful and reassuring to get to spend some time with her and my dad. I also finally got to meet my brother&#8217;s three-year-old son, who was born right before the pandemic started, and who I hadn&#8217;t managed to meet up until now.On the tech-related front, I used this week to take a break from Twitter (mostly), and to be honest&#8230; it was kinda refreshing! I had developed a pretty bad Twitter habit this year, doomscrolling for more time than I like to admit. While I really like Twitter and I&#8217;ve had some nice career boosts from it, it was also a time sink that was not entirely healthy.Admittedly, that time was somewhat replaced by playing around on the Fediverse / Mastodon. But with the lack of algorithmic suggestions, quote tweets, and other means of virality, that network so far feels a lot quieter and less time-consuming than Twitter. Tim Bray has a good post up which discusses some of the advantages and pitfalls of federated social media, and I can highly recommend reading that. I&#8217;m still a bit skeptical that it will be a practical &#8220;Twitter replacement&#8221; for most people, but so far I&#8217;m finding it pleasant.What I&#8217;m readingNonfiction book: Code, Second Edition, by Charles Petzold. This book walks through the process of building a working computer, starting with ideas like Morse code, then working up from logic gates on up. This is technically a re-read, as I read the first edition&#8230; 10+ years ago? But I&#8217;m getting a lot more out of it this time around, and really enjoying it.Fiction book: The Spare Man, by Mary Robinette Kowal. A cozy murder mystery on a luxury cruise to Mars. I&#8217;m only a few chapters in, but already greatly enjoying myself.&#8220;Hiding theory in practice&#8221;, by Fred Hebert. I&#8217;ve been reading a lot about safety engineering and its application to computing lately, but that community can sometimes get off into the weeds about points of theory that don&#8217;t have consensus in the broader computing community. This post has a good discussion of how to use the theory of safety engineering to guide decisions, without requiring that everyone working with you be handed a reading list.&#8220;Paper: Repentance as Rebuke: Betrayal and Moral Injury in Safety Engineering&#8221;, also by Fred Hebert. A discussion of a paper by Dekker et al which looks at the aftermath of the 737 MAX air disasters, and the public repentance of some of the engineers who were involved. Go read the post, it&#8217;s great. And I&#8217;m planning to read the original paper this week.&#8220;Cannon Lake: Intel&#8217;s Forgotten Generation&#8221;, from Chips and Cheese. Really I&#8217;ve been reading a bunch of the technical posts from Chips and Cheese lately, and they&#8217;re doing pretty good analyses of recent hardware. They&#8217;ve definitely earned that spot in my RSS reader.Glenn K Lockwood&#8217;s &#8220;SC&#8217;22 Recap&#8221;. I was sad to miss Supercomputing this year, though enough folks have come down with COVID that I don&#8217;t really regret the decision. But Glenn wrote up a really interesting recap post, with an interesting new viewpoint now that he&#8217;s working at Microsoft Azure. Among other things, he included a whole section titled The underwhelming, with the opening line &#8220;The biggest deal appears to be that exascale is here, and it turns out that it&#8217;s not that big of a deal.&#8221;Recent recipesBecause it was Thanksgiving, I did a lot of cooking this week! I&#8217;m not going to list everything I made, but a few of my favorites were:Cheesy Garlic Butter Rolls from Delish: Nothing special, but really tasty.Challah Stuffing from Smitten Kitchen: This recipe was a huge winner, with most of the family coming back for seconds, and then having more the next day for leftovers. It was really good, and is probably what I&#8217;ll make if I ever do stuffing again.Best Challah from Smitten Kitchen: I baked the bread that went into the stuffing, and it was really tasty on its own! This recipe makes two loaves, and I only needed one for the stuffing. So I also made french toast with it, which worked really nicely.Pet photosGotta have those pet photos.",
            "content_html": "<h2>What&#8217;s new</h2><p>The first thing that&#8217;s new is&#8230; this post! I&#8217;m going to try to do at least a weekly post on the blog now, just a general update and some links. This will <em>hopefully</em> help me get back into the habit of writing on the blog regularly, and maybe inspire me to write a bit more in general.</p><p><span id=\"more-264\"></span></p><p>I was off work this week for the Thanksgiving holiday, and traveled Michigan to visit my parents and my brother&#8217;s family. My mom has been struggling with some pretty major health issues this year, so it was really wonderful and reassuring to get to spend some time with her and my dad. I also finally got to meet my brother&#8217;s three-year-old son, who was born <em>right</em> before the pandemic started, and who I hadn&#8217;t managed to meet up until now.</p><p>On the tech-related front, I used this week to take a break from Twitter (mostly), and to be honest&#8230; it was kinda refreshing! I had developed a pretty bad Twitter habit this year, doomscrolling for more time than I like to admit. While I really like Twitter and I&#8217;ve had some nice career boosts from it, it was also a time sink that was not entirely healthy.</p><p>Admittedly, that time was somewhat replaced by playing around on the <a href=\"https://calico.social/ajdecon\">Fediverse / Mastodon</a>. But with the lack of algorithmic suggestions, quote tweets, and other means of virality, that network so far feels a lot quieter and less time-consuming than Twitter. Tim Bray has a <a href=\"https://www.tbray.org/ongoing/When/202x/2022/11/26/Bye-Twitter\">good post</a> up which discusses some of the advantages and pitfalls of federated social media, and I can highly recommend reading that. I&#8217;m still a bit skeptical that it will be a practical &#8220;Twitter replacement&#8221; for most people, but so far I&#8217;m finding it pleasant.</p><h2>What I&#8217;m reading</h2><ul><li><strong>Nonfiction book: </strong><a href=\"https://bookshop.org/p/books/code-the-hidden-language-of-computer-hardware-and-software-charles-petzold/18465738\">Code, Second Edition, by Charles Petzold</a>. This book walks through the process of building a working computer, starting with ideas like Morse code, then working up from logic gates on up. This is technically a re-read, as I read the first edition&#8230; 10+ years ago? But I&#8217;m getting a lot more out of it this time around, and really enjoying it.</li><li><strong>Fiction book: </strong><a href=\"https://bookshop.org/p/books/the-spare-man-mary-robinette-kowal/18834426\">The Spare Man, by Mary Robinette Kowal</a>. A cozy murder mystery on a luxury cruise to Mars. I&#8217;m only a few chapters in, but already greatly enjoying myself.</li><li><a href=\"https://ferd.ca/hiding-theory-in-practice.html\">&#8220;Hiding theory in practice&#8221;, by Fred Hebert</a>. I&#8217;ve been reading a lot about safety engineering and its application to computing lately, but that community can sometimes get off into the weeds about points of theory that don&#8217;t have consensus in the broader computing community. This post has a good discussion of how to use the theory of safety engineering to guide decisions, without requiring that everyone working with you be handed a reading list.</li><li><a href=\"https://cohost.org/mononcqc/post/385225-paper-repentance-as\">&#8220;Paper: Repentance as Rebuke: Betrayal and Moral Injury in Safety Engineering&#8221;, also by Fred Hebert</a>. A discussion of <a href=\"https://link.springer.com/article/10.1007/s11948-022-00412-2\">a paper by Dekker <em>et al</em></a> which looks at the aftermath of the 737 MAX air disasters, and the public repentance of some of the engineers who were involved. Go read the post, it&#8217;s great. And I&#8217;m planning to read the original paper this week.</li><li><a href=\"https://chipsandcheese.com/2022/11/15/cannon-lake-intels-forgotten-generation/\">&#8220;Cannon Lake: Intel&#8217;s Forgotten Generation&#8221;, from <em>Chips and Cheese</em></a>. Really I&#8217;ve been reading a bunch of the technical posts from <em>Chips and Cheese</em> lately, and they&#8217;re doing pretty good analyses of recent hardware. They&#8217;ve definitely earned that spot in my RSS reader.</li><li><a href=\"https://glennklockwood.blogspot.com/2022/11/sc22-recap.html\">Glenn K Lockwood&#8217;s &#8220;SC&#8217;22 Recap&#8221;</a>. I was sad to miss Supercomputing this year, though enough folks have come down with COVID that I don&#8217;t really regret the decision. But Glenn wrote up a really interesting recap post, with an interesting new viewpoint now that he&#8217;s working at Microsoft Azure. Among other things, he included a whole section titled <em>The underwhelming</em>, with the opening line &#8220;The biggest deal appears to be that exascale is here, and it turns out that it&#8217;s not that big of a deal.&#8221;</li></ul><h2>Recent recipes</h2><p>Because it was Thanksgiving, I did a lot of cooking this week! I&#8217;m not going to list everything I made, but a few of my favorites were:</p><ul><li><a href=\"https://www.delish.com/cooking/recipe-ideas/a23340027/cheesy-garlic-butter-rolls-recipe/\">Cheesy Garlic Butter Rolls from Delish</a>: Nothing special, but really tasty.</li><li><a href=\"https://smittenkitchen.com/2019/11/challah-stuffing/\">Challah Stuffing from Smitten Kitchen</a>: This recipe was a huge winner, with most of the family coming back for seconds, and then having more the next day for leftovers. It was really good, and is probably what I&#8217;ll make if I ever do stuffing again.</li><li><a href=\"https://smittenkitchen.com/2008/09/best-challah-egg-bread/\">Best Challah from Smitten Kitchen</a>: I baked the bread that went into the stuffing, and it was really tasty on its own! This recipe makes two loaves, and I only needed one for the stuffing. So I also made french toast with it, which worked really nicely.</li></ul><h2>Pet photos</h2><p>Gotta have those pet photos.</p><figure class=\"wp-block-image size-large is-resized\"><img alt=\"A blond golden doodle in a red harness and a blue bandanna lays on sandy dirt and looks into the camera\" class=\"wp-image-271\" height=\"233\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/11/IMG_6863-1024x768.jpeg\" width=\"311\" /></figure><figure class=\"wp-block-image size-large is-resized\"><img alt=\"A white calico cat sits on a blanket and washes her front paw\" class=\"wp-image-272\" height=\"410\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/11/69075713241__19379770-6B0C-4780-8DD0-30C62A033C88-768x1024.jpeg\" width=\"308\" /></figure><figure class=\"wp-block-image size-large is-resized\"><img alt=\"A gray-brown tabby cat wearing a green collar sitting on a wall, looking vaguely toward the camera\" class=\"wp-image-273\" height=\"405\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/11/69073206299__DB9CA33B-0EB5-4681-96DA-8368554B6B8A-768x1024.jpeg\" width=\"304\" /></figure>",
            "url": "https://hpc.social/personal-blog/2022/adam-s-weekly-update-2022-11-27/",
            
            
            
            
            
            "date_published": "2022-11-27T15:28:16-07:00",
            "date_modified": "2022-11-27T15:28:16-07:00",
            
                "author": "Thinking Out Loud"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/sc-22-recap/",
            "title": "SC'22 Recap",
            "summary": null,
            "content_text": "The biggest annual conference in HPC, the SC conference, was recently held in Dallas, Texas in its second hybrid incarnation since being all-remote for the pandemic. This year attracted over 11,000 attendees which is much closer to the pre-pandemic high of 14,000 than last year's 7,000, and judging from the crushed conference rooms and busy expo floor, it looks like SC is not that much worse for wear.This year's conference quite different for me since I attended for my first time as a vendor, not a researcher or practitioner, and I spent most of my days behind closed doors talking to customers. I didn't get to attend any of the keynotes, BOFs, or panels to which I wasn't invited as a result, so I'm not really qualified to give an erudite summary of the conference or expo this year.So instead, I'm just writing down what I remember in order that I remember it and not necessarily in a coherent narrative form. I'm sure I missed a lot (for example, mixed precision seemed big this year, and I heard Jack Dongarra gave a fantastic Turing Award talk) so I encourage others to write their own recaps and share with the community!High-level themesI actually started writing an SC'21 recap last year which I never posted, and re-reading the intro was funny--you'd think nothing has changed in the last year.The underwhelmingThe biggest deal appears to be that exascale is here, and it turns out that it's not that big of a deal. China let the air out of the tires by debuting their exascale systems at SC'21, and not only did they thumb their nose at Top500 by not submitting, they debuted by winning a Gordon Bell prize instead. The first US exascale system, Frontier, was debuted at ISC this year leaving its showing at SC a bit deflated too. Frontier was featured in the Gordon Bell prize-winning paper this year, but that work required the use of four Top-10 systems, not just Frontier, painting the reality that one giant computer rarely stands on its own when it comes to advancing science.This isn't to say that deploying exascale systems isn't a noteworthy feat and worth commendation, but I felt like the hype over the last five years treated the achievement like an end state instead of a milestone. And now that we've passed the milestone, the community is grasping to figure out what comes next. So what is next?Quantum had a strong and growing presence at SC, as it has for the last few years. But the conclusion of the panel \"Quantum Computing: A Future for HPC Acceleration\" was that no, it's not close to being ready.Disaggregation and composability was another theme with growing momentum. And like quantum, there was a panel asking the same question: \"Does HPC need composability now?\" The answer, again, was no, not yet. More on that below.What about RISC-V? Surely that will revolutionize the field. As it turns out, the answer there is also that RISC-V is not ready to do anything useful for HPC yet.The list goes on of technologies and trends that people are trying to boost now that exascale is \"solved.\" The reality, I think, is that \"exascale\" will take years to actually mature since it appears to have a ton of technical debt that accumulated during the race to be first. US Exascale rests on the shoulders of AMD and Intel, two companies whose software stacks have not caught up to the market leader, so there will be a lot of thrashing around as development practices and optimization settle out around these systems.Struggling with code porting is not very exciting to computer science Ph.D.s, so I expect future SCs to mirror this one and bifurcate into two distinct tracks: those struggling to identify the next big thing in the research space, and those struggling to use the systems that were rushed to deployment.The unexpectedMy SC experience was very biased since I didn't get out much, but two related themes kept popping up across different meetings and the sessions I did attend.Power efficiency is serious business now. It used to seem like people talked about the need for energy-efficient HPC in an abstract sense while continuing to jam more power into every rack without changing their approach to system design, facilities, and deployment models. That has hit a hard wall with energy prices soaring in Europe, though. The financial impacts of power-inefficient supercomputing have gone from a one-time capex cost to an ongoing opex cost that is putting many HPC facilities on an unsustainable cost trajectory. Even sites that aren't doing new deployments are facing sudden, sharp increases in their costs, and nobody has good answers about how they will keep the lights on.Cloud HPC is confusing. With only 15% of total HPC dollars winding up in the cloud, it's little surprise that most HPC folks are only peripherally aware of what HPC in the cloud really means. Worse yet, a subset of those folks are actively hostile towards the idea of running HPC workloads in the cloud. I spoke with my colleagues from all three major cloud service providers as well as my colleagues in DOE, NSF, and education throughout the week, and everyone painted this same general picture.There seems to be a mismatch between the expectations of on-prem HPC folks and cloud HPC folks. For example, I was asked why Windows doesn't support OpenMP very well, and after a bit of digging, I realized that the question really wasn't about using OpenMP on Windows as much as it was about using OpenMP in the cloud. There was a latent assumption that \"HPC in Microsoft's cloud\" must mean \"HPC on Windows\" which, for the record, is false--I don't even know how to use Windows anymore. Similarly, people decried the performance impacts of sharing HPC nodes with others in the cloud (they are not shared), overheads of virtualizing InfiniBand or GPUs (everyone uses PCIe passthrough or SR-IOV for HPC nodes), and other misconceptions.This isn't to say that cloud people aren't confused too; I heard stories about conversations that went sideways because a cloud folks (not from my employer, thankfully!) didn’t realize that the requirements of a traditional gov/edu HPC facility couldn’t be neatly wrapped up into a single workload with a single solution, contrary to the case across many commercial AI shops. And both sides are struggling to find models for partnership and engagement that mirror the traditional relationship between places like a DOE or NSF facility and a company like Cray. HPC departments are used to buying supercomputers and parallel file systems, while cloud providers sell computing and storage as a service. The distinction may seem trivial at the surface, but there's a large divide that becomes evident once both sides start trying to drill into the details of what a partnership would look like.Parallel I/O in Practice TutorialThis was my fifth year contributing to the Parallel I/O in Practice Tutorial with my colleagues at Argonne and Google, and it was our first time doing it in-person since 2019. It felt really good to be back in front of people to opine about the perils of POSIX and the greatness of the Darshan I/O profiling tool, and this year I retired out the material I used to present on burst buffers (since DataWarp and Infinite Memory Engine have lost relevance in HPC) and the TOKIO holistic I/O analysis framework (since it is no longer funded/maintained). In their stead, I presented material on benchmarking with IOR and mdtest I debuted at LUG 2022 this year.I haven't gotten feedback yet on whether this change was a net positive one, but I think it went over well. Benchmarking I/O is really challenging if you don't understand how things like page cache really work in distributed systems, and walking through some benchmark examples concretizes a lot of abstract parallel file system concepts like locking and striping. And since benchmarking is a rabbit hole of arbitrary complexity, ending the tutorial with advanced benchmarking topics turned out to be a nice way to add buffer to the end of an eight-hour stretch of carefully timed presentations. It's very easy to skip over the nuances of analyzing mdtest outputs if attendees have a lot of questions about more important things at the end of the day.The most surprising observation of the tutorial is how many attendees aren't using MPI anymore. We got a lot of questions last year about task-oriented I/O, and this year had some great questions about trying to understand or tune the I/O performed by Python-based analytics frameworks. We decided to add support for Darshan to profile non-MPI applications back in 2019 which is now paying dividends by ensuring it is a relevant tool for these new analytics and AI workloads, and we'll probably have to give more attention to optimizing these workloads' I/O in the future.DAOS User GroupMonday morning was cold and rainy--a perfect day to attend the 2022 DAOS User Group which was held off-site at the Fairmont Hotel.Whether you particularly care about DAOS or not, the cross-community HPC I/O brain trust is guaranteed to be in attendance, and this year did not disappoint. In addition to the expected stakeholders from Intel and DOE, representatives from all three big CSPs were in attendance. Google Cloud, Seagate, and HPE/Cray were all on the agenda, painting a diversifying landscape of large HPC companies investing time into DAOS and the strength and willingness of the DAOS team to partner with all comers.Life after OptaneThe question that opened up the meeting, of course, was \"what is the future of DAOS since Intel cancelled Optane?\" Kelsey Prantis had the official statement (I'll replace the grainy photo once the DUG slides are online...):The high-level project answer is that DAOS isn't going anywhere. Aurora, by virtue of still having Optane DIMMs, will not be affected, and DAOS will maintain support for Optane until Intel drops its last Optane DIMMs (Crow Pass for Sapphire Rapids) from support life sometime towards the end of this decade.For new customers who aren't going to use Optane, the answer is \"Metadata on NVMe,\" a development being codeveloped by Intel, HPE, and Google to implement a write-ahead log (WAL) and allow DAOS to use volatile DRAM instead of Optane. It will work like a file system journal in that a compact representation of writes will be committed to NVMe immediately after landing in DRAM, and then DAOS will asynchronously write back the properly serialized representation of that transaction after it is acknowledged. Johann Lombardi had a helpful cartoon that showed how this WAL will fit into DAOS:A key benefit of DAOS's implementation of this WAL is that it will be able to still service incoming writes while flushing old writes; although I don't fully grasp how this works, it is something enabled by the sophisticated I/O scheduler already implemented in DAOS.The complete implementation isn't expected to be released until Spring 2024, but it appears to touch only a few components of DAOS and doesn't affect anything above the VOS layer of the DAOS server.There was also mention of developing operability with new CXL-attached memory-semantic SSDs to keep the persistent memory capability of DAOS alive beyond Optane. I'm not sure if this would offer a performance benefit over the metadata-on-NVMe feature; early results show that metadata-on-NVMe actually delivers higher IOPS than Optane since the synchronous write path is much simpler without having to account for memory persistence. That said, I didn't really follow the full extent of options on the table for how DAOS metadata may work across different types of memory though.DAOS in the flesh at ArgonneKevin Harms presented an update on Aurora's massive 220 PB DAOS installation and laid out its configuration. There are 1,024 DAOS servers based on the Intel Coyote Pass server design, each sporting2x Intel Xeon 5320 (Ice Lake) sockets2x DAOS engines (one per socket)16x 32GB DDR4 DIMMs16x 512GB Optane DIMMs (Persistent Memory 200)16x 15.36 TB Samsung PM1733 NVMe SSDs2x 200 Gb/s Slingshot NICsThe total configuration is quoted at 220 PB usable, but Kevin pointed out that this assumes that every object is erasure coded at 16+2. Unlike virtually every other storage system out there, though, users can choose the data protection for their individual objects when they create them, meaning this 220 PB capacity is an upper limit to what users can do. Users with very hot, read-only objects may choose to replicate instead of erasure code, while others who are capacity-constrained may choose to erasure code everything at 16+2 at the cost of latency and IOPS. This flexibility is really powerful for users since they can tailor their object layout (\"object class\" in DAOS parlance) to match the needs of their workload.Argonne will be slicing up this DAOS system by giving each scientific project its own DAOS pool, and each pool will be assigned to only 80% of the available DAOS servers by default. This seems like a nice way of providing most of the storage system performance to every user, but offering more freedom to work around bad hardware, bad users, and other performance problems that plague file systems like Lustre that distribute everything across every single server equally.Finally, I noticed that Aurora will be using Samsung SSDs, not the Intel (now Solidigm) QLC NAND that appeared in all the DAOS slides floating around two years ago. I'm not sure what happened there, but the move from Solidigm QLC to Samsung TLC couldn't have been cheap.New features and contributionsDAOS is starting to pick up some truly valuable features that are being developed and contributed by third parties. Of note, croit has contributed a feature which allows DAOS to serve up NVMe over Fabrics targets, and Seagate contributed an S3 gateway for DAOS. Along with the DFS file system interface, DAOS now offers the trifecta of standard object, block, and file services just like Ceph. Unlike Ceph though, performance on DAOS is a first-class citizen. While croit made it clear that the NVMeoF support still has a ways to go to improve the way it does thread pooling and provides resilience, they showed 1.4 million IOPS from a single storage client using TCP over Ethernet with minimal client-side overhead.Intel is also developing multitenant support for DFUSE, allowing a single compute node to share a DAOS mount and let permissions be enforced through UID/GID just like a regular file system. Before this update, the FUSE-based nature of DAOS allowed any unprivileged user to mount their container (good), but only one FUSE agent could be alive on a single node at a time (not good) which prevented multiple users sharing a node from both mounting their own containers.DAOS also has some longer-term enhancements that I thought were interesting:expanding the range of POSIX calls supported by DAOS's intercept library to include metadata calls and memory-mapped I/O using userfaultfdimplementing collaborative caching - essentially reimplementing the Linux kernel page cache in userspace so that multiple processes can share cached DAOS pagessupporting a computational storage paradigm by enabling offload of userspace eBPF scripts to DAOS serversDAOS in a larger data center ecosystemDean Hildebrand from Google Cloud then gave an overview of Google's efforts in bringing DAOS into the cloud. He had some nice performance graphs and I'll link the full presentation here once it's uploaded (it's worth a watch), but the part I found the most insightful was how they are trying to decide where a technology like DAOS fits in the larger cloud storage ecosystem. He outlined two different ways DAOS could work in GCP:Caching: Google Cloud Storage (GCS) is the point of truth and DAOS is a cacheTiering: DAOS is a point of truth, and GCS is an archiveHe said they were leaning towards the caching model where data only lives ephemerally in DAOS, and personally, I think this is the right move since DAOS in the cloud is not resilient without Optane. However, this choice reflects a much larger tension in cloud storage for HPC:The centerpiece of every cloud's data story is a scalable, low-cost, low-performance object store which is analogous to what on-prem HPC would call campaign, community, or project storage.HPC demands higher performance than what these object stores can generally deliver though.To bridge the gap between these two truths, auxiliary services must bolt on to the object layer and provide higher performance, at a higher cost, for the duration of I/O-intensive HPC jobs. Some choose to provide true tiering from object into a resilient layer of flash (like FSx Lustre and Weka do), while others project the contents of the object through a high-performance caching layer (like HPC Cache and File Cache) and are never meant to persistently hold data.This isn't rocket science, but I never thought deeply about the two models since campaign/community/project storage in on-prem HPC is usually fast enough to avoid needing caches or fine-grained tiering capabilities.John Bent also had a thought-provoking presentation about how Seagate's now-\"deprioritized\" CORTX object store, which once competed with DAOS as Mero, contains ideas that can complement DAOS:Whereas DAOS delivers high performance using NVMe, CORTX delivers great economics using HDDs, and their strengths are complementary to each other. While I don't fully grasp how a tiered (or caching!) system comprised of DAOS and CORTX could be implemented, John rightly pointed out that the same level of space efficiency can deliver higher data protection if multi-level erasure coding is used to stripe across durable block storage. His specific example was erasure coding at 8+1 across servers and 10+1 within servers to deliver both high efficiency and high durability. This could map to something like running DAOS atop something like CORVAULT, but I don't think all the necessary pieces are in place to realize such a harmonious coexistence yet.Of course, completely tossing Reed-Solomon for something more sophisticated (like VAST does with its locally decodable 150+4 scheme) obviates the need for multilevel erasure entirely. But DAOS has not gone down that route yet.And as with every talk John gives, there were lots of other interesting nuggets scattered throughout his presentation. Two of my favorites were:A slide that pointed out that, when you buy something like Ceph as an appliance, you may be spending only 25% of the total cost on storage media and the rest is infrastructure, service, and support. This struck me as a bit on the low end, but some enterprisey NAS and midrange parallel file system appliances can go this low. Spending 60% to 90% on media is a lot nicer for the buyer (and companies like Seagate) if you can buy at scale or eschew the white-glove support, and John suggested that it's up to companies like Seagate to fix the software issues that require customers to pay for white-glove support in the first place.  After all, the less someone spends on support and licenses, the more they can spend on Seagate hard drives.John's final slide pointed out that object stores were originally designed to get around the limitations of POSIX file systems, but as they've evolved over the last decade, they're starting to look a lot like file systems anyway since they require strong consistency, hierarchical namespaces, and familiar file semantics. Has all the work put into developing super-fast object stores like DAOS over the last ten years really just brought us back full circle to parallel file systems?  Companies like VAST and Weka have shown that maybe POSIX isn't as bad as the research community (myself included!) have claimed it to be; it was really just low-performance implementations that nobody wanted.Once John's talk is uploaded to the DUG 2022 website, I'll link it here.  Like Dean Hildebrand's talk, it is well worth watching (but for wildly different reasons!)PDSW 2022I had to duck out of the DAOS User Group early to run (through the rain) to the 7th International Parallel Data Systems Workshop (PDSW 2022) on Monday afternoon.Much to everyone’s surprise, PDSW was only given a half day this year and everything felt a little compressed as a result. The organizers kept the work-in-progress (WIP) sessions which can often be an interesting peek into what students are pursuing, but little A/V problems and the unforgiving schedule probably did a disservice to the up-and-comers who use the WIP track to lay the groundwork for future full-length papers. Hopefully SC’23 restores PDSW to its original full-day status.&lt;p&gt;&lt;/p&gt;Splinters keynote from Arif Merchant at GoogleThe keynote presentation was given by Arif Merchant from Google about Splinters, the framework that Google Cloud uses to sample I/Os in a scalable way. The challenge they face is that it's impossible to trace and store every single I/O that hits Google's storage servers (D servers), but having an understanding of I/O patterns is essential for characterizing workload I/O behavior and planning for future infrastructure. In fact, this problem is so important that Google isn't the only cloud that's solved it!A lot of what Arif talked about is very similar to how Azure does its I/O tracing under the hood. I suppose it should not be surprise that there are only so many ways to solve the challenge of sampling individual IOPS in a way that fairly represents the aggregate workload of a huge distributed storage system. One really smart thing Splinters does that I liked was sample along two different dimensions: not only do they evenly sample across all IOPS at a fixed rate (the obvious thing), but they also sample across files at a fixed rate. In this latter case of per-file sampling, they take a tiny fraction of files and capture every I/O for that file to get a complete picture of how individual files are being accessed.This file sampling fills the huge gap that exists when randomly sampling IOPS alone. Because different I/Os have different \"costs\" (for example, reading a 1 MiB file using a single 1 MiB read op or 256x 4 KiB read ops are functionally equivalent to an application), randomly sampling ops introduces systematic biases that can be difficult to back out after the data has been sampled, subsampled, aggregated, and reduced. Splinters' approach lets you see the workload from two different angles (and biases) and answer a much larger range of questions about what's really happening across thousands of storage servers.That said, it was interesting to hear Arif describe how Splinters evolved out of a different internal Google project but wound up outliving it. Splinters is also similar to, but slightly different from, their Dapper infrastructure which also does scalable distributed system tracing. And he made overtures to F1, a scalable SQL database that is similar to (but not the same as) the SQL-like query interface that Splinters uses. I got the impression that new technologies come and go pretty quickly at Google, and there's a large appetite for creating new software systems outright rather than shoehorning an existing system into solving a new problem. I can't say one way is better than the other; I was just surprised at the contrast with my own experiences.Practical papersPDSW had a healthy combination of both very-researchy papers and applied research papers this year. I could only stick around for the applied papers, and two left an impression.In the first, Jean Luca Bez presented Drishti, a tool that lives downstream of the Darshan I/O profiling library and finally does what the Darshan community has danced around for years--turning a Darshan log into an actionable set of recommendations on how to improve I/O performance. It does this by cataloguing a bunch of heuristics and using Darshan's new Python integrations to pore through a log and identify known-problematic I/O patterns. Like Jean Luca's DXT Explorer tool, Drishti has a slick user interface and greatly extends the usability and insights that can be pulled out of a Darshan log file. It probably won't win a Turing Award, but this sort of work is probably going to benefit scores of HPC end-users by making Darshan (and troubleshooting I/O problems) much more accessible to mere mortals for years to come.Adrian Jackson also presented a very tidy apples-to-apples comparison of DAOS and Lustre on the same hardware using both a systems-level benchmark and an application-inspired, object-oriented data model benchmark. The specific bake-off of a new curiosity (DAOS) and the decades-old incumbent (Lustre) is probably interesting to storage nerds, but I think the real novelty of the work is in its exploration of some uncomfortable realities that the HPC I/O community will have to face in the coming years:Does \"slow memory\" (nonvolatile Optane or CXL-attached memory SSDs) give actual benefit to existing file systems (like Lustre), or is rethinking the entire storage stack (like DAOS did) really necessary to unlock the performance of new hardware?Do applications need to rethink their approach to I/O to make use of post-POSIX storage systems like DAOS, or is performing I/O as you would on a file system (Lustre) on a post-POSIX storage system (DAOS) good enough?My take from the work is that, for simple I/O patterns like checkpoint/restart, you can get pretty far by just treating something like DAOS the same as you would a parallel file system:Figure from Manubens et al, \"Performance Comparison of DAOS and Lustre for Object Data Storage Approaches.\"But if you want your data at rest to have the same data model as how it's handled within the application, you really ought to use a storage system that supports data models that are more expressive than a stream of bytes (which is what POSIX files are).The authors didn't do a perfect job of giving Lustre its fair shake since they chose to use (abuse) directories and files to represent their application's data model on-disk instead of developing an object-file model that file systems like Lustre handle a little better. But let's be real--HPC is full of applications that do the exact same thing and represent datasets on-disk using complex hierarchies of directories and files simply because that's the easiest way to map the application's representation of data into the standard file system model. In that sense, storage systems that represent rich data models in a high-performance way should be really valuable to naive applications that map in-memory data structures directly to files and directories.Going back to John Bent's closing slide from his DAOS User Group talk, though, does any of this even matter since all answers lead back to parallel file systems? Maybe there's something to be learned about adding better back-door APIs that support more diverse data models than what POSIX file interfaces give us.The SC22 ExpoThe expo is my favorite part of SC because it's when I get to talk to people one-on-one and learn about corners of the HPC industry that I would've never otherwise sought out. Much to my dismay, though, I had very little time to walk the floor this year--so little that I didn't get any swag. If you want to read up on what interesting technology was being showcased, I strongly recommend reading all the great content that Patrick Kennedy and his team at STH created covering the expo.That said, I did notice some curious trends about the show floor overall.The NVIDIA booth was notably absent this year (though they shared booth space with partners), and many of the usual top vendors had significantly smaller presence on the expo floor. Just for fun, I compiled the top ten(ish) vendors by booth size:Weka.io (3,200 sqft)VAST Data, Department of Energy, Penguin Computing, HPE, and Microsoft (2,500 sqft)AWS (2,000 sqft)Google and TACC (1,600 sqft)Supermicro, AMD, Intel, Dell, NASA, and Indiana University (1,500 sqft)I think it's amazing to see all-flash storage companies at the top of the list alongside all of the Big 3 cloud service providers. I may be reading too much into this, but this may mean that the money behind SC is shifting towards companies playing in the cloud-based AI space instead of traditional big iron for simulation. Or perhaps it's a sign that most of the traditional HPC players are taking a hard look at the return they get on a big booth given the current economic climate and pulled back this year.I did chat with a couple colleagues who completely opted out of a booth this year (for reference, SC'21 had 10% fewer exhibitor booths than SC'19), and the reasoning was consistent: they found more value in having staff meet with customers privately or attend the technical sessions and engage with people organically. Combined with a bit of bad taste left over from SC's high cost of hosting pandemic-era \"digital booths\" despite low return (did anyone visit digital booths at SC'20 or SC'21?), I can see why some vendors may have chosen to skip the expo this year.Whatever the reasons may be, I was a bit sad to see such a small presence from some of my favorites like IBM, Fujitsu, Atos, and NEC. Hopefully the SC Exhibits Committee (and the economy!) can find ways to bring back the pre-pandemic glory of the show floor.The expo wasn't all doom and gloom though! Even though I couldn't make my complete rounds this year, there were a couple of highlights for me.VAST's masterful marketingPerhaps the splashiest vendor at SC was VAST Data who had a brilliant marketing presence. First was the giant Vastronaut mascot that was the centerpiece of their booth:A quick search of Twitter shows just how many people seized the opportunity to take a selfie at their booth. I would love to know how they transported that thing to and from the conference, but whatever the cost, I'll bet it was worth it.At the Grand Opening Gala on Monday, they also gave out delightfully tacky light-up cowboy hats that everyone seemed to be wearing:We were there! #sc22 #sc2022 @VAST_Data pic.twitter.com/fWhuSgBfpL— ntnu-hpc (@ntnuhpc) November 15, 2022The subtle genius of this was that not only did people wear them during the gala and the Flop Gun-themed Beowulf Bash 2022 party later that night, but they had to wear them on their plane rides home since they were so inconveniently bulky. Proof in point, my wife (who doesn't work in tech) sent me this text message to confirm that she was waiting for me at the right luggage carousel at San Francisco Airport:I wonder how many innocent bystanders, traveling home for Thanksgiving on Thursday or Friday, saw the shiny cowboy hats at airports around the country and wondered what VAST was.The icing on the cake was VAST's CEO, Renen Hallak, parading around in an unmissable Chuck McGill-style space suit all week, clearly not taking himself too seriously and painting VAST as a work hard/play hard kind of company. Now, do flashy space suits and blinking cowboy hats alone mean VAST has a great product? I can't say**. But marketing is an art that I appreciate, and VAST hit some great notes this year.** (Seriously, I'm not sure I wouldn't get in trouble for opining about another company here.)The Microsoft hardware barThe only booth where I spent any appreciable time this year was my own employer's. I personally love booth duty and accosting strangers on the show floor, especially if there's something interesting at the booth to jumpstart a conversation. When I worked at SDSC it was a Raspberry Pi cluster, and at the Microsoft booth this year it was the \"hardware bar.\"In addition to the customary booth presentations with giveaways, swag desk, seating area, and a fun caricature artist, the physical servers that underpin the HPC nodes in Azure were on display. Microsoft contributes its hardware platform designs to the Open Compute Project so the physical hardware that runs in Azure data centers isn't entirely mysterious. Still, every cloud has its hardware secrets, so I was surprised to see these servers laid bare.The newest HPC node type (dubbed HBv4) on display was a node powered by AMD's Genoa processors just announced a few days earlier:This wasn't a display model, either; it had real DDR5 DRAM, a real NDR InfiniBand HCA, real PCIe Gen5, and real big OCP mezzanine card with real big aluminum heat sinks and a big Microsoft sticker on top. A couple visitors commented on the way the heat piping for those Genoa CPUs was done which I guess is unusual; rather than have a giant copper block on top of each socket, heat pipes connect the socket to massive aluminum heat sinks that are closer to the chassis inlets. In retrospect it makes sense; Genoa has a whopping twelve DDR5 DIMMs per socket which leaves little extra room for heat sinks, and these 88+ core sockets have a staggering thermal design power.Another exotic piece of hardware on display was an \"ND MI200 v4\" server:It's logically similar to Azure's \"ND A100 v4\" server platform with two CPU sockets, eight SXM4 GPU sockets, eight 200G HDR InfiniBand HCAs, and a bunch of M.2 NVMes. But this specific server has eight MI200 GPUs on a common OAM baseboard and uses Infinity Fabric for GPU-to-GPU communication. I've never seen an OAM-socketed anything in real life before, much less eight of them on a baseboard, so I thought this was pretty great to see in the flesh.The ND A100 v4 platform was also on display and looked very similar-but-different with its eight A100 GPUs and HGX baseboard:And unlike the MI200 variant, the general public can run on these nodes.I'm not sure what more I'm allowed to say, but my colleague Karl made a nice, quick video that runs through the entire Microsoft booth that's worth a watch, and more details can be had by contacting me or your favorite Microsoft account team privately.Of course, the hardware bar was just a way to lure people into the booth so I could achieve my real goal: meeting new folks. As I wrote before, one of my biggest realizations at SC this year is how generally confused people are about what HPC in the cloud really means--both people who come from traditional on-prem HPC and people who come from traditional enterprisey cloud. I found myself surprising many of the people with whom I spoke on the show floor with factoids that I have taken for granted. For example,Linux is the most common OS on these HPC node types. While you probably(?) can run Windows if you want on this stuff, I think only a few niche markets do this.The usage model for an HPC cluster in the cloud can be the same as on-prem. You can have login nodes, Slurm, home directories, parallel file systems, and all that. Jobs don't have to be containerized or turned into a VM image.The InfiniBand coming out of these nodes is real InfiniBand with real OFED that supports real mpich/mvapich/OpenMPI. It's the same stuff as in on-prem supercomputers. And nodes are assembled into full-bisection fat tree InfiniBand clusters just like normal.There's no noisy neighbor problem on compute nodes because HPC node types aren't shared between users. When you run a VM on an HPC node, you get the whole thing. Just like on large supercomputers.There's no horrible loss of performance due to running in a VM. Virtualization extensions, PCIe passthrough, and SR-IOV bypass the hypervisor for most things. Inside your VM, you see real Zen cores and real Mellanox HCAs, not virtualized devices.My takeaway impression is that a lot of traditional HPC folks looked at the cloud five or ten years ago, had a sour experience, and haven't paid attention since. In those last five years, though, AI has changed the game. Massive demand for the latest CPUs and accelerators, funded by live-fast-die-young venture capital, has given cloud vendors tremendous financial incentive to catch up to on-prem levels of performance efficiency for AI workloads. And it just so happens that infrastructure that's good for AI is also good for traditional modeling and simulation.SCinet!One of the unexpected highlights of my SC this year arose from a chance encounter with a former coworker from NERSC, Ron Kumar, who gave me a whirlwind tour of SCinet.I have to confess great ignorance around SCinet in general; I always saw it was a weird technological proof of concept that the strange networking people at work would go off and do in the weeks leading up to the actual conference. I knew they did some impressive wide-area transfer demos (like the petabyte-in-a-day demo at SC'16), but I didn't really get the significance.So what is SCinet? It's this yellow bundle of cables dangling from the ceiling.&lt;p&gt;The yellow cables are 144-core fiber trunks that bring over a terabit per second of bandwidth into the convention center from the Internet via the national research backbones like ESnet and Internet2 and distribute many terabits per second of capacity throughout the SC conference venue. For comparison, most HPC centers in the US only have a tenth of SCinet’s wide-area bandwidth at best since 400G infrastructure is still rolling out.&lt;/p&gt;Most attendees may be familiar with the row of expensive-looking networking racks behind a glass wall towards the back of the expo which is where those yellow cables dangling from the ceiling end. Here's a photo from inside that glass wall:What I didn't realize is that if you go around to the back of the giant walled area behind this glass display, there's a security checkpoint that gates entry into a massive network operations center (NOC) full of laptops, spools of fiber, meeting rooms, and busily working teams in charge of all the lower layers of the networking stack.The process to get into the NOC involves an escort and being tagged in with a tamper-proof wristband, and I learned on the tour that there's millions upon millions of dollars worth of high-end networking equipment in the racks shown above. If you look closely, you can see a security camera at the end of the aisle that speaks to this; that camera was one of many.Behind the pretty public-facing side of the SCinet racks is a mess of fiber and cables:I guess if you have to tear all this down after just a few weeks, there's no point in investing days in dressing it all up nicely! I particularly enjoyed the fiber panels in the third rack that appear to be affixed to the rack post with shoe laces.This year, SCinet did do a neat proof-of-concept where they demonstrated three 400G routers from three vendors (Juniper, Arista, and Cisco?) all talking the same protocol to handle what I assume is the core routing for everything in the convention center:I wish I remembered exactly what was going on here, but I know enough about networking to know that, despite there being standard protocols for coordinating between networking gear, each vendor does their own implementation that is rarely easy to get interoperability from. If anyone out there knows the details of this achievement, please let me know so I can explain this a little better!In addition to networking nerd-level demonstrations, SCinet also serves up all the wifi across the convention center. That is why there were tripods with access points scattered around, and why astute attendees may have noticed janky networking equipment scattered around that looked like this:Again, I get it: for a network infrastructure that's only going to last a week, I don't think it's a good use of anyone's time or money to nicely dress all the networking.One last factoid I didn't know until this year was that exhibitors can request 100 Gb/s network drops into their individual booths for demos (or downloading the latest version of a PowerPoint presentation really fast). The end result of supporting both a vast wifi network and 100G fiber across the show floor is that there was a lot of fiber going into the single row of SCinet equipment:Finally, when I posted some of these photos online during the conference, my colleague Bilel was kind enough to post a slide from the SC22 opening presentation that had the speeds and feeds of what I had toured:Candy Culhane shared Scinet facts #SC22 #HPC5.01 Tb/s of WAN capacity$70M in HW &amp; SW, &amp; services provided by 29 SCinet contrib.175 volunteers from 80 vol. organiz.&gt; 450 wireless deployed29 network research exhibition proposals11.7 miles of fiber 2384 fiber patch https://t.co/JtPhjVHZJd pic.twitter.com/kwGl5Ydqp5— Bilel Hadri (@mnoukhiya) November 16, 2022If you know anyone involved with SCinet, I highly recommend seeing if you can get a tour at the next SC. Even as a relative networking novice, I walked away with a much greater appreciation for the annual achievement of building SCinet. And who knows? Once I get bored of this whole storage thing, maybe I'll try getting into high-performance networking.Composability panelThis year I was invited to participate in a panel titled \"Smackdown! Does HPC Need Composability Now?\" moderated by Addison Snell and Dan Olds from Intersect360 Research. This panel was...different. Unlike the traditional SC panel where panelists take turns presenting slides and saying erudite things, this panel had two teams of panelists. And my team only had one slide to present:The ground rules included \"personal attacks are allowed,\" and needless to say, the panel was about equal parts entertainment and technical discourse. That's not a bad thing, though.Addison and Dan did a phenomenal job of pulling their respective teams together and leading discussion in a format that both brought forward the key pros and cons of composability in HPC while poking fun at the thinly veiled, ego-driven personalities that often make up these sorts of panels. Rather than politely dancing around issues like sacrificing memory bandwidth by putting accelerators at the far end of a PCIe bus or gaining higher utilization by allowing users to mix and match CPU, NICs, and GPUs, us panelists were free to shoot straight (or perhaps a bit hyperbolically) and call each other out on our hidden agendas.I hope it goes without saying that all us panelists were in on the format and don't actually think people on the other side are dumb. By wrapping technical arguments in snarky comments, we could keep the level of discussion accessible to a wide audience, drive home the key points from both sides, and ensure that we weren't losing audience members who don't care about the PhD-level details as much as they want to hear what their peers are thinking about this exciting new space. I got some feedback afterwards that I didn't seem to hold back, so if anyone did take anything I said seriously, I am very sorry!On a technical level, what was the outcome?It turns out that there was about a 60/40 split between people who felt composability wasn't required yet and those who felt it was after both sides argued their case. Even among panelists, many of us were a lot less convinced about our respective positions than we let on during the panel itself. I got a chuckle when I realized that I wasn't the only one who, when invited to be on the panel, asked \"what side do you want me to argue?\" I honestly could have gone either way because the dust has not yet settled. Dan Stanzione, director of TACC, gave the truest answer to the question of \"will composability help HPC\" up front--\"it depends.\" Maybe this is a growth opportunity, or maybe it's a lukewarm reception.Either way, composable technologies are hitting the market regardless of whether you think they'll be useful or not.  AMD Genoa supports CXL 1.1 with extensions for memory pooling, Samsung has memory-semantic SSDs, and everyone and their mother is working on photonics to get higher bandwidths and lower latencies over longer distances. This makes it easier for people to dip their toes in the water to see if composability makes sense, and I think that's what a lot of people will wind up doing in the coming years.Customer meetingsUnlike in years past, my SC experience this year was dominated by customer meetings. I've been on the customer side of the table plenty of times, but I was surprised to find that it was actually more fun to be on the vendor side for a change. I'm part salesman at heart, so I found it personally gratifying to end a meeting with people nodding along rather than scratching their heads. I learned as a customer that it's very easy for vendors to go way off the rails and waste everyone's time, so I was grateful to have avoided the awkward confusion that punctuates those kinds of meetings. I also went into the week worrying that I'd be sitting in the same room, hearing the same pitch and the same jokes, and answering the same questions all week. Thankfully, I work with some great field, business, and product teams who set up interesting conversations rather than rote recitations of boring roadmap slides. Approaching the same topics from different angles helped me figure out how all the pieces of what I'm working on fit together to make a complete picture too; there weren't nearly as many opportunities to do this in the DOE world since the end-users of the HPC systems on which I worked aren't told anything until all the design decisions have already been made.A few personal notesThis SC was significant to me at a variety of levels; it was the first time I'd gotten on an airplane since February 2020, the first time I'd traveled since starting a new job at a new company, and the first time I'd met any of my new coworkers outside of the structure of a Teams call. During the pandemic I realized that getting out into the world and talking to people from all corners of HPC were my favorite part of my job. Not being able to go to events like SC and maintain that a sense of community involvement dramatically impacted my level of professional satisfaction for the last two years, so I'm glad I was able to finally go this year.Though customer meetings were a lot more fun than I expected them to be, I still felt bummed that I could spend so little time walking the expo, talking to folks, and attending all the BOFs normally on my must-attend list. Compounding this was my personal choice to not dine indoors and consequently miss out on almost all other chances to catch up with old friends and colleagues. I also decided to leave SC a day earlier than I usually do to reduce my risk of getting sick which didn't help either. There's never enough time at SC, but this year was particularly pressed.I say all this not to complain, but to say how much I appreciated the people who went out of their way to come accost me during the precious few hours I actually had on the exhibit floor. Some I'd not seen since SC'19, and some I'd never actually met since we only started working together mid-pandemic. The conference is busy for everyone, so giving me a slice of your time was very meaningful. That sense of community membership is why I go to SC, it's why I still work in this business, and it's why I try to contribute whatever I can to whomever wants it whether it be a student, engineer, salesperson, or marketer.",
            "content_html": "<p>The biggest annual conference in HPC, the <a href=\"https://sc22.supercomputing.org\">SC conference</a>, was recently held in Dallas, Texas in its second hybrid incarnation since being all-remote for the pandemic. This year attracted over 11,000 attendees which is much closer to the pre-pandemic high of 14,000 than last year's 7,000, and judging from the crushed conference rooms and busy expo floor, it looks like SC is not that much worse for wear.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>This year's conference quite different for me since I attended for my first time as a vendor, not a researcher or practitioner, and I spent most of my days behind closed doors talking to customers. I didn't get to attend any of the keynotes, BOFs, or panels to which I wasn't invited as a result, so I'm not really qualified to give an erudite summary of the conference or expo this year.</p><p>So instead, I'm just writing down what I remember in order that I remember it and not necessarily in a coherent narrative form. I'm sure I missed a lot (for example, mixed precision seemed big this year, and I heard Jack Dongarra gave a fantastic Turing Award talk) so I encourage others to write their own recaps and share with the community!<span></span></p><p></p><h2 style=\"text-align: left;\">High-level themes</h2><p>I actually started writing an SC'21 recap last year which I never posted, and re-reading the intro was funny--you'd think nothing has changed in the last year.</p><h3 style=\"text-align: left;\">The underwhelming</h3><p>The biggest deal appears to be that exascale is here, and it turns out that it's not that big of a deal. China let the air out of the tires by debuting their exascale systems at SC'21, and not only did they thumb their nose at Top500 by not submitting, they debuted by winning a Gordon Bell prize instead. The first US exascale system, Frontier, was debuted at ISC this year leaving its showing at SC a bit deflated too. <a href=\"https://www.hpcwire.com/2022/11/17/2022-gordon-bell-prize-goes-to-plasma-accelerator-research/\">Frontier was featured in the Gordon Bell prize-winning paper</a> this year, but that work required the use of four Top-10 systems, not just Frontier, painting the reality that one giant computer rarely stands on its own when it comes to advancing science.</p><p>This isn't to say that deploying exascale systems isn't a noteworthy feat and worth commendation, but I felt like the hype over the last five years treated the achievement like an end state instead of a milestone. And now that we've passed the milestone, the community is grasping to figure out what comes next. So what <i>is</i> next?</p><p><b>Quantum</b> had a strong and growing presence at SC, as it has for the last few years. But the conclusion of the panel \"<a href=\"https://www.hpcwire.com/2022/11/19/quantum-are-we-there-or-close-yet-no-says-the-panel/\">Quantum Computing: A Future for HPC Acceleration</a>\" was that no, it's not close to being ready.</p><p><b>Disaggregation and composability</b> was another theme with growing momentum. And like quantum, there was a panel asking the same question: \"<a href=\"https://www.hpcwire.com/off-the-wire/informal-poll-of-sc22-attendees-suggests-a-bright-future-for-composability/\">Does HPC need composability now?</a>\" The answer, again, was no, not yet. More on that below.</p><p>What about <b>RISC-V</b>? Surely that will revolutionize the field. As it turns out, the answer there is also that <a href=\"https://www.hpcwire.com/2022/11/18/risc-v-is-far-from-being-an-alternative-to-x86-and-arm-in-hpc/\">RISC-V is not ready to do anything useful for HPC yet</a>.</p><p>The list goes on of technologies and trends that people are trying to boost now that exascale is \"solved.\" The reality, I think, is that \"exascale\" will take years to actually mature since it appears to have a ton of technical debt that accumulated during the race to be first. US Exascale rests on the shoulders of AMD and Intel, two companies whose software stacks have not caught up to the market leader, so there will be a lot of thrashing around as development practices and optimization settle out around these systems.</p><p>Struggling with code porting is not very exciting to computer science Ph.D.s, so I expect future SCs to mirror this one and bifurcate into two distinct tracks: those struggling to identify the next big thing in the research space, and those struggling to use the systems that were rushed to deployment.</p><h3 style=\"text-align: left;\">The unexpected</h3><p>My SC experience was very biased since I didn't get out much, but two related themes kept popping up across different meetings and the sessions I did attend.</p><p><b>Power efficiency is serious business now</b>. It used to seem like people talked about the need for energy-efficient HPC in an abstract sense while continuing to jam more power into every rack without changing their approach to system design, facilities, and deployment models. That has hit a hard wall with energy prices soaring in Europe, though. The financial impacts of power-inefficient supercomputing have gone from a one-time capex cost to an ongoing opex cost that is putting many HPC facilities on an unsustainable cost trajectory. Even sites that aren't doing new deployments are facing sudden, sharp increases in their costs, and nobody has good answers about how they will keep the lights on.</p><p><b>Cloud HPC is confusing</b>. With only <a href=\"https://www.nextplatform.com/2022/11/08/hpc-follows-the-enterprise-into-the-cloud/\">15% of total HPC dollars winding up in the cloud</a>, it's little surprise that most HPC folks are only peripherally aware of what HPC in the cloud really means. Worse yet, a subset of those folks are actively hostile towards the idea of running HPC workloads in the cloud. I spoke with my colleagues from all three major cloud service providers as well as my colleagues in DOE, NSF, and education throughout the week, and everyone painted this same general picture.</p><p>There seems to be a mismatch between the expectations of on-prem HPC folks and cloud HPC folks. For example, I was asked why Windows doesn't support OpenMP very well, and after a bit of digging, I realized that the question really wasn't about using OpenMP on Windows as much as it was about using OpenMP in the cloud. There was a latent assumption that \"HPC in Microsoft's cloud\" must mean \"HPC on Windows\" which, for the record, is false--I don't even know how to use Windows anymore. Similarly, people decried the performance impacts of sharing HPC nodes with others in the cloud (they are not shared), overheads of virtualizing InfiniBand or GPUs (everyone uses PCIe passthrough or SR-IOV for HPC nodes), and other misconceptions.</p><p>This isn't to say that cloud people aren't confused too; I heard stories about conversations that went sideways because a cloud folks (not from my employer, thankfully!) didn’t realize that the requirements of a traditional gov/edu HPC facility couldn’t be neatly wrapped up into a single workload with a single solution, contrary to the case across many commercial AI shops. And both sides are struggling to find models for partnership and engagement that mirror the traditional relationship between places like a DOE or NSF facility and a company like Cray. HPC departments are used to buying supercomputers and parallel file systems, while cloud providers sell computing and storage as a <i>service</i>. The distinction may seem trivial at the surface, but there's a large divide that becomes evident once both sides start trying to drill into the details of what a partnership would look like.</p><h2 style=\"text-align: left;\">Parallel I/O in Practice Tutorial</h2><p>This was my fifth year contributing to the Parallel I/O in Practice Tutorial with my colleagues at Argonne and Google, and it was our first time doing it in-person since 2019. It felt really good to be back in front of people to opine about the perils of POSIX and the greatness of the <a href=\"https://www.mcs.anl.gov/research/projects/darshan/\">Darshan I/O profiling tool</a>, and this year I retired out the material I used to present on burst buffers (since DataWarp and Infinite Memory Engine have lost relevance in HPC) and the <a href=\"https://www.nersc.gov/tokio/\">TOKIO holistic I/O analysis framework</a> (since it is no longer funded/maintained). In their stead, I presented material on <a href=\"https://wiki.lustre.org/Lustre_User_Group_2022\">benchmarking with IOR and mdtest I debuted at LUG 2022 this year</a>.</p><p>I haven't gotten feedback yet on whether this change was a net positive one, but I think it went over well. Benchmarking I/O is really challenging if you don't understand how things like page cache really work in distributed systems, and walking through some benchmark examples concretizes a lot of abstract parallel file system concepts like locking and striping. And since benchmarking is a rabbit hole of arbitrary complexity, ending the tutorial with advanced benchmarking topics turned out to be a nice way to add buffer to the end of an eight-hour stretch of carefully timed presentations. It's very easy to skip over the nuances of analyzing mdtest outputs if attendees have a lot of questions about more important things at the end of the day.</p><p>The most surprising observation of the tutorial is how many attendees aren't using MPI anymore. We got a lot of questions last year about task-oriented I/O, and this year had some great questions about trying to understand or tune the I/O performed by Python-based analytics frameworks. We decided to add support for <a href=\"https://www.mcs.anl.gov/research/projects/darshan/2019/12/11/new-experimental-version-of-darshan-available-for-instrumenting-non-mpi-applications/\">Darshan to profile non-MPI applications back in 2019</a> which is now paying dividends by ensuring it is a relevant tool for these new analytics and AI workloads, and we'll probably have to give more attention to optimizing these workloads' I/O in the future.</p><h2 style=\"text-align: left;\">DAOS User Group</h2><p>Monday morning was cold and rainy--a perfect day to attend the <a href=\"https://daosio.atlassian.net/wiki/spaces/DC/pages/11248861216/DUG22\">2022 DAOS User Group</a> which was held off-site at the Fairmont Hotel.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>Whether you particularly care about DAOS or not, the cross-community HPC I/O brain trust is guaranteed to be in attendance, and this year did not disappoint. In addition to the expected stakeholders from Intel and DOE, representatives from all three big CSPs were in attendance. Google Cloud, Seagate, and HPE/Cray were all on the agenda, painting a diversifying landscape of large HPC companies investing time into DAOS and the strength and willingness of the DAOS team to partner with all comers.</p><h3 style=\"text-align: left;\">Life after Optane</h3><p>The question that opened up the meeting, of course, was \"what is the future of DAOS since Intel cancelled Optane?\" Kelsey Prantis had the official statement (I'll replace the grainy photo once the DUG slides are online...):</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>The high-level project answer is that DAOS isn't going anywhere. Aurora, by virtue of still having Optane DIMMs, will not be affected, and DAOS will maintain support for Optane until Intel drops its last Optane DIMMs (Crow Pass for Sapphire Rapids) from support life sometime towards the end of this decade.</p><p>For new customers who aren't going to use Optane, the answer is \"<a href=\"https://daosio.atlassian.net/issues/?jql=labels%20%3D%20%22md_on_ssd%22\">Metadata on NVMe</a>,\" a development being codeveloped by Intel, HPE, and Google to implement a write-ahead log (WAL) and allow DAOS to use volatile DRAM instead of Optane. It will work like a file system journal in that a compact representation of writes will be committed to NVMe immediately after landing in DRAM, and then DAOS will asynchronously write back the properly serialized representation of that transaction after it is acknowledged. Johann Lombardi had a helpful cartoon that showed how this WAL will fit into DAOS:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>A key benefit of DAOS's implementation of this WAL is that it will be able to still service incoming writes while flushing old writes; although I don't fully grasp how this works, it is something enabled by the sophisticated I/O scheduler already implemented in DAOS.</p><p>The complete implementation isn't expected to be released until Spring 2024, but it appears to touch only a few components of DAOS and doesn't affect anything above the VOS layer of the DAOS server.</p><p>There was also mention of developing operability with new <a href=\"https://news.samsung.com/global/samsung-electronics-unveils-far-reaching-next-generation-memory-solutions-at-flash-memory-summit-2022\">CXL-attached memory-semantic SSDs</a> to keep the persistent memory capability of DAOS alive beyond Optane. I'm not sure if this would offer a performance benefit over the metadata-on-NVMe feature; early results show that metadata-on-NVMe actually delivers higher IOPS than Optane since the synchronous write path is much simpler without having to account for memory persistence. That said, I didn't really follow the full extent of options on the table for how DAOS metadata may work across different types of memory though.</p><h3 style=\"text-align: left;\">DAOS in the flesh at Argonne</h3><p>Kevin Harms presented an update on Aurora's massive 220 PB DAOS installation and laid out its configuration. There are 1,024 DAOS servers based on the Intel Coyote Pass server design, each sporting</p><p></p><ul style=\"text-align: left;\"><li>2x Intel Xeon 5320 (Ice Lake) sockets</li><li>2x DAOS engines (one per socket)</li><li>16x 32GB DDR4 DIMMs</li><li>16x 512GB Optane DIMMs (Persistent Memory 200)</li><li>16x 15.36 TB Samsung PM1733 NVMe SSDs</li><li>2x 200 Gb/s Slingshot NICs</li></ul><p>The total configuration is quoted at 220 PB usable, but Kevin pointed out that this assumes that every object is erasure coded at 16+2. Unlike virtually every other storage system out there, though, users can choose the data protection for their individual objects when they create them, meaning this 220 PB capacity is an upper limit to what users can do. Users with very hot, read-only objects may choose to replicate instead of erasure code, while others who are capacity-constrained may choose to erasure code everything at 16+2 at the cost of latency and IOPS. This flexibility is really powerful for users since they can tailor their object layout (\"<a href=\"https://www.intel.com/content/www/us/en/developer/articles/technical/understanding-data-redundancy-and-sharding-in-daos.html\">object class</a>\" in DAOS parlance) to match the needs of their workload.</p><p>Argonne will be slicing up this DAOS system by giving each scientific project its own DAOS pool, and each pool will be assigned to only 80% of the available DAOS servers by default. This seems like a nice way of providing most of the storage system performance to every user, but offering more freedom to work around bad hardware, bad users, and other performance problems that plague file systems like Lustre that distribute everything across every single server equally.</p><p>Finally, I noticed that Aurora will be using Samsung SSDs, not the Intel (now Solidigm) QLC NAND that appeared in all the DAOS slides floating around two years ago. I'm not sure what happened there, but the move from Solidigm QLC to Samsung TLC couldn't have been cheap.</p><h3 style=\"text-align: left;\">New features and contributions</h3><p>DAOS is starting to pick up some truly valuable features that are being developed and contributed by third parties. Of note, croit has contributed a feature which allows DAOS to serve up NVMe over Fabrics targets, and Seagate contributed an S3 gateway for DAOS. Along with the DFS file system interface, DAOS now offers the trifecta of standard object, block, and file services just like Ceph. Unlike Ceph though, performance on DAOS is a first-class citizen. While croit made it clear that the NVMeoF support still has a ways to go to improve the way it does thread pooling and provides resilience, they showed 1.4 million IOPS from a single storage client using TCP over Ethernet with minimal client-side overhead.</p><p>Intel is also developing multitenant support for DFUSE, allowing a single compute node to share a DAOS mount and let permissions be enforced through UID/GID just like a regular file system. Before this update, the FUSE-based nature of DAOS allowed any unprivileged user to mount their container (good), but only one FUSE agent could be alive on a single node at a time (not good) which prevented multiple users sharing a node from both mounting their own containers.</p><p>DAOS also has some longer-term enhancements that I thought were interesting:</p><p></p><ul style=\"text-align: left;\"><li>expanding the range of POSIX calls supported by DAOS's intercept library to include metadata calls and memory-mapped I/O using <a href=\"https://docs.kernel.org/admin-guide/mm/userfaultfd.html\">userfaultfd</a></li><li>implementing collaborative caching - essentially reimplementing the Linux kernel page cache in userspace so that multiple processes can share cached DAOS pages</li><li>supporting a computational storage paradigm by enabling offload of <a href=\"https://github.com/rlane/ubpf\">userspace eBPF scripts</a> to DAOS servers</li></ul><h3 style=\"text-align: left;\">DAOS in a larger data center ecosystem</h3><p>Dean Hildebrand from Google Cloud then gave an overview of Google's efforts in bringing DAOS into the cloud. He had some nice performance graphs and I'll link the full presentation here once it's uploaded (it's worth a watch), but the part I found the most insightful was how they are trying to decide where a technology like DAOS fits in the larger cloud storage ecosystem. He outlined two different ways DAOS could work in GCP:</p><p></p><ol style=\"text-align: left;\"><li><b>Caching</b>: Google Cloud Storage (GCS) is the point of truth and DAOS is a cache</li><li><b>Tiering</b>: DAOS is a point of truth, and GCS is an archive</li></ol><p></p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>He said they were leaning towards the caching model where data only lives ephemerally in DAOS, and personally, I think this is the right move since DAOS in the cloud is not resilient without Optane. However, this choice reflects a much larger tension in cloud storage for HPC:</p><p></p><ol style=\"text-align: left;\"><li>The centerpiece of every cloud's data story is a scalable, low-cost, low-performance object store which is analogous to what on-prem HPC would call campaign, community, or project storage.</li><li>HPC demands higher performance than what these object stores can generally deliver though.</li></ol><div>To bridge the gap between these two truths, auxiliary services must bolt on to the object layer and provide higher performance, at a higher cost, for the duration of I/O-intensive HPC jobs. Some choose to provide true tiering from object into a resilient layer of flash (like <a href=\"https://aws.amazon.com/fsx/lustre/\">FSx Lustre</a> and <a href=\"https://docs.weka.io/overview/data-storage\">Weka</a> do), while others project the contents of the object through a high-performance caching layer (like <a href=\"https://azure.microsoft.com/en-us/products/hpc-cache/#overview\">HPC Cache</a> and <a href=\"https://aws.amazon.com/blogs/aws/amazon-file-cache-a-high-performance-cache-on-aws-for-your-on-premises-file-systems/\">File Cache</a>) and are never meant to persistently hold data.</div><p></p><p>This isn't rocket science, but I never thought deeply about the two models since campaign/community/project storage in on-prem HPC is usually fast enough to avoid needing caches or fine-grained tiering capabilities.</p><p>John Bent also had a thought-provoking presentation about how Seagate's now-\"deprioritized\" CORTX object store, which once <a href=\"https://blog.seagate.com/enterprises/seagate-and-sage-project-innovate-to-boost-hpc-and-big-data-community/\">competed with DAOS as Mero</a>, contains ideas that can complement DAOS:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>Whereas DAOS delivers high performance using NVMe, CORTX delivers great economics using HDDs, and their strengths are complementary to each other. While I don't fully grasp how a tiered (or caching!) system comprised of DAOS and CORTX could be implemented, John rightly pointed out that the same level of space efficiency can deliver higher data protection if multi-level erasure coding is used to stripe across durable block storage. His specific example was erasure coding at 8+1 across servers and 10+1 within servers to deliver both high efficiency and high durability. This could map to something like running DAOS atop something like CORVAULT, but I don't think all the necessary pieces are in place to realize such a harmonious coexistence yet.</p><p>Of course, completely tossing Reed-Solomon for something more sophisticated (like VAST does with its locally decodable 150+4 scheme) obviates the need for multilevel erasure entirely. But DAOS has not gone down that route yet.</p><p>And as with every talk John gives, there were lots of other interesting nuggets scattered throughout his presentation. Two of my favorites were:</p><p></p><ul style=\"text-align: left;\"><li>A slide that pointed out that, when you buy something like Ceph as an appliance, you may be spending only 25% of the total cost on storage media and the rest is infrastructure, service, and support. This struck me as a bit on the low end, but some enterprisey NAS and midrange parallel file system appliances can go this low. Spending 60% to 90% on media is a lot nicer for the buyer (and companies like Seagate) if you can buy at scale or eschew the white-glove support, and John suggested that it's up to companies like Seagate to fix the software issues that require customers to pay for white-glove support in the first place.  After all, the less someone spends on support and licenses, the more they can spend on Seagate hard drives.</li><li>John's final slide pointed out that object stores were originally designed to get around the limitations of POSIX file systems, but as they've evolved over the last decade, they're starting to look a lot like file systems anyway since they require strong consistency, hierarchical namespaces, and familiar file semantics. Has all the work put into developing super-fast object stores like DAOS over the last ten years really just brought us back full circle to parallel file systems?  Companies like VAST and Weka have shown that <a href=\"https://www.nextplatform.com/2017/09/11/whats-bad-posix-io/\">maybe POSIX isn't as bad as the research community (myself included!) have claimed it to be</a>; it was really just low-performance implementations that nobody wanted.</li></ul><div>Once John's talk is uploaded to the DUG 2022 website, I'll link it here.  Like Dean Hildebrand's talk, it is well worth watching (but for wildly different reasons!)</div><p></p><p></p><p></p><h2 style=\"text-align: left;\">PDSW 2022</h2><p>I had to duck out of the DAOS User Group early to run (through the rain) to the 7th International Parallel Data Systems Workshop (PDSW 2022) on Monday afternoon.</p><p></p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p><br />Much to everyone’s surprise, PDSW was only given a half day this year and everything felt a little compressed as a result. The organizers kept the work-in-progress (WIP) sessions which can often be an interesting peek into what students are pursuing, but little A/V problems and the unforgiving schedule probably did a disservice to the up-and-comers who use the WIP track to lay the groundwork for future full-length papers. Hopefully SC’23 restores PDSW to its original full-day status.&lt;p&gt;&lt;/p&gt;</p><h3 style=\"text-align: left;\">Splinters keynote from Arif Merchant at Google</h3><p>The keynote presentation was given by Arif Merchant from Google about Splinters, the framework that Google Cloud uses to sample I/Os in a scalable way. The challenge they face is that it's impossible to trace and store every single I/O that hits Google's storage servers (D servers), but having an understanding of I/O patterns is essential for characterizing workload I/O behavior and planning for future infrastructure. In fact, this problem is so important that Google isn't the only cloud that's solved it!</p><p>A lot of what Arif talked about is very similar to how Azure does its I/O tracing under the hood. I suppose it should not be surprise that there are only so many ways to solve the challenge of sampling individual IOPS in a way that fairly represents the aggregate workload of a huge distributed storage system. One really smart thing Splinters does that I liked was sample along two different dimensions: not only do they evenly sample across all IOPS at a fixed rate (the obvious thing), but they also sample across files at a fixed rate. In this latter case of per-file sampling, they take a tiny fraction of files and capture every I/O for that file to get a complete picture of how individual files are being accessed.</p><p>This file sampling fills the huge gap that exists when randomly sampling IOPS alone. Because different I/Os have different \"costs\" (for example, reading a 1 MiB file using a single 1 MiB read op or 256x 4 KiB read ops are functionally equivalent to an application), randomly sampling ops introduces systematic biases that can be difficult to back out after the data has been sampled, subsampled, aggregated, and reduced. Splinters' approach lets you see the workload from two different angles (and biases) and answer a much larger range of questions about what's really happening across thousands of storage servers.</p><p>That said, it was interesting to hear Arif describe how Splinters evolved out of a different internal Google project but wound up outliving it. Splinters is also similar to, but slightly different from, their <a href=\"https://research.google/pubs/pub36356/\">Dapper</a> infrastructure which also does scalable distributed system tracing. And he made overtures to <a href=\"https://research.google/pubs/pub41344/\">F1</a>, a scalable SQL database that is similar to (but not the same as) the SQL-like query interface that Splinters uses. I got the impression that new technologies come and go pretty quickly at Google, and there's a large appetite for creating new software systems outright rather than shoehorning an existing system into solving a new problem. I can't say one way is better than the other; I was just surprised at the contrast with my own experiences.</p><h3 style=\"text-align: left;\">Practical papers</h3><p>PDSW had a healthy combination of both very-researchy papers and applied research papers this year. I could only stick around for the applied papers, and two left an impression.</p><p>In the first, <a href=\"https://jeanlucabez.io\">Jean Luca Bez</a> presented <a href=\"https://github.com/hpc-io/drishti\">Drishti</a>, a tool that lives downstream of the Darshan I/O profiling library and finally does what the Darshan community has danced around for years--turning a Darshan log into an actionable set of recommendations on how to improve I/O performance. It does this by cataloguing a bunch of heuristics and using Darshan's new Python integrations to pore through a log and identify known-problematic I/O patterns. Like Jean Luca's <a href=\"https://dxt-explorer.readthedocs.io/en/latest/\">DXT Explorer tool</a>, Drishti has a slick user interface and greatly extends the usability and insights that can be pulled out of a Darshan log file. It probably won't win a Turing Award, but this sort of work is probably going to benefit scores of HPC end-users by making Darshan (and troubleshooting I/O problems) much more accessible to mere mortals for years to come.</p><p>Adrian Jackson also presented a very tidy <a href=\"https://arxiv.org/abs/2211.09162\">apples-to-apples comparison of DAOS and Lustre on the same hardware</a> using both a systems-level benchmark and an application-inspired, object-oriented data model benchmark. The specific bake-off of a new curiosity (DAOS) and the decades-old incumbent (Lustre) is probably interesting to storage nerds, but I think the real novelty of the work is in its exploration of some uncomfortable realities that the HPC I/O community will have to face in the coming years:</p><p></p><ul style=\"text-align: left;\"><li>Does \"slow memory\" (nonvolatile Optane or CXL-attached memory SSDs) give actual benefit to existing file systems (like Lustre), or is rethinking the entire storage stack (like DAOS did) really necessary to unlock the performance of new hardware?</li><li>Do applications need to rethink their approach to I/O to make use of post-POSIX storage systems like DAOS, or is performing I/O as you would on a file system (Lustre) on a post-POSIX storage system (DAOS) good enough?</li></ul><p>My take from the work is that, for simple I/O patterns like checkpoint/restart, you can get pretty far by just treating something like DAOS the same as you would a parallel file system:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><b><span style=\"font-size: x-small;\">Figure from Manubens et al, \"<a href=\"https://arxiv.org/abs/2211.09162\">Performance Comparison of DAOS and Lustre for Object Data Storage Approaches</a>.\"</span></b></div><p>But if you want your data at rest to have the same data model as how it's handled within the application, you really ought to use a storage system that supports data models that are more expressive than a stream of bytes (which is what POSIX files are).</p><p>The authors didn't do a perfect job of giving Lustre its fair shake since they chose to use (abuse) directories and files to represent their application's data model on-disk instead of developing an object-file model that file systems like Lustre handle a little better. But let's be real--HPC is full of applications that do the exact same thing and represent datasets on-disk using complex hierarchies of directories and files simply because that's the easiest way to map the application's representation of data into the standard file system model. In that sense, storage systems that represent rich data models in a high-performance way should be really valuable to naive applications that map in-memory data structures directly to files and directories.</p><p>Going back to John Bent's closing slide from his DAOS User Group talk, though, does any of this even matter since all answers lead back to parallel file systems? Maybe there's something to be learned about adding better back-door APIs that support more diverse data models than what POSIX file interfaces give us.</p><h2 style=\"text-align: left;\">The SC22 Expo</h2><p>The expo is my favorite part of SC because it's when I get to talk to people one-on-one and learn about corners of the HPC industry that I would've never otherwise sought out. Much to my dismay, though, I had very little time to walk the floor this year--so little that I didn't get any swag. If you want to read up on what interesting technology was being showcased, I strongly recommend reading <a href=\"https://www.servethehome.com/?s=sc22\">all the great content that Patrick Kennedy and his team at STH created covering the expo</a>.</p><p>That said, I did notice some curious trends about the show floor overall.</p><p>The NVIDIA booth was notably absent this year (though they shared booth space with partners), and many of the usual top vendors had significantly smaller presence on the expo floor. Just for fun, I compiled the top ten(ish) vendors by booth size:</p><p></p><ol style=\"text-align: left;\"><li>Weka.io (3,200 sqft)</li><li>VAST Data, Department of Energy, Penguin Computing, HPE, and Microsoft (2,500 sqft)</li><li>AWS (2,000 sqft)</li><li>Google and TACC (1,600 sqft)</li><li>Supermicro, AMD, Intel, Dell, NASA, and Indiana University (1,500 sqft)</li></ol><p>I think it's amazing to see all-flash storage companies at the top of the list alongside all of the Big 3 cloud service providers. I may be reading too much into this, but this may mean that the money behind SC is shifting towards companies playing in the cloud-based AI space instead of traditional big iron for simulation. Or perhaps it's a sign that most of the traditional HPC players are taking a hard look at the return they get on a big booth given the current economic climate and pulled back this year.</p><p>I did chat with a couple colleagues who completely opted out of a booth this year (for reference, <a href=\"https://hallerickson.ungerboeck.com/prod/app85.cshtml?AppCode=VFP&amp;OrgCode=34&amp;EvtID=5025&amp;CC=SC22SM\">SC'21</a> had 10% fewer exhibitor booths than <a href=\"https://hallerickson.ungerboeck.com/prod/app85.cshtml?AppCode=VFP&amp;OrgCode=34&amp;EvtID=5020&amp;CC=SC19\">SC'19</a>), and the reasoning was consistent: they found more value in having staff meet with customers privately or attend the technical sessions and engage with people organically. Combined with a bit of bad taste left over from SC's <a href=\"https://sc21.supercomputing.org/exhibits/exhibit-at-sc/\">high cost of hosting pandemic-era \"digital booths\"</a> despite low return (did anyone visit digital booths at SC'20 or SC'21?), I can see why some vendors may have chosen to skip the expo this year.</p><p>Whatever the reasons may be, I was a bit sad to see such a small presence from some of my favorites like IBM, Fujitsu, Atos, and NEC. Hopefully the SC Exhibits Committee (and the economy!) can find ways to bring back the pre-pandemic glory of the show floor.</p><p>The expo wasn't all doom and gloom though! Even though I couldn't make my complete rounds this year, there were a couple of highlights for me.</p><p></p><h3 style=\"text-align: left;\">VAST's masterful marketing</h3><p>Perhaps the splashiest vendor at SC was VAST Data who had a brilliant marketing presence. First was the giant Vastronaut mascot that was the centerpiece of their booth:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>A <a href=\"https://twitter.com/search?q=sc22%20vast&amp;f=live\">quick search of Twitter</a> shows just how many people seized the opportunity to take a selfie at their booth. I would love to know how they transported that thing to and from the conference, but whatever the cost, I'll bet it was worth it.</p><p>At the Grand Opening Gala on Monday, they also gave out delightfully tacky light-up cowboy hats that everyone seemed to be wearing:</p><blockquote class=\"twitter-tweet\"><p dir=\"ltr\" lang=\"en\">We were there! <a href=\"https://twitter.com/hashtag/sc22?src=hash&amp;ref_src=twsrc%5Etfw\">#sc22</a> <a href=\"https://twitter.com/hashtag/sc2022?src=hash&amp;ref_src=twsrc%5Etfw\">#sc2022</a> <a href=\"https://twitter.com/VAST_Data?ref_src=twsrc%5Etfw\">@VAST_Data</a> <a href=\"https://t.co/fWhuSgBfpL\">pic.twitter.com/fWhuSgBfpL</a></p>— ntnu-hpc (@ntnuhpc) <a href=\"https://twitter.com/ntnuhpc/status/1592330266932301829?ref_src=twsrc%5Etfw\">November 15, 2022</a></blockquote><p>The subtle genius of this was that not only did people wear them during the gala and the <a href=\"https://beowulfbash.com\">Flop Gun-themed Beowulf Bash 2022 party</a> later that night, but they had to wear them on their plane rides home since they were so inconveniently bulky. Proof in point, my wife (who doesn't work in tech) sent me this text message to confirm that she was waiting for me at the right luggage carousel at San Francisco Airport:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>I wonder how many innocent bystanders, traveling home for Thanksgiving on Thursday or Friday, saw the shiny cowboy hats at airports around the country and wondered what VAST was.</p><p>The icing on the cake was VAST's CEO, Renen Hallak, parading around in an unmissable Chuck McGill-style space suit all week, clearly not taking himself too seriously and painting VAST as a work hard/play hard kind of company. Now, do flashy space suits and blinking cowboy hats alone mean VAST has a great product? I can't say<sup>**</sup>. But marketing is an art that I appreciate, and VAST hit some great notes this year.</p><p style=\"font-size: xx-small;\"><sup>**</sup> (Seriously, I'm not sure I wouldn't get in trouble for opining about another company here.)</p><h3 style=\"text-align: left;\">The Microsoft hardware bar</h3><p>The only booth where I spent any appreciable time this year was my own employer's. I personally love booth duty and accosting strangers on the show floor, especially if there's something interesting at the booth to jumpstart a conversation. When I worked at SDSC it was a <a href=\"https://www.sdsc.edu/News%20Items/PR111213_meteor.html\">Raspberry Pi cluster</a>, and at the Microsoft booth this year it was the \"hardware bar.\"</p><p>In addition to the customary booth presentations with giveaways, swag desk, seating area, and a fun caricature artist, the physical servers that underpin the HPC nodes in Azure were on display. <a href=\"https://www.opencompute.org/wiki/Server/ProjectOlympus\">Microsoft contributes its hardware platform designs to the Open Compute Project</a> so the physical hardware that runs in Azure data centers isn't entirely mysterious. Still, every cloud has its hardware secrets, so I was surprised to see these servers laid bare.</p><p>The newest HPC node type (dubbed <a href=\"https://learn.microsoft.com/en-us/azure/virtual-machines/hbv4-series\">HBv4</a>) on display was a node powered by AMD's Genoa processors just announced a few days earlier:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>This wasn't a display model, either; it had real DDR5 DRAM, a real NDR InfiniBand HCA, real PCIe Gen5, and real big OCP mezzanine card with real big aluminum heat sinks and a big Microsoft sticker on top. A couple visitors commented on the way the heat piping for those Genoa CPUs was done which I guess is unusual; rather than have a giant copper block on top of each socket, heat pipes connect the socket to massive aluminum heat sinks that are closer to the chassis inlets. In retrospect it makes sense; Genoa has a whopping twelve DDR5 DIMMs per socket which leaves little extra room for heat sinks, and these 88+ core sockets have a staggering thermal design power.</p><p>Another exotic piece of hardware on display was an \"ND MI200 v4\" server:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>It's logically similar to Azure's \"<a href=\"https://learn.microsoft.com/en-us/azure/virtual-machines/nda100-v4-series\">ND A100 v4</a>\" server platform with two CPU sockets, eight SXM4 GPU sockets, eight 200G HDR InfiniBand HCAs, and a bunch of M.2 NVMes. But this specific server has eight MI200 GPUs on a common OAM baseboard and uses Infinity Fabric for GPU-to-GPU communication. I've never seen an OAM-socketed anything in real life before, much less eight of them on a baseboard, so I thought this was pretty great to see in the flesh.</p><p>The ND A100 v4 platform was also on display and looked very similar-but-different with its eight A100 GPUs and HGX baseboard:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>And unlike the MI200 variant, the general public can run on these nodes.</p><p>I'm not sure what more I'm allowed to say, but my colleague Karl made a nice, <a href=\"https://twitter.com/KarlPodesta/status/1593627537330126851?s=20&amp;t=uthjeb7YYmTZWRVWaF4XUA\">quick video that runs through the entire Microsoft booth</a> that's worth a watch, and more details can be had by contacting me or your favorite Microsoft account team privately.</p><p>Of course, the hardware bar was just a way to lure people into the booth so I could achieve my real goal: meeting new folks. As I wrote before, one of my biggest realizations at SC this year is how generally confused people are about what HPC in the cloud really means--both people who come from traditional on-prem HPC and people who come from traditional enterprisey cloud. I found myself surprising many of the people with whom I spoke on the show floor with factoids that I have taken for granted. For example,</p><p></p><ul style=\"text-align: left;\"><li>Linux is the most common OS on these HPC node types. While you probably(?) can run Windows if you want on this stuff, I think only a few niche markets do this.</li><li>The usage model for an HPC cluster in the cloud can be the same as on-prem. You can have login nodes, Slurm, home directories, parallel file systems, and all that. Jobs don't have to be containerized or turned into a VM image.</li><li>The InfiniBand coming out of these nodes is real InfiniBand with real OFED that supports real mpich/mvapich/OpenMPI. It's the same stuff as in on-prem supercomputers. And nodes are assembled into <a href=\"https://learn.microsoft.com/en-us/azure/virtual-machines/sizes-hpc\">full-bisection fat tree InfiniBand</a> clusters just like normal.</li><li>There's no noisy neighbor problem on compute nodes because HPC node types aren't shared between users. When you run a VM on an HPC node, you get the whole thing. Just like on large supercomputers.</li><li>There's no horrible loss of performance due to running in a VM. Virtualization extensions, PCIe passthrough, and SR-IOV bypass the hypervisor for most things. Inside your VM, you see real Zen cores and real Mellanox HCAs, not virtualized devices.</li></ul><p>My takeaway impression is that a lot of traditional HPC folks looked at the cloud five or ten years ago, had a sour experience, and haven't paid attention since. In those last five years, though, AI has changed the game. Massive demand for the latest CPUs and accelerators, funded by live-fast-die-young venture capital, has given cloud vendors tremendous financial incentive to catch up to on-prem levels of performance efficiency for AI workloads. And it just so happens that infrastructure that's good for AI is also good for traditional modeling and simulation.</p><h2 style=\"text-align: left;\">SCinet!</h2><p>One of the unexpected highlights of my SC this year arose from a chance encounter with a former coworker from NERSC, <a href=\"https://www.nersc.gov/about/nersc-staff/networking-security/ronal-kumar/\">Ron Kumar</a>, who gave me a whirlwind tour of SCinet.</p><p>I have to confess great ignorance around SCinet in general; I always saw it was a weird technological proof of concept that the strange networking people at work would go off and do in the weeks leading up to the actual conference. I knew they did some impressive wide-area transfer demos (like the <a href=\"https://scinet.supercomputing.org/community/documents/43/sc17-Kettimuthu-transferring_1petabyte_per_day.pdf\">petabyte-in-a-day demo at SC'16</a>), but I didn't really get the significance.</p><p>So what is SCinet? It's this yellow bundle of cables dangling from the ceiling.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p><br />&lt;p&gt;The yellow cables are 144-core fiber trunks that bring over a terabit per second of bandwidth into the convention center from the Internet via the national research backbones like ESnet and Internet2 and distribute many terabits per second of capacity throughout the SC conference venue. For comparison, most HPC centers in the US only have a tenth of SCinet’s wide-area bandwidth at best since 400G infrastructure is still rolling out.&lt;/p&gt;</p><p>Most attendees may be familiar with the row of expensive-looking networking racks behind a glass wall towards the back of the expo which is where those yellow cables dangling from the ceiling end. Here's a photo from inside that glass wall:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>What I didn't realize is that if you go around to the back of the giant walled area behind this glass display, there's a security checkpoint that gates entry into a massive network operations center (NOC) full of laptops, spools of fiber, meeting rooms, and busily working teams in charge of all the lower layers of the networking stack.</p><p>The process to get into the NOC involves an escort and being tagged in with a tamper-proof wristband, and I learned on the tour that there's millions upon millions of dollars worth of high-end networking equipment in the racks shown above. If you look closely, you can see a security camera at the end of the aisle that speaks to this; that camera was one of many.</p><p>Behind the pretty public-facing side of the SCinet racks is a mess of fiber and cables:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>I guess if you have to tear all this down after just a few weeks, there's no point in investing days in dressing it all up nicely! I particularly enjoyed the fiber panels in the third rack that appear to be affixed to the rack post with shoe laces.</p><p>This year, SCinet did do a neat proof-of-concept where they demonstrated three 400G routers from three vendors (Juniper, Arista, and Cisco?) all talking the same protocol to handle what I assume is the core routing for everything in the convention center:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>I wish I remembered exactly what was going on here, but I know enough about networking to know that, despite there being standard protocols for coordinating between networking gear, each vendor does their own implementation that is rarely easy to get interoperability from. If anyone out there knows the details of this achievement, please let me know so I can explain this a little better!</p><p>In addition to networking nerd-level demonstrations, SCinet also serves up all the wifi across the convention center. That is why there were tripods with access points scattered around, and why astute attendees may have noticed janky networking equipment scattered around that looked like this:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>Again, I get it: for a network infrastructure that's only going to last a week, I don't think it's a good use of anyone's time or money to nicely dress all the networking.</p><p>One last factoid I didn't know until this year was that exhibitors can request 100 Gb/s network drops into their individual booths for demos (or downloading the latest version of a PowerPoint presentation <i>really fast</i>). The end result of supporting both a vast wifi network and 100G fiber across the show floor is that there was a <u>lot</u> of fiber going into the single row of SCinet equipment:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>Finally, when I <a href=\"https://twitter.com/glennklockwood/status/1592725187015114752?s=61&amp;t=1c4Kbx75SpTJhCruzuy0Ng\">posted some of these photos online</a> during the conference, my colleague Bilel was kind enough to post a slide from the SC22 opening presentation that had the speeds and feeds of what I had toured:</p><blockquote class=\"twitter-tweet\"><p dir=\"ltr\" lang=\"en\">Candy Culhane shared Scinet facts <a href=\"https://twitter.com/hashtag/SC22?src=hash&amp;ref_src=twsrc%5Etfw\">#SC22</a> <a href=\"https://twitter.com/hashtag/HPC?src=hash&amp;ref_src=twsrc%5Etfw\">#HPC</a><br /><br />5.01 Tb/s of WAN capacity<br />$70M in HW &amp; SW, &amp; services provided by 29 SCinet contrib.<br />175 volunteers from 80 vol. organiz.<br />&gt; 450 wireless deployed<br />29 network research exhibition proposals<br />11.7 miles of fiber <br />2384 fiber patch <a href=\"https://t.co/JtPhjVHZJd\">https://t.co/JtPhjVHZJd</a> <a href=\"https://t.co/kwGl5Ydqp5\">pic.twitter.com/kwGl5Ydqp5</a></p>— Bilel Hadri (@mnoukhiya) <a href=\"https://twitter.com/mnoukhiya/status/1592737463617089536?ref_src=twsrc%5Etfw\">November 16, 2022</a></blockquote><p>If you know anyone involved with SCinet, I highly recommend seeing if you can get a tour at the next SC. Even as a relative networking novice, I walked away with a much greater appreciation for the annual achievement of building SCinet. And who knows? Once I get bored of this whole storage thing, maybe I'll try getting into high-performance networking.</p><h2 style=\"text-align: left;\">Composability panel</h2><p>This year I was invited to participate in a panel titled \"Smackdown! Does HPC Need Composability Now?\" moderated by Addison Snell and Dan Olds from <a href=\"https://www.intersect360.com\">Intersect360 Research</a>. This panel was...different. Unlike the traditional SC panel where panelists take turns presenting slides and saying erudite things, this panel had two teams of panelists. And my team only had one slide to present:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>The ground rules included \"personal attacks are allowed,\" and needless to say, the panel was about equal parts entertainment and technical discourse. That's not a bad thing, though.</p><p>Addison and Dan did a phenomenal job of pulling their respective teams together and leading discussion in a format that both brought forward the key pros and cons of composability in HPC while poking fun at the thinly veiled, ego-driven personalities that often make up these sorts of panels. Rather than politely dancing around issues like sacrificing memory bandwidth by putting accelerators at the far end of a PCIe bus or gaining higher utilization by allowing users to mix and match CPU, NICs, and GPUs, us panelists were free to shoot straight (or perhaps a bit hyperbolically) and call each other out on our hidden agendas.</p><p>I hope it goes without saying that all us panelists were in on the format and don't actually think people on the other side are dumb. By wrapping technical arguments in snarky comments, we could keep the level of discussion accessible to a wide audience, drive home the key points from both sides, and ensure that we weren't losing audience members who don't care about the PhD-level details as much as they want to hear what their peers are thinking about this exciting new space. I got some feedback afterwards that I didn't seem to hold back, so if anyone did take anything I said seriously, I am very sorry!</p><p>On a technical level, what was the outcome?</p><p>It turns out that <a href=\"https://www.hpcwire.com/off-the-wire/informal-poll-of-sc22-attendees-suggests-a-bright-future-for-composability/\">there was about a 60/40 split between people who felt composability wasn't required yet and those who felt it was</a> after both sides argued their case. Even among panelists, many of us were a lot less convinced about our respective positions than we let on during the panel itself. I got a chuckle when I realized that I wasn't the only one who, when invited to be on the panel, asked \"what side do you want me to argue?\" I honestly could have gone either way because the dust has not yet settled. <a href=\"https://www.tacc.utexas.edu/about/directory/dan-stanzione\">Dan Stanzione, director of TACC</a>, gave the truest answer to the question of \"will composability help HPC\" up front--\"<a href=\"https://twitter.com/HPC_Guru/status/1592604467698241537?s=20&amp;t=tn3WQBUY9M0MWSfqx1XLKA\">it depends</a>.\" Maybe this is a growth opportunity, or maybe it's a lukewarm reception.</p><p>Either way, composable technologies are hitting the market regardless of whether you think they'll be useful or not.  <a href=\"https://www.nextplatform.com/2022/11/10/amd-genoa-epyc-server-cpus-take-the-heavyweight-title/\">AMD Genoa supports CXL 1.1 with extensions for memory pooling</a>, <a href=\"https://news.samsung.com/global/samsung-electronics-unveils-far-reaching-next-generation-memory-solutions-at-flash-memory-summit-2022\">Samsung has memory-semantic SSDs</a>, and everyone and their mother is working on photonics to get higher bandwidths and lower latencies over longer distances. This makes it easier for people to dip their toes in the water to see if composability makes sense, and I think that's what a lot of people will wind up doing in the coming years.</p><h2 style=\"text-align: left;\">Customer meetings</h2><p>Unlike in years past, my SC experience this year was dominated by customer meetings. I've been on the customer side of the table plenty of times, but I was surprised to find that it was actually more fun to be on the vendor side for a change. I'm part salesman at heart, so I found it personally gratifying to end a meeting with people nodding along rather than scratching their heads. I learned as a customer that it's very easy for vendors to go way off the rails and waste everyone's time, so I was grateful to have avoided the awkward confusion that punctuates those kinds of meetings. </p><p>I also went into the week worrying that I'd be sitting in the same room, hearing the same pitch and the same jokes, and answering the same questions all week. Thankfully, I work with some great field, business, and product teams who set up interesting conversations rather than rote recitations of boring roadmap slides. Approaching the same topics from different angles helped me figure out how all the pieces of what I'm working on fit together to make a complete picture too; there weren't nearly as many opportunities to do this in the DOE world since the end-users of the HPC systems on which I worked aren't told anything until all the design decisions have already been made.</p><h2 style=\"text-align: left;\">A few personal notes</h2><p>This SC was significant to me at a variety of levels; it was the first time I'd gotten on an airplane since February 2020, the first time I'd traveled since starting a new job at a new company, and the first time I'd met any of my new coworkers outside of the structure of a Teams call. During the pandemic I realized that getting out into the world and talking to people from all corners of HPC were my favorite part of my job. Not being able to go to events like SC and maintain that a sense of community involvement dramatically impacted my level of professional satisfaction for the last two years, so I'm glad I was able to finally go this year.</p><p>Though customer meetings were a lot more fun than I expected them to be, I still felt bummed that I could spend so little time walking the expo, talking to folks, and attending all the BOFs normally on my <a href=\"https://sc22.supercomputing.org/presentation/?id=bof124&amp;sess=sess331\">must</a>-<a href=\"https://sc22.supercomputing.org/presentation/?id=bof112&amp;sess=sess307\">attend</a> <a href=\"https://sc22.supercomputing.org/presentation/?id=bof110&amp;sess=sess369\">list</a>. Compounding this was my personal choice to not dine indoors and consequently miss out on almost all other chances to catch up with old friends and colleagues. I also decided to leave SC a day earlier than I usually do to reduce my risk of getting sick which didn't help either. There's never enough time at SC, but this year was particularly pressed.</p><p>I say all this not to complain, but to say how much I appreciated the people who went out of their way to come accost me during the precious few hours I actually had on the exhibit floor. Some I'd not seen since SC'19, and some I'd never actually met since we only started working together mid-pandemic. The conference is busy for everyone, so giving me a slice of your time was very meaningful. That sense of community membership is why I go to SC, it's why I still work in this business, and it's why I try to contribute whatever I can to whomever wants it whether it be a student, engineer, salesperson, or marketer.</p>",
            "url": "https://hpc.social/personal-blog/2022/sc-22-recap/",
            
            
            
            
            
            "date_published": "2022-11-24T02:00:00-07:00",
            "date_modified": "2022-11-24T02:00:00-07:00",
            
                "author": "Glenn K. Lockwood's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/converged-computing/",
            "title": "Converged Computing",
            "summary": null,
            "content_text": "For many years, there has been a battle between cloud and HPC. The cloud side of the equation says “micro services, cloud native!”and the HPC side says “too expensive!” Conversations often don’t progress because both sides are up-in-arms and focused on why they cannot work together. At best, we might get access to cloud from an HPC center,or an company might present a product as branded for “HPC.” But it’s not truly collaborative in the way that I’d like.I’ll also step back and comment that (I do not believe) folks (myself included) on the HPC side have done enoughto sit at the table. For example, we haven’t been a voice in the Open Containers Initiative (although I’ve tried), nor have we been present (historically) for conferences that are more focused around cloud native technologies.There is no pointing fingers or fault here - it’s just a matter of two different cultures, and it’s been challenging figuring out how to talk to one another, and how to work together. I’ve tried my best to be involved, to the best of my ability, in small ways on both sides. But I’m only one person. This isn’t to say there haven’t been small collaborations, but I believe we can do more.Change is ComingI think this is going to change. The reason is because both sides of the equation have started to realize we have similar goals,and it’s not about creating hybrid environments – having both pancakes and waffles for breakfast – but rather convergence – recognizing that pancakes and waffles are both kinds of breakfast cakes, and we can take features that we like of each to create a breakfast cake that will make everyone happy.The idea of “Converged Computing” comes from my amazing team (see Dan’s talk at KubeCon here) and is the idea that technologies from HPC can be integrated into more traditionally cloud approaches to produce a solution thatsolves problems on both sides. Explicitly for these projects, it means testing the Flux Framework scheduler alongside Kubernetes. Do we still want portable workflows that can move from an HPC environment to cloud? Of course.However, the niche or gradient that I’m interested in is the space that lives between these two worlds.While I won’t go into huge detail (this would be more appropriate for a talk) the lab openly works on Flux Framework, a resource manager that (in my opinion) is one of the coolest projects coming out of our space. I started working with these teams a few months ago, and am bringing my excitement and vision for (what I hope to be) a future where we are actively developing alongside other Kubernetes projects, and our work is well-known and established in this space.What does that mean? Let me share some cool work under development. This is all being done publicly on GitHub, so there isno issue to talk about it! My first year or so at the lab I was hired under a research project, and although I learned a lot, I haven’t felt inspired and driven until starting this work. Let’s talk about some of it! 🎉️The Flux OperatorIf you aren’t familiar with Kubernetes Operators, let’s step back and talk about a human operator. If you are a syadmin managing appswith associated services and databases on a cluster, you often had to do maintenance or update tasks like increasing a storage volume,or modifying a service to a new user need. As this pattern has emerged as a common thing, they have come up with the concept of a Kubernetes Operator - an actual controller you install to your cluster that can automate this. In simple terms, after you install an operator to your cluster,you can hand it a desired state (represented in a yaml configuration file) and the operator will do whatever it takes to reach that state. What does that means in the context of Flux? The Flux Operator is interested in creatingwhat we are calling a “Mini Cluster,” illustrated below.In Kubernetes object terms this is an Indexed Job, a few config maps, secrets, and a RESTFul API and user interface that I designed exposed as a service.  You can read more about our current design here.This Mini Cluster is generated from a “custom resource definition” or CRD (the yaml you provide), and it can take these parameters. Concetually, you as the user own the Mini Cluster and can submit jobs to it (either via the web interface or the API) until you are done. When you are done, you can bring down the cluster.We are excited for this work because in the next months (to a bit longer) we are going to be testing different kinds of workloads running using Flux alongside this Mini Cluster, but on Kubernetes! I’ve started a small repository of dummy examples that I’m extending quickly atrse-ops/flux-hpc and please open an issue there if you have a suggestion.Stay Tuned!Stay tuned for more work in this space! I’ve been doing a ton of programming in Go, Python, and workingon a wide range of technologies, and fairly quickly, and I am very much in my happy place. Please come and join us! ❤️",
            "content_html": "<p>For many years, there has been a battle between cloud and HPC. The cloud side of the equation says “micro services, cloud native!”and the HPC side says “too expensive!” Conversations often don’t progress because both sides are up-in-arms and focused on why they cannot work together. At best, we might get access to cloud from an HPC center,or an company might present a product as branded for “HPC.” But it’s not truly collaborative in the way that I’d like.</p><p>I’ll also step back and comment that (I do not believe) folks (myself included) on the HPC side have done enoughto sit at the table. For example, we haven’t been a voice in the Open Containers Initiative (<a href=\"https://supercontainers.github.io/containers-wg/\" target=\"_blank\">although I’ve tried</a>), nor have we been present (historically) for conferences that are more focused around cloud native technologies.There is no pointing fingers or fault here - it’s just a matter of two different cultures, and it’s been challenging figuring out how to talk to one another, and how to work together. I’ve tried my best to be involved, to the best of my ability, in small ways on both sides. But I’m only one person. This isn’t to say there haven’t been small collaborations, but I believe we can do more.</p><h2 id=\"change-is-coming\">Change is Coming</h2><p>I think this is going to change. The reason is because both sides of the equation have started to realize we have similar goals,and it’s not about creating hybrid environments – having both pancakes and waffles for breakfast – but rather convergence – recognizing that pancakes and waffles are both kinds of breakfast cakes, and we can take features that we like of each to create a breakfast cake that will make everyone happy.The idea of “Converged Computing” comes from my amazing team (see <a href=\"https://www.youtube.com/watch?v=9VwAcSOtph0\" target=\"_blank\">Dan’s talk at KubeCon here</a>) and is the idea that technologies from HPC can be integrated into more traditionally cloud approaches to produce a solution thatsolves problems on both sides. Explicitly for these projects, it means testing the Flux Framework scheduler alongside Kubernetes. Do we still want portable workflows that can move from an HPC environment to cloud? Of course.However, the niche or gradient that I’m interested in is the space that lives <em>between</em> these two worlds.</p><p>While I won’t go into huge detail (this would be more appropriate for a talk) the lab openly works on <a href=\"https://github.com/flux-framework\" target=\"_blank\">Flux Framework</a>, a resource manager that (in my opinion) is one of the coolest projects coming out of our space. I started working with these teams a few months ago, and am bringing my excitement and vision for (what I hope to be) a future where we are actively developing alongside other Kubernetes projects, and our work is well-known and established in this space.What does that mean? Let me share some cool work under development. This is all being done publicly on GitHub, so there isno issue to talk about it! My first year or so at the lab I was hired under a research project, and although I learned a lot, I haven’t felt inspired and driven until starting this work. Let’s talk about some of it! 🎉️</p><h3 id=\"the-flux-operator\">The Flux Operator</h3><div style=\"padding: 20px;\"><img src=\"https://flux-framework.org/flux-operator/_images/the-operator.jpg\" /></div><p>If you aren’t familiar with Kubernetes Operators, let’s step back and talk about a human operator. If you are a syadmin managing appswith associated services and databases on a cluster, you often had to do maintenance or update tasks like increasing a storage volume,or modifying a service to a new user need. As this pattern has emerged as a common thing, they have come up with the concept of a Kubernetes Operator - an actual controller you install to your cluster that can automate this. In simple terms, after you install an operator to your cluster,you can hand it a desired state (represented in a yaml configuration file) and the operator will do whatever it takes to reach that state. What does that means in the context of Flux? The Flux Operator is interested in creatingwhat we are calling a “Mini Cluster,” illustrated below.</p><div style=\"padding: 20px;\"><img src=\"https://flux-framework.org/flux-operator/_images/design-three-team1.png\" /></div><p>In Kubernetes object terms this is an <a href=\"https://kubernetes.io/docs/tasks/job/indexed-parallel-processing-static/\" target=\"_blank\">Indexed Job</a>, a few config maps, secrets, and a <a href=\"https://flux-framework.org/flux-restful-api/\" target=\"_blank\">RESTFul API</a> and user interface that I designed exposed as a service.  You can read more about our current design <a href=\"https://flux-framework.org/flux-operator/development/designs.html\" target=\"_blank\">here</a>.</p><p>This Mini Cluster is generated from a “custom resource definition” or CRD (the yaml you provide), and it can take <a href=\"https://flux-framework.org/flux-operator/getting_started/custom-resource-definition.html\" target=\"_blank\">these parameters</a>. Concetually, you as the user own the Mini Cluster and can submit jobs to it (either via the web interface or the API) until you are done. When you are done, you can bring down the cluster.</p><p>We are excited for this work because in the next months (to a bit longer) we are going to be testing different kinds of workloads running using Flux alongside this Mini Cluster, but on Kubernetes! I’ve started a small repository of dummy examples that I’m extending quickly at<a href=\"https://github.com/rse-ops/flux-hpc\" target=\"_blank\">rse-ops/flux-hpc</a> and please open an issue there if you have a suggestion.</p><h3 id=\"stay-tuned\">Stay Tuned!</h3><p>Stay tuned for more work in this space! I’ve been doing a ton of programming in Go, Python, and workingon a wide range of technologies, and fairly quickly, and I am very much in my happy place. Please come and join us! ❤️</p>",
            "url": "https://hpc.social/personal-blog/2022/converged-computing/",
            
            
            
            
            
            "date_published": "2022-11-18T08:30:00-07:00",
            "date_modified": "2022-11-18T08:30:00-07:00",
            
                "author": "Vanessasaurus"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/ceph-osd-cpu-scaling-part-1/",
            "title": "Ceph OSD CPU Scaling - Part 1",
            "summary": null,
            "content_text": "Last summer we had a user that hit some performance issues based on a recommendation to use 2 cores per OSD in their systems.  I wanted to provide some data for the community and wrote up a blog post on the ceph.io website.  Please take a look!",
            "content_html": "<p>Last summer we had a user that hit some performance issues based on a recommendation to use 2 cores per OSD in their systems.  I wanted to provide some data for the community and wrote up a blog <a href=\"https://ceph.io/en/news/blog/2022/ceph-osd-cpu-scaling/\">post</a> on the ceph.io website.  Please take a look!</p>",
            "url": "https://hpc.social/personal-blog/2022/ceph-osd-cpu-scaling-part-1/",
            
            
            
            
            
            "date_published": "2022-11-08T00:00:00-07:00",
            "date_modified": "2022-11-08T00:00:00-07:00",
            
                "author": "Mark Nelson's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/containerize-it-baby/",
            "title": "Containerize It, Baby!",
            "summary": null,
            "content_text": "I’ve just submit my entry to the HPC Guru Elevator Pitch Contest for the Supercomputing 2022 conference!I’m fairly sure (like many of these contests) it will be a politically correct winner - someone that is best appealingto the conference, but I’ll take a stand right now that I think my submission is tops in terms of creativityand excited energy! I mean, there is just no alternative when it comes to technologies I’m excited about.  Containerize it, baby!Mic Drop! 🎙️Regardless of the outcome of this contest, I feel like I’ve already won - I’ve had so much fun making this and sharing with the community! 🎉️",
            "content_html": "<p>I’ve just submit my <a href=\"https://twitter.com/vsoch/status/1588215058009464832\" target=\"_blank\">entry</a> to the HPC Guru Elevator Pitch Contest for the Supercomputing 2022 conference!</p><p>I’m fairly sure (like many of these contests) it will be a politically correct winner - someone that is best appealingto the conference, but I’ll take a stand right now that I think my submission is tops in terms of creativityand excited energy! I mean, there is just no alternative when it comes to technologies I’m excited about.</p><blockquote>  <p>Containerize it, baby!</p></blockquote><p><em>Mic Drop!</em> 🎙️</p><p>Regardless of the outcome of this contest, I feel like I’ve already won - I’ve had so much fun making this and sharing with the community! 🎉️</p>",
            "url": "https://hpc.social/personal-blog/2022/containerize-it-baby/",
            
            
            
            
            
            "date_published": "2022-11-03T09:30:00-06:00",
            "date_modified": "2022-11-03T09:30:00-06:00",
            
                "author": "Vanessasaurus"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/happy-living-close-ish-to-the-metal/",
            "title": "happy living close (-ish) to the metal",
            "summary": null,
            "content_text": "For various reasons, I’ve been doing a little bit of career introspection lately. One of the interesting realizations to come out of this is that, despite in practice doing mostly software work, I’ve been happiest when my work involved a strong awareness of the hardware I was running on.I suppose it shouldn’t be a surprise, exactly, but I hadn’t exactly thought about it in those terms before! Before I got into computing, I got a bachelors degree in physics, and got through much of a PhD in materials science. While I wasn’t building computers directly, I was definitely working regularly on hardware, building experimental apparatus involving various combinations of vacuum chambers, lasers, exotic microscopes, custom electronics, and microfluidics.In terms of my computing career, I’ve generally worked in the area of “high-performance computing”, a buzzword that means I’ve focused on building fast parallel systems aimed at researchers. It’s a sub-field that lends itself to awareness of hardware: even as a new baby sysadmin, I was staring at motherboard block diagrams and thinking about the performance differences between different PCIe topologies. And because HPC is one of the areas that took the longest to embrace cloud computing, I spent a lot of years doing work in datacenters. Most of my work would usually involve writing code, doing configuration management, and managing Linux systems… but on a regular basis I’d head into a big loud room full of air conditioners and server racks, carrying a screwdriver.Amusingly, my relatively recent stint at a hyperscaler was the first time I had worked on computers, but didn’t have my office in the same building as the computers I was running! Even there I was at least somewhat cognizant of hardware specifics, and one of my early projects was performance testing on the Bryce Canyon storage node, to see if it was ready for use in a large-scale distributed filesystem.And these days, at NVIDIA, I’m enjoying being even closer to the metal. (At least conceptually; I still work remote…) I spend my days thinking about datacenter requirements, cable lengths, firmware upgrades, hardware health checks, and application performance tests on large clusters. And I love getting to play with these shiny toys.Anyway, this is just a ramble. But a useful one. While I’d be the first to admit that cloud has its place, and I use it for some personal projects, I really enjoy understanding the hardware I run on. I have trouble thinking of computers as remote abstractions with no underlying detail. They are pleasingly physical in my mind, even if they’re thousands of miles away.",
            "content_html": "<p>For various reasons, I’ve been doing a little bit of career introspection lately. One of the interesting realizations to come out of this is that, despite in practice doing mostly software work, I’ve been happiest when my work involved a strong awareness of the hardware I was running on.</p><p><span id=\"more-247\"></span></p><p>I suppose it shouldn’t be a surprise, exactly, but I hadn’t exactly thought about it in those terms before! Before I got into computing, I got a bachelors degree in physics, and got through much of a PhD in materials science. While I wasn’t building computers directly, I was definitely working regularly on hardware, building experimental apparatus involving various combinations of vacuum chambers, lasers, exotic microscopes, custom electronics, and microfluidics.</p><p>In terms of my computing career, I’ve generally worked in the area of “high-performance computing”, a buzzword that means I’ve focused on building fast parallel systems aimed at researchers. </p><p>It’s a sub-field that lends itself to awareness of hardware: even as a new baby sysadmin, I was staring at motherboard block diagrams and thinking about the performance differences between different PCIe topologies. </p><p>And because HPC is one of the areas that took the longest to embrace cloud computing, I spent a lot of years doing work in datacenters. Most of my work would usually involve writing code, doing configuration management, and managing Linux systems… but on a regular basis I’d head into a big loud room full of air conditioners and server racks, carrying a screwdriver.</p><p>Amusingly, my relatively recent stint at a hyperscaler was the first time I had worked on computers, but didn’t have my office in the same building as the computers I was running! Even there I was at least somewhat cognizant of hardware specifics, and one of my early projects was performance testing on the B<a href=\"https://www.opencompute.org/documents/facebook-bryce-canyon-storage-system-specification\">ryce Canyon </a>storage node, to see if it was ready for use in a large-scale distributed filesystem.</p><p>And these days, at NVIDIA, I’m enjoying being even closer to the metal. (At least conceptually; I still work remote…) I spend my days thinking about datacenter requirements, cable lengths, firmware upgrades, hardware health checks, and application performance tests on large clusters. And I love getting to play with these shiny toys.</p><p>Anyway, this is just a ramble. But a useful one. While I’d be the first to admit that cloud has its place, and I use it for some personal projects, I really enjoy understanding the hardware I run on. I have trouble thinking of computers as remote abstractions with no underlying detail. They are pleasingly physical in my mind, even if they’re thousands of miles away.</p>",
            "url": "https://hpc.social/personal-blog/2022/happy-living-close-ish-to-the-metal/",
            
            
            
            
            
            "date_published": "2022-11-02T00:18:17-06:00",
            "date_modified": "2022-11-02T00:18:17-06:00",
            
                "author": "Thinking Out Loud"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/the-web-services-i-self-host/",
            "title": "The web services I self-host",
            "summary": null,
            "content_text": "Why self-host anything?In a lot of ways, self-hosting web services is signing up for extra pain. Most useful web services are available in SaaS format these days, and most people don&#8217;t want to be a sysadmin just to use chat, email, or read the news.In general, I decide to self-host a service if one of two things is true:Self-hosting is going to add a capability that&#8217;s difficult to find in a SaaS alternative. That might be privacy, or extra compute, or just an extra degree of customization that I want.I find it interesting or amusing to self-host it! I have been a professional sysadmin, and ran production web services for over a decade. So I enjoy messing around with servers, and can have a fair amount of fun with this.Infrastructure and general toolingRight now my self-hosted services are hosted on Oracle Cloud Infrastructure, for a very simple reason: OCI includes a very generous Always Free tier, which doesn&#8217;t even ask for a credit card! So I&#8217;m confident I&#8217;m not going to accidentally spend any money. I use ARM Ampere A1 Compute instances for service hosting.The individual services are mostly managed using Docker Compose files, though a few are just running bare-metal. I have so far managed to resist the urge to put everything in Kubernetes.Everything is backed up on a regular basis using Tarsnap.I also use Tailscale to provide a VPN between my cloud servers and my various client devices (phone, laptop, tablet). If a service needs to be exposed to the public Internet to function, I do that&#8230; but otherwise, everything is only exposed within the Tailscale VPN, so that only my own devices can access them. This is both a lovely convenience (not having to manage as many DNS records), and provides an extra degree of security by hiding services that no one else needs to access.Services that I self-hostRSS reader: Despite the demise of Google Reader back in the mists of time, I&#8217;ve been a consistently heavy user of RSS feed since at least 2008. At times I&#8217;ve used commercial products such as Feedly, but these days I self-host the aggregator using FreshRSS. I use FreshRSS partly because it&#8217;s pretty easy to spin up and administer, and partly because it&#8217;s compatible with Reeder, a Mac and iOS app that I generally use to actually read my feeds.Fediverse instance: I run a self-hosted instance on the Fediverse ensemble of social networking sites. The best-known tool for this is Mastodon, but I currently use the Pleroma server, mostly because it seemed less painful to set up and configure. I run my own instance partly out of curiosity, and partly because I didn&#8217;t strongly resonate with any particular topic-specific server that&#8217;s already out there.IRC bouncer: I&#8217;m not on IRC very much these days, but I do like to avoid losing messages, and sometimes want to be logged into the same channels on different physical clients. So I run a ZNC server to maintain persistence.Matrix server: Matrix is a decentralized messaging platform that supports end-to-end encryption. Think of it as being a little like the Fediverse, but for chat rather than microblogging. This falls pretty squarely in the category of &#8220;I find this amusing to run&#8221;, because I mostly chat with less-nerdy folks on other, commercial platforms.Git server: I run a Gitea server which I use to mirror my own repos, as well as a variety of other open source repos. This is mostly to ensure that I have an up-to-date backup of repos I care about, independent of Github or whatever provider.Jupyter notebooks: I keep a persistent Jupyter notebook instance running for random code experiments and as a tiny development playground. This runs on its own VM where I also do other random software development, and it&#8217;s separate from the other services mostly so I don&#8217;t take down all my personal infra with an accidental OOM from a big build.Software package repository: I run an instance of Nexus Repository OSS, mostly to cache Docker images and other content that run the rest of the services above!Services where I use managed hosting but don&#8217;t own the serverThis website! My regular website and this blog run on a shared hosting provider, mostly through inertia. (I&#8217;ve used the same hosting provider for web hosting since around 2008.)Email: In theory it&#8217;s an open, federated system similar to the Fediverse. In practice, the combination of spam and the growth of large providers makes it increasingly painful to run a server yourself. This post from Carlos Fenollosa does a good job of describing the difficulties.I do, however, run all my email through my own domain, though it&#8217;s hosted via Google Apps GSuite Google Workspace. I also back up my inbox locally on a regular basis. That means that if Google ever decides to remove my account, charge obnoxious costs, or otherwise misbehave, my email address is at least portable to other providers.",
            "content_html": "<h2>Why self-host anything?</h2><p>In a lot of ways, self-hosting web services is signing up for extra pain. Most useful web services are available in SaaS format these days, and most people don&#8217;t want to be a sysadmin just to use chat, email, or read the news.</p><p>In general, I decide to self-host a service if one of two things is true:</p><p><span id=\"more-235\"></span></p><ul><li>Self-hosting is going to add a capability that&#8217;s difficult to find in a SaaS alternative. That might be privacy, or extra compute, or just an extra degree of customization that I want.<br /></li><li>I find it interesting or amusing to self-host it! I <em>have been</em> a professional sysadmin, and ran production web services for over a decade. So I enjoy messing around with servers, and can have a fair amount of fun with this.</li></ul><h2>Infrastructure and general tooling</h2><p>Right now my self-hosted services are hosted on <a href=\"https://www.oracle.com/cloud/\">Oracle Cloud Infrastructure</a>, for a very simple reason: OCI includes a <em>very</em> generous <a href=\"https://www.oracle.com/cloud/free/\">Always Free tier</a>, which doesn&#8217;t even ask for a credit card! So I&#8217;m confident I&#8217;m not going to accidentally spend any money. I use ARM Ampere A1 Compute instances for service hosting.</p><p>The individual services are mostly managed using <a href=\"https://docs.docker.com/compose/\">Docker Compose files</a>, though a few are just running bare-metal. I have so far managed to resist the urge to put everything in Kubernetes.</p><p>Everything is backed up on a regular basis using <a href=\"https://www.tarsnap.com/\">Tarsnap</a>.</p><p>I also use <a href=\"https://tailscale.com/\">Tailscale</a> to provide a VPN between my cloud servers and my various client devices (phone, laptop, tablet). If a service needs to be exposed to the public Internet to function, I do that&#8230; but otherwise, everything is only exposed within the Tailscale VPN, so that only my own devices can access them. This is both a lovely convenience (not having to manage as many DNS records), and provides an extra degree of security by hiding services that no one else needs to access.</p><h2>Services that I self-host</h2><ul><li><strong>RSS reader: </strong>Despite the demise of Google Reader back in the mists of time, I&#8217;ve been a consistently heavy user of RSS feed since at least 2008. At times I&#8217;ve used commercial products such as <a href=\"https://feedly.com/\">Feedly</a>, but these days I self-host the aggregator using <a href=\"https://freshrss.org/\">FreshRSS</a>. I use FreshRSS partly because it&#8217;s pretty easy to spin up and administer, and partly because it&#8217;s compatible with <a href=\"https://reederapp.com/\">Reeder</a>, a Mac and iOS app that I generally use to actually read my feeds.<br /></li><li><strong>Fediverse instance: </strong>I run a <a href=\"https://calico.social/\">self-hosted instance</a> on the <a href=\"https://en.wikipedia.org/wiki/Fediverse\">Fediverse</a> ensemble of social networking sites. The best-known tool for this is <a href=\"https://joinmastodon.org/\">Mastodon</a>, but I currently use the <a href=\"https://pleroma.social/\">Pleroma server</a>, mostly because it seemed less painful to set up and configure. I run my own instance partly out of curiosity, and partly because I didn&#8217;t strongly resonate with any particular topic-specific server that&#8217;s already out there.<br /></li><li><strong>IRC bouncer: </strong>I&#8217;m not on IRC very much these days, but I do like to avoid losing messages, and sometimes want to be logged into the same channels on different physical clients. So I run a <a href=\"https://wiki.znc.in/ZNC\">ZNC</a> server to maintain persistence.<br /></li><li><strong>Matrix server: </strong><a href=\"https://matrix.org/\">Matrix</a> is a decentralized messaging platform that supports end-to-end encryption. Think of it as being a little like the Fediverse, but for chat rather than microblogging. This falls pretty squarely in the category of &#8220;I find this amusing to run&#8221;, because I mostly chat with less-nerdy folks on other, commercial platforms.<br /></li><li><strong>Git server: </strong>I run a <a href=\"https://gitea.io/en-us/\">Gitea</a> server which I use to mirror my own repos, as well as a variety of other open source repos. This is mostly to ensure that I have an up-to-date backup of repos I care about, independent of Github or whatever provider.<br /></li><li><strong>Jupyter notebooks: </strong>I keep a persistent <a href=\"https://jupyter.org/\">Jupyter</a> notebook instance running for random code experiments and as a tiny development playground. This runs on its own VM where I also do other random software development, and it&#8217;s separate from the other services mostly so I don&#8217;t take down all my personal infra with an accidental OOM from a big build.<br /></li><li><strong>Software package repository: </strong>I run an instance of <a href=\"https://www.sonatype.com/products/repository-oss-download\">Nexus Repository OSS</a>, mostly to cache Docker images and other content that run the rest of the services above!</li></ul><h2>Services where I use managed hosting but don&#8217;t own the server</h2><ul><li><strong>This website!</strong> My <a href=\"https://www.ajdecon.org\">regular website</a> and this blog run on a shared hosting provider, mostly through inertia. (I&#8217;ve used the same hosting provider for web hosting since around 2008.)<br /></li><li><strong>Email: </strong>In theory it&#8217;s an open, federated system similar to the Fediverse. In practice, the combination of spam and the growth of large providers makes it increasingly painful to run a server yourself. This <a href=\"https://cfenollosa.com/blog/after-self-hosting-my-email-for-twenty-three-years-i-have-thrown-in-the-towel-the-oligopoly-has-won.html\">post from Carlos Fenollosa</a> does a good job of describing the difficulties.<br /><br />I do, however, run all my email through my own domain, though it&#8217;s hosted via <s>Google Apps</s> <s>GSuite</s> Google Workspace. I also back up my inbox locally on a regular basis. That means that if Google ever decides to remove my account, charge obnoxious costs, or otherwise misbehave, my email address is at least portable to other providers.</li></ul><p></p>",
            "url": "https://hpc.social/personal-blog/2022/the-web-services-i-self-host/",
            
            
            
            
            
            "date_published": "2022-10-30T21:59:55-06:00",
            "date_modified": "2022-10-30T21:59:55-06:00",
            
                "author": "Thinking Out Loud"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/qemu-kvm-ceph-librbd-performance/",
            "title": "QEMU/KVM + Ceph Librbd Performance",
            "summary": null,
            "content_text": "Checkout my blog post at the ceph.io website about tuning QEMU/KVM for high performance with librbd.  We got over 123K random read IOPs with 16K IOs from a single VM!",
            "content_html": "<p>Checkout my blog <a href=\"https://ceph.io/en/news/blog/2022/qemu-kvm-tuning/\">post</a> at the ceph.io website about tuning QEMU/KVM for high performance with librbd.  We got over 123K random read IOPs with 16K IOs from a single VM!</p>",
            "url": "https://hpc.social/personal-blog/2022/qemu-kvm-ceph-librbd-performance/",
            
            
            
            
            
            "date_published": "2022-10-24T01:00:00-06:00",
            "date_modified": "2022-10-24T01:00:00-06:00",
            
                "author": "Mark Nelson's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/tunel-apps-for-hpc/",
            "title": "Tunel- Apps for HPC",
            "summary": null,
            "content_text": "A few months ago I was talking about ssh tunnels. The reason was because I was looking for a solution to deploy apps (like a Jupyter notebook) onto HPC.After an adventure I got it working, and it came down a relatively simple set of commands that I needed to just write into my app logic and forget about.The reason for this was working on my new personal project, tunel.  Tunel is named for what it does. “Tunel” is an elegant derivation of “tunnel” and will do exactly that - create a tunnel between your local workstation and an HPC cluster. In short, tunel will provide a collection of “apps” that are easy to deploy to HPC. There are concepts called launchers, and examples are singularity, slurm, or htcondor. And we can add more! It’s the job of a launcher to take a an app recipe (a definition in yaml plus helper scripts that can be customized on the fly by the user) and get it running, whatever that means (run a job? a container? monitor something? something else?). For the most part, most apps that I’ve developing have web interfaces, as they have historically been the most challenging thing to get easily working on HPC. As a quick example, to run a jupyter notebook via Singularity on my login node, after I install tunel and have my ssh connection defined as “osg” I can do:$ tunel run-app osg singularity/socket/jupyter --jupyterlab=true The name “singularity/socket/jupyter” is the unique identifier (and path) to the recipe and config, and I can provide custom arguments as shown above. And although this is the “singularity” launcher, we can do the same kind of interaction with a slurm launcher, going one level deeper to run the notebook on a node after we submit a job!And in my typical way of doing things, I have automation that generates a table and documentation for each of these apps. Check them out here!. I’m mostly working on singularity an HTCondor apps at the moment because I use the open science grid (OSG) for development, as this is a personal project. Thanks to Matthew West for showing me OSG - I was pretty handicapped to develop before finding it!Django template with a socket?This kind of framework can be powerful if I develop a bunch of custom apps, but it’s much more powerful if I can enable YOU to easily do that too! Thus, I knew one of the first tasks I wanted to do is create a template, likely in each of Flask, Django, and FastAPI, that would plug immediately into Tunel. And while I have much work left to do, last night and this evening I figured out a technical issue that is going to empower us to make so many cool things and I wanted to share! Let’s talk about the problem, what I tried, and what ultimately worked.Traditional Setup with uwsgi and nginxIf you look at a family of Python + web interface apps, you’ll find this uwsgi guy in the middle (I don’t know the correct pronunciation but I say YOU-SKI). It’s a fairly rich tool, but in layman’s terms I think of it as a middleman between Python and a traditional web server. But actually, you don’t technically need the web server - and this is where things start to get interesting. For a traditonal setup, you might find a nginx (a web server) configuration file that looks like this.# the upstream component nginx needs to connect toupstream django {    server unix:///tmp/tunel-django.sock;}# configuration of the serverserver {    # the port your site will be served on    listen      8000;    charset     utf-8;    server_name           localhost;    client_max_body_size 10024M;    client_body_buffer_size 10024M;    client_body_timeout 120;    ...    location ~* \\.(php|aspx|myadmin|asp)$ {      deny all;    }    location /static/ {        autoindex on;        alias /var/www/static/;    }    # Finally, send all non-media requests to the Django server.    location / {        uwsgi_pass  django;        uwsgi_max_temp_file_size 10024m;        include /code/scripts/nginx/uwsgi_params.par;    }}I’ve made a lot of web apps, and whether I use docker-compose with separate containers or a single one, I usually have to write a nginx configuration. The above gets started in the container entrypoint with my app calling uwsgi, and defining that same socket:$ uwsgi --socket=${socket} /code/scripts/uwsgi.iniAnd of course things happen before that, but that’s the main last line. The uwsgi.ini is a configuration filethat makes it easier to define settings.[uwsgi]master = trueprocesses = 4threads = 4py-autoreload = 1#socket = :3031chdir = /code/post-buffering = truelog-date = truemax-requests = 5000http-timeout = 3600000socket-timeout = 120chmod-socket = 666wsgi-file = tuneldjango/wsgi.pyignore-sigpipe = trueignore-write-errors = truedisable-write-exception = truebuffer-size=32768Without going into huge detail, the above says that the app that I wrote (in Python) is listening on that socket, so requests to the web server will either be directed to some static file, filtered out, or sent to our application. And we typically want to use nginx because it’s really good at serving static files and handling traffic.But now let’s step back. If you look under the server in the config above, you’ll notice we are servingcontent on port 8000. This is why I can open the browser to localhost and that port and see my application.But as we know with headless HPC, there are no ports. I can’t use this. So this was my first predicament, last night. I had created this application and it ran locally, but I needed to somehow get the entire thing routed through a tunneled socket to take a next step.Uwsgi Only?I’ll skip over the many hours of things that I tried and failed. I really liked having nginx so I first wanted to somehow send it to the user via a socket, but that never worked. I had an idea to just map the original socket and then have a second container on the host for nginx, but I decided that was too complex. What would up working is realizing that uwsgi can serve http directly, and that came down to a single addition to its config:listen=200protocol=httpOnce I did that, I tried the same technique to map the socket being written to directly to a port via the ssh tunnel, and boum I saw a page! But it was really ugly, because it had no style. This is where I was like OHNO I need nginx for static. But then I found this page and it was a message from the heavens - I could define the same static and media URls using uwsgi directly! That looked like this:$ uwsgi --socket=${socket} --static-map /static=/code/static /code/scripts/uwsgi-standalone.iniAt this point I held my breath, re-ran my app, and wow! There it was - my entire app being served by a container running on a remote machine, only accessible to me through a physical socket. And guess what? I added a file browser, and it even worked to upload a dinosaur picture! Here is the entire page for the app - you can see there are many flags you can add and customize to interact. While it’s only accessible to you and there isn’t need for any kind of login, I did add the default username/password login to Django, and require it for logging in to the file browser. Of course I will eventually need this to be more formally security audited, but at least I don’t have anything interesting on my OSG home to be worried about. And is using just uwsgi a performance issue? I think probably not since the expected use case is only once person.A Future for AppsThis is just the beginning - my plan is to put together a list of use cases for a GUI on a cluster, and then just package them into the core template apps for the developer user to easilyc customize. I have big plans for working on this, and honestly I’m so excited that I find I’m staying up way too late and just egging for the work day to end so I can continue. This idea is so powerful, because it’s using existing technologies to deploy containerized apps on HPC, where you don’t need any special permission. Just to show y’all, here is what it looks like to launch my app template:$ tunel run-app osg singularity/socket/tunel-django --tag=dev --pullI added the pull flag and a custom tag because I am actively developing, and my workflow is to quickly rebuild, push, and then run that command. That then shows me the ssh tunnel command that will immediately connect me to my app on a port in my browser.$ ssh -NT -L 7789:/../tunel/singularity/singularity/socket/tunel-django/singularity-socket-tunel-django.sock sochat1@osgAnd that’s seriously it. You as the developer user are empowered to make and deploy apps, and they have interfaces, and you don’t need to do something silly like open a port or actually deploy a web server. It’s so stupidly easy - I’m looking around at all these complex web app setups that people have made for HPC over the years and I wonder why they aren’t doing something simpler. Maybe it’s just a space of development that people gave up on, or there are some security things I’m missing. Either way, I’m going to charge forward working on this! It’s too simple, and the idea is to beautiful to do anything else by this point.",
            "content_html": "<p>A few months ago I was talking about <a href=\"https://vsoch.github.io/2022/ssh-tunnels/\" target=\"_blank\">ssh tunnels</a>. The reason was because I was looking for a solution to deploy apps (like a Jupyter notebook) onto HPC.After an adventure I got it working, and it came down a relatively simple set of commands that I needed to just <a href=\"https://github.com/tunel-apps/tunel/blob/main/tunel/ssh/commands.py\">write into my app logic</a> and forget about.The reason for this was working on my new personal project, <a href=\"https://tunel-apps.github.io/tunel/\" target=\"_blank\">tunel</a>.</p><blockquote>  <p>Tunel is named for what it does. “Tunel” is an elegant derivation of “tunnel” and will do exactly that - create a tunnel between your local workstation and an HPC cluster.</p></blockquote><div style=\"padding: 20px;\"> <img src=\"https://vsoch.github.io/assets/images/posts/tunel/tunel-docs.png\" /></div><p>In short, tunel will provide a collection of “apps” that are easy to deploy to HPC. There are concepts called launchers, and examples are singularity, slurm, or htcondor. And we can add more! It’s the job of a launcher to take a an app recipe (a definition in yaml plus helper scripts that can be customized on the fly by the user) and get it running, whatever that means (run a job? a container? monitor something? something else?). For the most part, most apps that I’ve developing have web interfaces, as they have historically been the most challenging thing to get easily working on HPC. As a quick example, to run a jupyter notebook via Singularity on my login node, after I install tunel and have my ssh connection defined as “osg” I can do:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>tunel run-app osg singularity/socket/jupyter <span class=\"nt\">--jupyterlab</span><span class=\"o\">=</span><span class=\"nb\">true</span> </code></pre></div></div><p>The name “singularity/socket/jupyter” is the unique identifier (and path) to the recipe and config, and I can provide custom arguments as shown above. And although this is the “singularity” launcher, we can do the same kind of interaction with a slurm launcher, going one level deeper to run the notebook on a node after we submit a job!And in my typical way of doing things, I have automation that generates a table and documentation for each of these apps. <a href=\"https://tunel-apps.github.io/tunel/_static/apps/\" target=\"_blank\">Check them out here!</a>.</p><div style=\"padding: 20px;\"> <img src=\"https://vsoch.github.io/assets/images/posts/tunel/table.png\" /></div><p>I’m mostly working on singularity an HTCondor apps at the moment because I use the open science grid (OSG) for development, as this is a personal project. Thanks to <a href=\"https://twitter.com/westbynoreaster\" target=\"_blank\">Matthew West</a> for showing me OSG - I was pretty handicapped to develop before finding it!</p><h2 id=\"django-template-with-a-socket\">Django template with a socket?</h2><p>This kind of framework can be powerful if I develop a bunch of custom apps, but it’s much more powerful if I can enable YOU to easily do that too! Thus, I knew one of the first tasks I wanted to do is create a template, likely in each of Flask, Django, and FastAPI, that would plug immediately into Tunel. And while I have much work left to do, last night and this evening I figured out a technical issue that is going to empower us to make so many cool things and I wanted to share! Let’s talk about the problem, what I tried, and what ultimately worked.</p><h3 id=\"traditional-setup-with-uwsgi-and-nginx\">Traditional Setup with uwsgi and nginx</h3><p>If you look at a family of Python + web interface apps, you’ll find this <a href=\"https://uwsgi-docs.readthedocs.io/en/latest/\" target=\"_blank\">uwsgi</a> guy in the middle (I don’t know the correct pronunciation but I say YOU-SKI). It’s a fairly rich tool, but in layman’s terms I think of it as a middleman between Python and a traditional web server. But actually, you don’t technically need the web server - and this is where things start to get interesting. For a traditonal setup, you might find a nginx (a web server) configuration file that <a href=\"https://github.com/tunel-apps/tunel-django/blob/main/scripts/nginx/nginx.conf\" target=\"_blank\">looks like this</a>.</p><div class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># the upstream component nginx needs to connect to</span><span class=\"s\">upstream django {</span>    <span class=\"s\">server unix:///tmp/tunel-django.sock;</span><span class=\"err\">}</span><span class=\"c1\"># configuration of the server</span><span class=\"s\">server {</span>    <span class=\"s\"># the port your site will be served on</span>    <span class=\"s\">listen      8000;</span>    <span class=\"s\">charset     utf-8;</span>    <span class=\"s\">server_name           localhost;</span>    <span class=\"s\">client_max_body_size 10024M;</span>    <span class=\"s\">client_body_buffer_size 10024M;</span>    <span class=\"s\">client_body_timeout 120;</span>    <span class=\"s\">...</span>    <span class=\"s\">location ~* \\.(php|aspx|myadmin|asp)$ {</span>      <span class=\"s\">deny all;</span>    <span class=\"s\">}</span>    <span class=\"s\">location /static/ {</span>        <span class=\"s\">autoindex on;</span>        <span class=\"s\">alias /var/www/static/;</span>    <span class=\"s\">}</span>    <span class=\"s\"># Finally, send all non-media requests to the Django server.</span>    <span class=\"s\">location / {</span>        <span class=\"s\">uwsgi_pass  django;</span>        <span class=\"s\">uwsgi_max_temp_file_size 10024m;</span>        <span class=\"s\">include /code/scripts/nginx/uwsgi_params.par;</span>    <span class=\"s\">}</span><span class=\"err\">}</span></code></pre></div></div><p>I’ve made a lot of web apps, and whether I use docker-compose with separate containers or a single one, I usually have to write a nginx configuration. The above gets started in the container entrypoint with my app calling uwsgi, and defining that same socket:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>uwsgi <span class=\"nt\">--socket</span><span class=\"o\">=</span><span class=\"k\">${</span><span class=\"nv\">socket</span><span class=\"k\">}</span> /code/scripts/uwsgi.ini</code></pre></div></div><p>And of course things happen before that, but that’s the main last line. The uwsgi.ini is a configuration filethat makes it easier to define settings.</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>[uwsgi]master = trueprocesses = 4threads = 4py-autoreload = 1#socket = :3031chdir = /code/post-buffering = truelog-date = truemax-requests = 5000http-timeout = 3600000socket-timeout = 120chmod-socket = 666wsgi-file = tuneldjango/wsgi.pyignore-sigpipe = trueignore-write-errors = truedisable-write-exception = truebuffer-size=32768</code></pre></div></div><p>Without going into huge detail, the above says that the app that I wrote (in Python) is listening on that socket, so requests to the web server will either be directed to some static file, filtered out, or sent to our application. And we typically want to use nginx because it’s really good at serving static files and handling traffic.</p><p>But now let’s step back. If you look under the server in the config above, you’ll notice we are servingcontent on port 8000. This is why I can open the browser to localhost and that port and see my application.But as we know with headless HPC, there are no ports. I can’t use this. So this was my first predicament, last night. I had created this application and it ran locally, but I needed to somehow get the entire thing routed through a tunneled socket to take a next step.</p><h3 id=\"uwsgi-only\">Uwsgi Only?</h3><p>I’ll skip over the many hours of things that I tried and failed. I really liked having nginx so I first wanted to somehow send it to the user via a socket, but that never worked. I had an idea to just map the original socket and then have a second container on the host for nginx, but I decided that was too complex. What would up working is realizing that uwsgi can serve http directly, and that came down to a single addition to its config:</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>listen=200protocol=http</code></pre></div></div><p>Once I did that, I tried the same technique to map the socket being written to directly to a port via the ssh tunnel, and <em>boum</em> I saw a page! But it was really ugly, because it had no style. This is where I was like OHNO I need nginx for static. But then I found <a href=\"https://uwsgi-docs.readthedocs.io/en/latest/StaticFiles.html\" target=\"_blank\">this page</a> and it was a message from the heavens - I could define the same static and media URls using uwsgi directly! That looked like this:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>uwsgi <span class=\"nt\">--socket</span><span class=\"o\">=</span><span class=\"k\">${</span><span class=\"nv\">socket</span><span class=\"k\">}</span> <span class=\"nt\">--static-map</span> /static<span class=\"o\">=</span>/code/static /code/scripts/uwsgi-standalone.ini</code></pre></div></div><p>At this point I held my breath, re-ran my app, and wow!</p><div style=\"padding: 20px;\"> <img src=\"https://vsoch.github.io/assets/images/posts/tunel/home.png\" /></div><p>There it was - my entire app being served by a container running on a remote machine, only accessible to me through a physical socket. And guess what? I added a file browser, and it even worked to upload a dinosaur picture!</p><div style=\"padding: 20px;\"> <img src=\"https://vsoch.github.io/assets/images/posts/tunel/browser.png\" /></div><p>Here is the entire page for the app - you can see there are many flags you can add and customize to interact.</p><div style=\"padding: 20px;\"> <img src=\"https://vsoch.github.io/assets/images/posts/tunel/app.png\" /></div><p>While it’s only accessible to you and there isn’t need for any kind of login, I did add the default username/password login to Django, and require it for logging in to the file browser. Of course I will eventually need this to be more formally security audited, but at least I don’t have anything interesting on my OSG home to be worried about. And is using just uwsgi a performance issue? I think probably not since the expected use case is only once person.</p><h3 id=\"a-future-for-apps\">A Future for Apps</h3><p>This is just the beginning - my plan is to put together a list of use cases for a GUI on a cluster, and then just package them into the core template apps for the developer user to easilyc customize. I have big plans for working on this, and honestly I’m so excited that I find I’m staying up way too late and just egging for the work day to end so I can continue. This idea is so powerful, because it’s using existing technologies to deploy containerized apps on HPC, where you don’t need any special permission. Just to show y’all, here is what it looks like to launch my app template:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>tunel run-app osg singularity/socket/tunel-django <span class=\"nt\">--tag</span><span class=\"o\">=</span>dev <span class=\"nt\">--pull</span></code></pre></div></div><p>I added the pull flag and a custom tag because I am actively developing, and my workflow is to quickly rebuild, push, and then run that command. That then shows me the ssh tunnel command that will immediately connect me to my app on a port in my browser.</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>ssh <span class=\"nt\">-NT</span> <span class=\"nt\">-L</span> 7789:/../tunel/singularity/singularity/socket/tunel-django/singularity-socket-tunel-django.sock sochat1@osg</code></pre></div></div><p>And that’s seriously it. You as the developer user are empowered to make and deploy apps, and they have interfaces, and you don’t need to do something silly like open a port or actually deploy a web server. It’s so stupidly easy - I’m looking around at all these complex web app setups that people have made for HPC over the years and I wonder why they aren’t doing something simpler. Maybe it’s just a space of development that people gave up on, or there are some security things I’m missing. Either way, I’m going to charge forward working on this! It’s too simple, and the idea is to beautiful to do anything else by this point.</p>",
            "url": "https://hpc.social/personal-blog/2022/tunel-apps-for-hpc/",
            
            
            
            
            
            "date_published": "2022-08-04T13:30:00-06:00",
            "date_modified": "2022-08-04T13:30:00-06:00",
            
                "author": "Vanessasaurus"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/ceph-rocksdb-tuning-deep-dive/",
            "title": "Ceph RocksDB Tuning Deep-Dive",
            "summary": null,
            "content_text": "See my post on the Ceph.io blog about tuning RocksDB in Ceph!",
            "content_html": "<p>See my <a href=\"https://ceph.io/en/news/blog/2022/rocksdb-tuning-deep-dive/\">post</a> on the Ceph.io blog about tuning RocksDB in Ceph!</p>",
            "url": "https://hpc.social/personal-blog/2022/ceph-rocksdb-tuning-deep-dive/",
            
            
            
            
            
            "date_published": "2022-07-25T01:00:00-06:00",
            "date_modified": "2022-07-25T01:00:00-06:00",
            
                "author": "Mark Nelson's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/the-utility-vs-the-professional-services-firm/",
            "title": "The Utility vs the Professional Services Firm",
            "summary": null,
            "content_text": "As research computing and data becomes more complex and diverse, we need more professional services firms and fewer utilties(Note: This post is adapted from #127 of the Research Computing Teams Newsletter)I get to talk with a lot of research computing and data teams - software, data, and systems.  Sometimes in these conversations it’s pretty clear that some teams, or the team and their funder, or a team and I, are talking a bit past each other.  And that’s usually because they or we are (currently) operating with very different mental models of how they operate.Some research computing and data teams are operating as Utilities, and see the world through that lens; a growing number are operating as Professional Services Firms.  Others are moving from one to the other, and are at different places along that very abrupt transition.  Some kinds of groups (like bioinformatics cores) are much more likely to already be operating in service mode, while others (like research compute infrastructure teams) are more likely to still think of themselves as utilities.  It varies from place to place, though, depending on local conditions.  But they’re very different models!    Utility service and professional services delivery are very different, and require different funding, management, and career development models.  Image credit: left and right.Utilities, like power companies or garbage collection or municipal potable water, were really the only sensible role models for the first decades of research computing and data teams.  Those teams were entirely about operating large equipment purchased from vendors.  Costs were mostly a big capital expense.  Everyone who needed the utility needed the same thing - undifferentiated flops and bytes, or 60Hz 120VAC.  Because everyone needed the same thing, economies of scale led to natural monopolies; the most reasonable provision model was for the local jurisdiction/institution to own or control a single operator.  Differentiation or strategy, or gaining new customers, weren’t meaningful discussion topics.  The only thing that really makes a difference is scale, which leads to mergers.  Innovation happens slowly, top-down, at the industry-wide scale and usually from the vendors (“hey, did you hear about those new gas compressors Dyneco announced?”), and diffuses outwards.  Employees take pride in and the organization values operational skill and things ticking along smoothly.  Customers value reliability.  The only thing that matters for any individual operator is to operate effectively and to provide the standard service with the right amount of cost: high enough to absorb the available subsidy, low enough to not go broke.  If a customer needs something other than what the utility provides, rather than that being a market opportunity, it’s either an inconvenience or an irrelevance.  The power company or the water utility or the old phone monopoly just doesn’t serve that need.Professional Service Firms — say engineering firms, or architects, or consultancies — are very different beasts.  They might very well have significant capital investment in specialized equipment, but their main selling point and their biggest cost is expertise.  Competing for and retaining that expertise, and developing that expertise in house and amongst their clients, are principal concerns.  As part of a “full-service” offering they they likely have some fairly standard services they offer at the low end, where operating cost and efficiency is vital.  But what the organization values, and the employees enjoy, is at the high-touch end — getting deeply involved with the client work, and being as much a collaborator or partner or “trusted advisor” as a service provider.  Different clients want very different things, and that high-touch high-expertise work is specialized and labour intensive, so the firms themselves need a clear focus; they can’t meet all needs.  Clients can go elsewhere, so there is redundancy and competition, but less than you’d think at a distance.  In civil engineering a geotechnical firm is complementary, not competing, with one that specializes in water resource engineering.As in the rest of our lives, in research computing we need to have utilities.  As research data management matures, institutional or regional data depositories become mature and “enterprise” enough to become utilities, likely run by IT or the Library.  Teaching or CI/CD or MLOps resources for data science or software development are likely best served by this model.  The closer the operations are to standard, something that can be run by IT, the more likely it is to be a utility.  But one has to be careful.  Utilies are commodoties: they tend to get merged together wherever feasible, since scale matters and it’s all undifferentiated commodity provision.As research computing becomes broader and faster changing and more diverse, we need more and more professional services firms, too; nimble groups specialized to particular needs and ready to adapt as those needs change.  As even infrastructure is becoming less one-size-fits-all, and methods for making use of computing and data for diverse fields grow more complex and expertise intensive, the preconditions for the utility model are met in fewer situations than used to be.A lot of research computing teams are interested in providing something more like professional services, but were created in the Utility model, and are stuck there by their funders.  The institutional or external funders still have this very specific (and to their mind time tested and successful) operating model in their plans.  Utilities are funded very differently than professional services firms.  At utility scale, it doesn’t make sense to outsource things, or develop non-standard services (who wants non-standard power coming into their house!)  Funders requirements on eligible expenses may focus almost entirely on the capital spend, and not on operating funding that’s needed to make effective use of the capital, or to be more agile in how services are delivered.Even those teams who aren’t being held back by funders and who want to make the switch to professional services from their original utility model find it a hard transition. There’s no obvious, incremental path to go from providing a standard, stable commodity to changing, specialized, bundles of expertise.  Utilities operate very differently from professional services firms.  They value different things. The models for staff growth are different.  So they have to be managed quiet differently, and there’s no clear path internally from A to B.Besides funding, and internal considerations, utilities and professional services firms are also percieved and valued by their clients very differently.  Utilities’ existing customers don’t want change, and new customers aren’t yet interested in getting advanced app software development suggestions from what they perceive to still be the mobile telephony provider.But research computing and data is changing, increasingly quickly, and the utility approach only meets a piece of these growing needs.  Navigating the transition isn’t going to be easy, for RCD teams, leaders, or funders; but expressing it clearly and talking about it more will maybe mean we’re not talking past each other so often.",
            "content_html": "<h2 id=\"as-research-computing-and-data-becomes-more-complex-and-diverse-we-need-more-professional-services-firms-and-fewer-utilties\">As research computing and data becomes more complex and diverse, we need more professional services firms and fewer utilties</h2><p>(Note: This post is adapted from <a href=\"https://www.researchcomputingteams.org/newsletter_issues/0127\">#127</a> of the <a href=\"https://www.researchcomputingteams.org\">Research Computing Teams Newsletter</a>)</p><p>I get to talk with a lot of research computing and data teams - software, data, and systems.  Sometimes in these conversations it’s pretty clear that some teams, or the team and their funder, or a team and I, are talking a bit past each other.  And that’s usually because they or we are (currently) operating with very different mental models of how they operate.</p><p>Some research computing and data teams are operating as Utilities, and see the world through that lens; a growing number are operating as Professional Services Firms.  Others are moving from one to the other, and are at different places along that very abrupt transition.  Some kinds of groups (like bioinformatics cores) are much more likely to already be operating in service mode, while others (like research compute infrastructure teams) are more likely to still think of themselves as utilities.  It varies from place to place, though, depending on local conditions.  But they’re very different models!</p><figure style=\"width: 45%; float: right;\">  <img alt=\"Utility vs professional services.  Image Credit: left, John Moore (@thejmoore) at Unsplash.com; right, Jason Goodman @jasongoodman_youxventures at Unsplash.com\" src=\"https://www.dursi.ca/assets/imgs/utility-vs-professional-svc.png\" />  <figcaption><i>Utility service and professional services delivery are very different, and require different funding, management, and career development models.  Image credit: <a href=\"https://unsplash.com/photos/0MKzwPmehRE\">left</a> and <a href=\"https://unsplash.com/photos/X8H8vPcelPk\">right</a>.</i></figcaption></figure><p>Utilities, like power companies or garbage collection or municipal potable water, were really the only sensible role models for the first decades of research computing and data teams.  Those teams were entirely about operating large equipment purchased from vendors.  Costs were mostly a big capital expense.  Everyone who needed the utility needed the same thing - undifferentiated flops and bytes, or 60Hz 120VAC.  Because everyone needed the same thing, economies of scale led to <a href=\"https://en.wikipedia.org/wiki/Natural_monopoly\">natural monopolies</a>; the most reasonable provision model was for the local jurisdiction/institution to own or control a single operator.  Differentiation or strategy, or gaining new customers, weren’t meaningful discussion topics.  The only thing that really makes a difference is scale, which leads to mergers.  Innovation happens slowly, top-down, at the industry-wide scale and usually from the vendors (“hey, did you hear about those new gas compressors Dyneco announced?”), and diffuses outwards.  Employees take pride in and the organization values operational skill and things ticking along smoothly.  Customers value reliability.  The only thing that matters for any individual operator is to operate effectively and to provide the standard service with the right amount of cost: high enough to absorb the available subsidy, low enough to not go broke.  If a customer needs something other than what the utility provides, rather than that being a market opportunity, it’s either an inconvenience or an irrelevance.  The power company or the water utility or the <a href=\"https://vimeo.com/355556831\">old phone monopoly</a> just doesn’t serve that need.</p><p>Professional Service Firms — say engineering firms, or architects, or consultancies — are very different beasts.  They might very well have significant capital investment in specialized equipment, but their main selling point and their biggest cost is expertise.  Competing for and retaining that expertise, and developing that expertise in house and amongst their clients, are principal concerns.  As part of a “full-service” offering they they likely have some fairly standard services they offer at the low end, where operating cost and efficiency is vital.  But what the organization values, and the employees enjoy, is at the high-touch end — getting deeply involved with the client work, and being as much a collaborator or partner or “trusted advisor” as a service provider.  Different clients want very different things, and that high-touch high-expertise work is specialized and labour intensive, so the firms themselves need a clear focus; they <em>can’t</em> meet all needs.  Clients can go elsewhere, so there is redundancy and competition, but less than you’d think at a distance.  In civil engineering a geotechnical firm is complementary, not competing, with one that specializes in water resource engineering.</p><p>As in the rest of our lives, in research computing we need to have utilities.  As research data management matures, institutional or regional data depositories become mature and “enterprise” enough to become utilities, likely run by IT or the Library.  Teaching or CI/CD or MLOps resources for data science or software development are likely best served by this model.  The closer the operations are to standard, something that can be run by IT, the more likely it is to be a utility.  But one has to be careful.  Utilies are commodoties: they tend to get merged together wherever feasible, since scale matters and it’s all undifferentiated commodity provision.</p><p>As research computing becomes broader and faster changing and more diverse, we need more and more professional services firms, too; nimble groups specialized to particular needs and ready to adapt as those needs change.  As even infrastructure is becoming less one-size-fits-all, and methods for making use of computing and data for diverse fields grow more complex and expertise intensive, the preconditions for the utility model are met in fewer situations than used to be.</p><p>A lot of research computing teams are interested in providing something more like professional services, but were created in the Utility model, and are stuck there by their funders.  The institutional or external funders still have this very specific (and to their mind time tested and successful) operating model in their plans.  Utilities are funded very differently than professional services firms.  At utility scale, it doesn’t make sense to outsource things, or develop non-standard services (who wants non-standard power coming into their house!)  Funders requirements on eligible expenses may focus almost entirely on the capital spend, and not on operating funding that’s needed to make effective use of the capital, or to be more agile in how services are delivered.</p><p>Even those teams who aren’t being held back by funders and who want to make the switch to professional services from their original utility model find it a hard transition. There’s no obvious, incremental path to go from providing a standard, stable commodity to changing, specialized, bundles of expertise.  Utilities operate very differently from professional services firms.  They value different things. The models for staff growth are different.  So they have to be managed quiet differently, and there’s no clear path internally from A to B.</p><p>Besides funding, and internal considerations, utilities and professional services firms are also percieved and valued by their clients very differently.  Utilities’ existing customers don’t want change, and new customers aren’t yet interested in getting advanced app software development suggestions from what they perceive to still be the mobile telephony provider.</p><p>But research computing and data is changing, increasingly quickly, and the utility approach only meets a piece of these growing needs.  Navigating the transition isn’t going to be easy, for RCD teams, leaders, or funders; but expressing it clearly and talking about it more will maybe mean we’re not talking past each other so often.</p>",
            "url": "https://hpc.social/personal-blog/2022/the-utility-vs-the-professional-services-firm/",
            
            
            
            
            
            "date_published": "2022-07-03T01:00:00-06:00",
            "date_modified": "2022-07-03T01:00:00-06:00",
            
                "author": "Jonathan Dursi's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/ssh-tunnels/",
            "title": "SSH Tunnels",
            "summary": null,
            "content_text": "Today I want to talk about ssh tunnels. Very abstractly, we would want to use an sshtunnel to securely send information. In the case of HPC, you are probably familiar with ssh,(Secure Shell or Secure Socket Shell) when you login to your node. You might do something like this:$ ssh dinosaur@server.address.eduOr if you have a proper setup in your ~/.ssh/config (with a named server) you might just do:$ ssh dinosaurI like to use ssh connection multiplexingso the connection is kept alive for a bit, but I won’t go into detail becausethis post isn’t specifically about the details of ssh. The use case I’m interested in (and the thingthat HPC is very bad at) is how to deploy something interactive on an HPC cluster.SSH Tunnel with PortsGiven that a cluster has exposed ports (either the login node, or both the login node and compute nodes)creating a tunnel is fairly straight forward! In the past I created a tool called forward to handle all the manual steps to get this working, meaning:  Show the user how to set up their ~/.ssh/config (once)  Define (once) parameters like a port, memory, GPUs, and if the cluster has isolated nodes  Start any number of provided apps that come with forward (e.g., jupyter, singularity, etc.)An interaction using forward might look like any of the following:# Run a Singularity container that already exists on your resource (recommended)bash start-node.sh singularity-run /scratch/users/vsochat/share/pytorch-dev.simg# Execute a custom command to the same Singularity containerbash start-node.sh singularity-exec /scratch/users/vsochat/share/pytorch-dev.simg echo \"Hello World\"# Run a Singularity container from a url, `docker://ubuntu`bash start-node.sh singularity-run docker://ubuntu# Execute a custom command to the same containerbash start-node.sh singularity-exec docker://ubuntu echo \"Hello World\"# To start a jupyter notebook in a specific directory ON the cluster resourcebash start.sh jupyter &lt;cluster-dir&gt;# To start a jupyter notebook with tensorflow in a specific directorybash start.sh py3-tensorflow &lt;cluster-dir&gt;Note that the last set of commands are pertaining to notebooks, which is where these tunnels come into play!A notebook is going to be run on a compute node that looks something like the following:$ jupyter notebook --no-browser --port=$PORTAnd if you ran this with a Singularity container, you’d also want to bind jovyan’s home to be the user’s, along with the jupyter config directory:$ singularity exec --home ${HOME} \\    --bind ${HOME}/.local:/home/jovyan/.local \\    --bind ${HOME}/.jupyter:/home/jovyan/.jupyter \\      datascience_notebook.sif jupyter notebook --no-browser --port=$PORT --ip 0.0.0.0As we described earlier here,there are subtle differences between making a tunnel (with a port) given that you have isolated nodes (or not).You can determine this based on your ability to ssh into a non-login node (meaning where your job is running) from “the outside world”that is your computer. If you cannot, your nodes are isolated, which we will discuss next.Isolated NodesLet’s say that we need to create a tunnel (using ports) to an isolated node. This means that we are basically goingto establish a tunnel to the login node, and then from the login node another one to the compute node.We might use a command that looks like this:$ ssh -L $PORT:localhost:$PORT ${RESOURCE} ssh -L $PORT:localhost:$PORT -N \"$MACHINE\" &amp;In the command above, the first half (ssh -L $PORT:localhost:$PORT ${RESOURCE}) is executed on the local machine, which establishes a port forwarding to the login node. The “-L” in the above (from the man pages) :  Specifies that connections to the given TCP port or Unix socket on the local (client) host are to be forwarded to thegiven host and port, or Unix socket, on the remote side.This works by allocating a socket to listen to either a TCPport on the local side, optionally bound to the specifiedbind_address, or to a Unix socket.  Whenever a connection ismade to the local port or socket, the connection is for‐warded over the secure channel, and a connection is made toeither host port hostport, or the Unix socket remote_socket,from the remote machine.Or in layman’s terms:  Forward whatever is running on the second port on my resource to my local machine.Since we are forwarding ports, this would require minimally the login node to expose ports.The next line ssh -L $PORT:localhost:$PORT -N \"$MACHINE\" &amp; is a second command run from the login node, and port forwards it to the compute node, since you can only access the compute node from the login nodes.You’ll notice it looks just like the first, and this works because ssh commands can be chained.The -N says “don’t execute a remote command (and just forward the port).”Finally, the last $MACHINE is the node that the jupyter notebook is running on.Not IsolatedFor HPCs where the compute node is not isolated from the outside world the ssh command for port forwarding first establishes a connection the login node, but then continues to pass on the login credentials to the compute node to establish a tunnel between the localhost and the port on the compute node. The ssh command in this case utilizes the flag -K that forwards the login credentials to the compute node:$ ssh \"$DOMAINNAME\" -l $FORWARD_USERNAME -K -L  $PORT:$MACHINE:$PORT -N  &amp;I’m not sure in practice how common this is anymore. At least at my current employer it’s not even the casethat ports are exposed on the login node! It’s probably better that way, because in cases where you do get ports it’s sort of a “pick a port above this range and hope that no other user picks the same one!” It’s messy. So let’s talk about the case of not having ports exposed next, since this was the entire reason I wanted to write this post!SSH Tunnel with SocketMore than a year ago, I had this realization that a lot of people at Stanford used the “forward” tool, and just for notebooks (and thiswas before they were available via Open OnDemand, which is what I’d recommend to a Stanford user at this point). I decided I wanted to make a new open source tool, “tunel” (an elegant derivation of “tunnel”) vsoch/tunel to make it easyto run what I call “apps” on an HPC cluster. Are there better ways of exposing user interfaces on HPC? Yes, indeed. But not everyonehas easy access. It was also a stubborn “I want this to work” proof of concept. This new tool would be like forward, but a little nicer.Because I, along with every other HPC developer and user, wishes we could have nice things 😭️.At this time I had just started a new role at a national lab, and I realized that none of my old techniques for launchingthe job worked because of the lack of exposed ports. Thinking this was impossible, I abandoned it for a year. But then this last week I found this! I was motivated! I was excited! The basic launch command of the notebook looks like this:$ jupyter notebook --sock /tmp/test.sock --no-browserAnd then with a different looking tunnel, we could forward this socket to the host, and map it to a port! My excitement was then brought downby what led to two days of struggling. I first tried my entire tunel workflow, meaning launching a job on a node,and then running that command, and providing the instruction to the user to create the tunnel as follows:$ ssh -L 8888:/tmp/test.sock -N user@this_hostThat didn’t work (and remember this socket was created on the isolated node, that’s important to remember for later). So I started looking at the socket with “nc”  - “arbitrary TCP and UDP connections and listens” from the login node. The “-U” below is for UNIX sockets:$ nc -U /tmp/test.sockAnd from the head node I saw:Ncat: Connection refused.So then I knew I needed a simpler, dummier example. I got rid of tunel and just ran the notebook command on the head node.Dear reader, it still did not work. I opened an issue and asked Twitter for help. Someone else on Twitter reported that it worked for them, and that (in my opinion) is the challenge and story of HPC - given the huge differences in setups, it’s hard to reproduce what another person does unless you scope to a very specificenvironment or technology and hugely go out of your way to do it. I’m always grateful when someone tries to help, but when the ultimate answer is just“But it works on my machine!” I (and I think all of us) are like:(╯°□°)╯︵ ┻━┻🤣️Please know that is intended to be funny, and I really am grateful for the attempt to help! Anyway, the first night I was devastated because I was so excited about the possibility of this working! But of course (as it usually does) my quasi-sadness turned again into relentless stubborn-ness, and for my SaturdayI embarked on trying everything. I call this the stubborn brute force approach, and it actually leads to some pretty good outcomes?Socket from Login NodeFirst from the login node, I started reading about flags in detail, again from the man pages. It occurred to me that the suggested command included “-L” (discussed earlier) but there were a ton of other flags to try, and maybe I need them for my setup? The command that wound up working (after much trial and error) was just:# Running on login node$ ssh -NT -L 8888:/tmp/test.sock user@serverAnd here again was the suggested command:$ ssh -L 8888:/tmp/test.sock -N user@this_hostSo they are very similar - and the main difference is the -T is to “Disable pseudo-terminal allocation.”So I suspect (also based on the version of ssl I’m using) that without the flag, you might be making a request for a pty to the server(more details here) and then it could abort. Adding the flag just skips this, because we don’t need that - we just need the simple forward. And yes, this indeed feels very specific to your ssh setup, version of ssh, and server configuration. Of course, this was only the beginning of figuring things out, because I had no idea how to get this working from one level deeper - an isolated compute node.Socket with Isolated NodesRemember that when we created the socket on the isolated node and we tried this out from the login node:$ nc -U /tmp/test.sockAnd the result was this:Ncat: Connection refused.My spidey senses were telling me that this should work. Indeed, when I ssh into the isolated node from the login node,that same command allowed me to connect (meaning it hung / there was no error output). So my first task, I decided, was to tryand “forward” this socket to the login node. Again, back to the man pages! I wound up with something like this (run from the login node):$ ssh isolated-node -NT -L /home/dinosaur/login-node.sock:/home/dinosaur/jupyter.sockThe above is again using -L but instead of a port (which aren’t exposed) we are using a socket! It’s kind of neat you can switch out those two. When I tried the same nc command from the loginnode, we had progress (no connection refused message!) 🎉️ And then I moved this up one level to see if I could make this same request from my local machine, sort of combining the first command that worked with the login node notebook with this one. That looked like this (and yes this took more trial and error):$ ssh -NT user@server ssh isolated-node -NT -L /home/dinosaur/login-node.sock:/home/dinosaur/jupyter.sockAnd to confirm it was working, I’d ssh into the server and again run that nc command to ensure that the newly forwarded socket was readable fromthe login node. After this, again with more trial and error, I tried running a second command to just forward that (now working socket) to my host.That eventually looked like this:# And another for the local socket$ ssh -NT -L 8899:/home/dinosaur/login-node.sock user@serverAnd then (all together now!) I tried putting them together.$ ssh -NT -L 8899:/home/dinosaur/login-node.sock user@server ssh isolated-node \\       -NT -L /home/dinosaur/login-node.sock:/home/dinosaur/jupyter.sockAnd then I spent some time integrating it into tunel, and surprise! the first implementation didn’t work. The first bug was that I needed to clean up old sockets each time the “same” app was run (determined by the job name and organizational namespace so the user can only run one of a particular interactive app at once, and not forget about previous runs). The second issue was about opening the tunnel - it didn’t seem to work if the process exited and/or it was run in a subshell (that also probably exits). I realized that (for the time being) running this connection step on behalf of the user, since it’s something the user should have more control over, probably wasn’t the right way to go. If the user hasn’t added something like an rsa key to ~/.ssh/authorized_keys on their clusters, it would also ask for a password interactively, making it harder for me to manage. So for simplicity sake, and assuming that we really should put the user in control of deciding when to start/stop the tunnel, I simply print the full ssh command in the terminal and let them copy paste it. A successful connection might then prompt them for their password for that second ssh, which (by default) I don’t think is carrying forward auth from the first.So that was my adventure! Mind you, this entire adventure was only about two days, and that included time to write this post, so I still have lots in front of me to work on. However, with these updated commands (and some nice tweaks from Python’s rich library) I quickly had a nice set of commands to run and stop an app with an interactive jupyter notebook, and using sockets on isolated nodes!$ tunel run-app server slurm/socket/jupyter$ tunel stop-app server slurm/socket/jupyterAs a sidenote, one thing I like about rich is that it puts the aesthetic as a first class citizen.So many tools just don’t consider this, and I love that with rich I can think about colors, presentation,and even animations like spinners!Getting a socket working  means I’ll be able to continue working on this library (hooray!) so if you have ideas or requests for appsyou’d like to run on HPC, assuming just this basic technology, please give me a ping and I’d love to chat and support them.I’m also going to be requesting an allocation on the Open Science Grid, which hopefully will give me other kinds of clustersto test on. I hope this was interesting to read, thanks for doing that!",
            "content_html": "<p>Today I want to talk about ssh tunnels. Very abstractly, we would want to use an sshtunnel to securely send information. In the case of HPC, you are probably familiar with ssh,(Secure Shell or Secure Socket Shell) when you login to your node. You might do something like this:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>ssh dinosaur@server.address.edu</code></pre></div></div><p>Or if you have a proper setup in your <code class=\"language-plaintext highlighter-rouge\">~/.ssh/config</code> (with a named server) you might just do:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>ssh dinosaur</code></pre></div></div><p>I like to use <a href=\"https://en.wikibooks.org/wiki/OpenSSH/Cookbook/Multiplexing\" target=\"_blank\">ssh connection multiplexing</a>so the connection is kept alive for a bit, but I won’t go into detail becausethis post isn’t specifically about the details of ssh. The use case I’m interested in (and the thingthat HPC is very bad at) is how to deploy something interactive on an HPC cluster.</p><h2 id=\"ssh-tunnel-with-ports\">SSH Tunnel with Ports</h2><p>Given that a cluster has exposed ports (either the login node, or both the login node and compute nodes)creating a tunnel is fairly straight forward! In the past I created a tool called <a href=\"https://github.com/vsoch/forward\" target=\"_blank\">forward</a> to handle all the manual steps to get this working, meaning:</p><ol class=\"custom-counter\">  <li>Show the user <a href=\"https://github.com/vsoch/forward#ssh-config\" target=\"_blank\">how to set up their ~/.ssh/config</a> (once)</li>  <li>Define (once) parameters like a port, memory, GPUs, and if the cluster has isolated nodes</li>  <li>Start any number of provided apps that come with forward (e.g., jupyter, singularity, etc.)</li></ol><p>An interaction using forward might look like any of the following:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\"># Run a Singularity container that already exists on your resource (recommended)</span>bash start-node.sh singularity-run /scratch/users/vsochat/share/pytorch-dev.simg<span class=\"c\"># Execute a custom command to the same Singularity container</span>bash start-node.sh singularity-exec /scratch/users/vsochat/share/pytorch-dev.simg <span class=\"nb\">echo</span> <span class=\"s2\">\"Hello World\"</span><span class=\"c\"># Run a Singularity container from a url, `docker://ubuntu`</span>bash start-node.sh singularity-run docker://ubuntu<span class=\"c\"># Execute a custom command to the same container</span>bash start-node.sh singularity-exec docker://ubuntu <span class=\"nb\">echo</span> <span class=\"s2\">\"Hello World\"</span><span class=\"c\"># To start a jupyter notebook in a specific directory ON the cluster resource</span>bash start.sh jupyter &lt;cluster-dir&gt;<span class=\"c\"># To start a jupyter notebook with tensorflow in a specific directory</span>bash start.sh py3-tensorflow &lt;cluster-dir&gt;</code></pre></div></div><p>Note that the last set of commands are pertaining to notebooks, which is where these tunnels come into play!A notebook is going to be run on a compute node that looks something like the following:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>jupyter notebook <span class=\"nt\">--no-browser</span> <span class=\"nt\">--port</span><span class=\"o\">=</span><span class=\"nv\">$PORT</span></code></pre></div></div><p>And if you ran this with a Singularity container, you’d also want to bind jovyan’s home to be the user’s, along with the jupyter config directory:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>singularity <span class=\"nb\">exec</span> <span class=\"nt\">--home</span> <span class=\"k\">${</span><span class=\"nv\">HOME</span><span class=\"k\">}</span> <span class=\"se\">\\</span>    <span class=\"nt\">--bind</span> <span class=\"k\">${</span><span class=\"nv\">HOME</span><span class=\"k\">}</span>/.local:/home/jovyan/.local <span class=\"se\">\\</span>    <span class=\"nt\">--bind</span> <span class=\"k\">${</span><span class=\"nv\">HOME</span><span class=\"k\">}</span>/.jupyter:/home/jovyan/.jupyter <span class=\"se\">\\ </span>     datascience_notebook.sif jupyter notebook <span class=\"nt\">--no-browser</span> <span class=\"nt\">--port</span><span class=\"o\">=</span><span class=\"nv\">$PORT</span> <span class=\"nt\">--ip</span> 0.0.0.0</code></pre></div></div><p>As we described earlier <a href=\"https://github.com/vsoch/forward#ssh-port-forwarding-considerations\" target=\"_blank\">here</a>,there are subtle differences between making a tunnel (with a port) given that you have isolated nodes (or not).You can determine this based on your ability to ssh into a non-login node (meaning where your job is running) from “the outside world”that is your computer. If you cannot, your nodes are isolated, which we will discuss next.</p><h3 id=\"isolated-nodes\">Isolated Nodes</h3><p>Let’s say that we need to create a tunnel (using ports) to an isolated node. This means that we are basically goingto establish a tunnel to the login node, and then from the login node another one to the compute node.We might use a command that looks like this:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>ssh <span class=\"nt\">-L</span> <span class=\"nv\">$PORT</span>:localhost:<span class=\"nv\">$PORT</span> <span class=\"k\">${</span><span class=\"nv\">RESOURCE</span><span class=\"k\">}</span> ssh <span class=\"nt\">-L</span> <span class=\"nv\">$PORT</span>:localhost:<span class=\"nv\">$PORT</span> <span class=\"nt\">-N</span> <span class=\"s2\">\"</span><span class=\"nv\">$MACHINE</span><span class=\"s2\">\"</span> &amp;</code></pre></div></div><p>In the command above, the first half (<code class=\"language-plaintext highlighter-rouge\">ssh -L $PORT:localhost:$PORT ${RESOURCE}</code>) is executed on the local machine, which establishes a port forwarding to the login node. The “-L” in the above (from the <a href=\"https://linuxcommand.org/lc3_man_pages/ssh1.html\" target=\"_blank\">man pages</a>) :</p><blockquote>  <p>Specifies that connections to the given TCP port or Unix socket on the local (client) host are to be forwarded to thegiven host and port, or Unix socket, on the remote side.This works by allocating a socket to listen to either a TCPport on the local side, optionally bound to the specifiedbind_address, or to a Unix socket.  Whenever a connection ismade to the local port or socket, the connection is for‐warded over the secure channel, and a connection is made toeither host port hostport, or the Unix socket remote_socket,from the remote machine.</p></blockquote><p>Or in layman’s terms:</p><blockquote>  <p>Forward whatever is running on the second port on my resource to my local machine.</p></blockquote><p>Since we are forwarding ports, this would require minimally the login node to expose ports.The next line <code class=\"language-plaintext highlighter-rouge\">ssh -L $PORT:localhost:$PORT -N \"$MACHINE\" &amp;</code> is a second command run from the login node, and port forwards it to the compute node, since you can only access the compute node from the login nodes.You’ll notice it looks just like the first, and this works because ssh commands can be chained.The <code class=\"language-plaintext highlighter-rouge\">-N</code> says “don’t execute a remote command (and just forward the port).”Finally, the last <code class=\"language-plaintext highlighter-rouge\">$MACHINE</code> is the node that the jupyter notebook is running on.</p><h3 id=\"not-isolated\">Not Isolated</h3><p>For HPCs where the compute node is not isolated from the outside world the ssh command for port forwarding first establishes a connection the login node, but then continues to pass on the login credentials to the compute node to establish a tunnel between the localhost and the port on the compute node. The ssh command in this case utilizes the flag <code class=\"language-plaintext highlighter-rouge\">-K</code> that forwards the login credentials to the compute node:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>ssh <span class=\"s2\">\"</span><span class=\"nv\">$DOMAINNAME</span><span class=\"s2\">\"</span> <span class=\"nt\">-l</span> <span class=\"nv\">$FORWARD_USERNAME</span> <span class=\"nt\">-K</span> <span class=\"nt\">-L</span>  <span class=\"nv\">$PORT</span>:<span class=\"nv\">$MACHINE</span>:<span class=\"nv\">$PORT</span> <span class=\"nt\">-N</span>  &amp;</code></pre></div></div><p>I’m not sure in practice how common this is anymore. At least at my current employer it’s not even the casethat ports are exposed on the login node! It’s probably better that way, because in cases where you do get ports it’s sort of a “pick a port above this range and hope that no other user picks the same one!” It’s messy. So let’s talk about the case of not having ports exposed next, since this was the entire reason I wanted to write this post!</p><h2 id=\"ssh-tunnel-with-socket\">SSH Tunnel with Socket</h2><p>More than a year ago, I had this realization that a lot of people at Stanford used the “forward” tool, and just for notebooks (and thiswas before they were available via Open OnDemand, which is what I’d recommend to a Stanford user at this point). I decided I wanted to make a new open source tool, “tunel” (an elegant derivation of “tunnel”) <a href=\"https://github.com/vsoch/tunel\" target=\"_blank\">vsoch/tunel</a> to make it easyto run what I call “apps” on an HPC cluster. Are there better ways of exposing user interfaces on HPC? Yes, indeed. But not everyonehas easy access. It was also a stubborn “I want this to work” proof of concept. This new tool would be like forward, but a little nicer.Because I, along with every other HPC developer and user, wishes we could have nice things 😭️.</p><p>At this time I had just started a new role at a national lab, and I realized that none of my old techniques for launchingthe job worked because of the lack of exposed ports. Thinking this was impossible, I abandoned it for a year. But then this last week I found <a href=\"https://github.com/jupyter/notebook/pull/4835\" target=\"_blank\">this</a>! I was motivated! I was excited! The basic launch command of the notebook looks like this:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>jupyter notebook <span class=\"nt\">--sock</span> /tmp/test.sock <span class=\"nt\">--no-browser</span></code></pre></div></div><p>And then with a different looking tunnel, we could forward this socket to the host, and map it to a port! My excitement was then brought downby what led to two days of struggling. I first tried my entire tunel workflow, meaning launching a job on a node,and then running that command, and providing the instruction to the user to create the tunnel as follows:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>ssh <span class=\"nt\">-L</span> 8888:/tmp/test.sock <span class=\"nt\">-N</span> user@this_host</code></pre></div></div><p>That didn’t work (and remember this socket was created on the isolated node, that’s important to remember for later). So I started looking at the socket with “nc”  - “arbitrary TCP and UDP connections and listens” from the login node. The “-U” below is for UNIX sockets:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>nc <span class=\"nt\">-U</span> /tmp/test.sock</code></pre></div></div><p>And from the head node I saw:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Ncat: Connection refused.</code></pre></div></div><p>So then I knew I needed a simpler, dummier example. I got rid of tunel and just ran the notebook command on the head node.Dear reader, it still did not work. I <a href=\"https://github.com/jupyter/notebook/issues/6459\" target=\"_blank\">opened an issue</a> and asked <a href=\"https://twitter.com/vsoch/status/1540546526044250112\" target=\"_blank\">Twitter for help</a>. Someone else on Twitter reported that <a href=\"https://twitter.com/al3x609/status/1540846694262243328\" target=\"_blank\">it worked for them</a>, and that (in my opinion) is the challenge and story of HPC - given the huge differences in setups, it’s hard to reproduce what another person does unless you scope to a very specificenvironment or technology and hugely go out of your way to do it. I’m always grateful when someone tries to help, but when the ultimate answer is just“But it works on my machine!” I (and I think all of us) are like:</p><p><span style=\"font-size: 50px; color: darkorchid;\">(╯°□°)╯︵ ┻━┻</span></p><p>🤣️</p><p>Please know that is intended to be funny, and I really am grateful for the attempt to help! Anyway, the first night I was devastated because I was so excited about the possibility of this working! But of course (as it usually does) my quasi-sadness turned again into relentless stubborn-ness, and for my SaturdayI embarked on trying everything. I call this the stubborn brute force approach, and it actually leads to some pretty good outcomes?</p><h3 id=\"socket-from-login-node\">Socket from Login Node</h3><p>First from the login node, I started reading about flags in detail, again from the <a href=\"https://linuxcommand.org/lc3_man_pages/ssh1.html\" target=\"_blank\">man pages</a>. It occurred to me that the suggested command included “-L” (discussed earlier) but there were a ton of other flags to try, and maybe I need them for my setup? The command that wound up working (after much trial and error) was just:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\"># Running on login node</span><span class=\"nv\">$ </span>ssh <span class=\"nt\">-NT</span> <span class=\"nt\">-L</span> 8888:/tmp/test.sock user@server</code></pre></div></div><p>And here again was the suggested command:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>ssh <span class=\"nt\">-L</span> 8888:/tmp/test.sock <span class=\"nt\">-N</span> user@this_host</code></pre></div></div><p>So they are very similar - and the main difference is the <code class=\"language-plaintext highlighter-rouge\">-T</code> is to “Disable pseudo-terminal allocation.”So I suspect (also based on the version of ssl I’m using) that without the flag, you might be making a request for a pty to the server(<a href=\"https://stackoverflow.com/questions/10330678/gitolite-pty-allocation-request-failed-on-channel-0/10346575#10346575\" target=\"_blank\">more details here</a>) and then it could abort. Adding the flag just skips this, because we don’t need that - we just need the simple forward. And yes, this indeed feels very specific to your ssh setup, version of ssh, and server configuration. Of course, this was only the beginning of figuring things out, because I had no idea how to get this working from one level deeper - an isolated compute node.</p><h3 id=\"socket-with-isolated-nodes\">Socket with Isolated Nodes</h3><p>Remember that when we created the socket on the isolated node and we tried this out from the login node:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>nc <span class=\"nt\">-U</span> /tmp/test.sock</code></pre></div></div><p>And the result was this:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Ncat: Connection refused.</code></pre></div></div><p>My spidey senses were telling me that this should work. Indeed, when I ssh into the isolated node from the login node,that same command allowed me to connect (meaning it hung / there was no error output). So my first task, I decided, was to tryand “forward” this socket to the login node. Again, back to the man pages! I wound up with something like this (run from the login node):</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>ssh isolated-node <span class=\"nt\">-NT</span> <span class=\"nt\">-L</span> /home/dinosaur/login-node.sock:/home/dinosaur/jupyter.sock</code></pre></div></div><p>The above is again using <code class=\"language-plaintext highlighter-rouge\">-L</code> but instead of a port (which aren’t exposed) we are using a socket! It’s kind of neat you can switch out those two. When I tried the same nc command from the loginnode, we had progress (no connection refused message!) 🎉️ And then I moved this up one level to see if I could make this same request from my local machine, sort of combining the first command that worked with the login node notebook with this one. That looked like this (and yes this took more trial and error):</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>ssh <span class=\"nt\">-NT</span> user@server ssh isolated-node <span class=\"nt\">-NT</span> <span class=\"nt\">-L</span> /home/dinosaur/login-node.sock:/home/dinosaur/jupyter.sock</code></pre></div></div><p>And to confirm it was working, I’d ssh into the server and again run that nc command to ensure that the newly forwarded socket was readable fromthe login node. After this, again with more trial and error, I tried running a second command to just forward that (now working socket) to my host.That eventually looked like this:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\"># And another for the local socket</span><span class=\"nv\">$ </span>ssh <span class=\"nt\">-NT</span> <span class=\"nt\">-L</span> 8899:/home/dinosaur/login-node.sock user@server</code></pre></div></div><p>And then (all together now!) I tried putting them together.</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>ssh <span class=\"nt\">-NT</span> <span class=\"nt\">-L</span> 8899:/home/dinosaur/login-node.sock user@server ssh isolated-node <span class=\"se\">\\</span>       <span class=\"nt\">-NT</span> <span class=\"nt\">-L</span> /home/dinosaur/login-node.sock:/home/dinosaur/jupyter.sock</code></pre></div></div><p>And then I spent some time integrating it into tunel, and <em>surprise!</em> the first implementation didn’t work. The first bug was that I needed to clean up old sockets each time the “same” app was run (determined by the job name and organizational namespace so the user can only run one of a particular interactive app at once, and not forget about previous runs). The second issue was about opening the tunnel - it didn’t seem to work if the process exited and/or it was run in a subshell (that also probably exits). I realized that (for the time being) running this connection step on behalf of the user, since it’s something the user should have more control over, probably wasn’t the right way to go. If the user hasn’t added something like an rsa key to <code class=\"language-plaintext highlighter-rouge\">~/.ssh/authorized_keys</code> on their clusters, it would also ask for a password interactively, making it harder for me to manage. So for simplicity sake, and assuming that we really should put the user in control of deciding when to start/stop the tunnel, I simply print the full ssh command in the terminal and let them copy paste it. A successful connection might then prompt them for their password for that second ssh, which (by default) I don’t think is carrying forward auth from the first.</p><p>So that was my adventure! Mind you, this entire adventure was only about two days, and that included time to write this post, so I still have lots in front of me to work on. However, with these updated commands (and some nice tweaks from Python’s <a href=\"https://github.com/Textualize/rich\" target=\"_blank\">rich</a> library) I quickly had a nice set of commands to run and stop an app with an interactive jupyter notebook, and using sockets on isolated nodes!</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>tunel run-app server slurm/socket/jupyter<span class=\"nv\">$ </span>tunel stop-app server slurm/socket/jupyter</code></pre></div></div><p>As a sidenote, one thing I like about rich is that it puts the aesthetic as a first class citizen.So many tools just don’t consider this, and I love that with rich I can think about colors, presentation,and even animations like spinners!</p><p>Getting a socket working  means I’ll be able to continue working on this library (hooray!) so if you have ideas or requests for appsyou’d like to run on HPC, assuming just this basic technology, please give me a ping and I’d love to chat and support them.I’m also going to be requesting an allocation on the Open Science Grid, which hopefully will give me other kinds of clustersto test on. I hope this was interesting to read, thanks for doing that!</p>",
            "url": "https://hpc.social/personal-blog/2022/ssh-tunnels/",
            
            
            
            
            
            "date_published": "2022-06-26T13:30:00-06:00",
            "date_modified": "2022-06-26T13:30:00-06:00",
            
                "author": "Vanessasaurus"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/research-software-registries/",
            "title": "Research Software Registries",
            "summary": null,
            "content_text": "This post spurred from some original thinking about research software registries, and my recent discovery of the SciCodes Consortium, which I’m excited to find (and a bit surprised I didn’t earlier given my experience with research software and registries)! Since I’ve developed registries and been involved extensively in communities that develop standards and tooling for them, I’ve naturally been ruminating over ideas for several months, and hoping to find others that are motivated to think about similar things. This is the motivation of this post - to ruminate, share my thinking, and think together about ideas. You can read the content, or listen to the ideas below.vsoch · Research Software RegistriesWhy do we want research software registries?Research software registries have value when they are deployed for a specific context. However,I’m not convinced that a research software registry, at the most basic form providing archives with DOIS and metadata, is a useful thing in and of itself. It’s adding complexity and redundancy to an already cluttered ecosystem. The reason is because the source of truth of software is usually the source code in version control, e.g., the GitHub repository, which often already has support for features we need to enable easy citation (CITATION.cff), tagged releases, and programmatically accessible metadata. In this context, any kind of registry that provides another identifier and points to the first is providing redundant information. The only potential benefit is grouping and curation, which I would then argue should still point to the version control and/or a specific release as a source of truth.I’m also not convinced that we have established an actual use case of “searching a registry for software.” What happens in labs and communities is that you establish communities around the software, and then there are established workflows or slack communities or GitHub organizations to join around that. Most labs already have chosen languages, and even software pipelines that new members extend or work on. I would even go as far to say that for some (myself included) I don’t find research software, but it finds me. It appears as a link in some social media or chat channel, and I click the link and then there are about 15 seconds during which I make a determination if the software can help me to solve a problem that I have, or if it looks easy, professional, and/or fun and I simply want to try it out. If the answer is “yes” then I add it to a list in a Google Document with other things to try out when I have time. If not, I close the lab and life moves on. But I want to point out that nowhere in this workflow do I explicitly go looking for software. The software often finds me, and then I keep a mental cache of “tools that I’ve seen” and go back to it when the use case arises.So being able to answer this question about wanting research software registries is especially challenging because I’m not sure I’ve ever wanted one.Unless there is a specific kind of context around a registry (e.g., search for a specific name in a package manager to use, or look for an already assembled workflow) I haven’t been able to convince myself (yet) that I would find a use for one. I could be wrong about this, however, because as we know, people (myself included) are fairly bad at predicting the future, and perhaps there could be some future where “checking a research software registry” is a part of a daily workflow. I am skeptical because I think that a context is needed. Even if some central source of software ability truth was established, would it not be the case that a graduate student or researcher needs to go there with a use case or context in mind? I can’t imagine just mindlessly browsing for the sake of browsing. It’s akin to search engines - we are usually looking for something very specific. We don’t search without a purpose. The question here then is, what is the purpose?Research Software Registries with a PurposeA very good example of purpose comes down to workflows. This is the “I need to perform this specific function and I want to use what many others have done before me and not re-invent the wheel.” The minimum example of a workflow registry would be a search interface that indexes pipelines that are perhaps stored in version control. And extended version of that includes being able to provide structured inputs, outputs, and arguments, so the registry can programmatically provide this information to tools. You can then also quickly see how changing this to be general inputs/outputs of software (and functions within) and entrypoints of containers can quickly become a more generalized registry for software that could be used by any workflow manager that knows how to consume its information. However, there is a fine line here, because when we talk about I/O we are goingsquarely into workflow management territoty, and again in my opinion, we have to be careful about that scope. The closest thing that comes to mind for providing workflows as a service is something like openneuro that has a beautiful idea of “Get your data into this standard format and we will serve it and provide other easy ways to analyze it.” This kind of success story tells me that perhaps there is something to say for developing anything related to processing or pipelines in the context of a community. You can’t create the perfect registry for every scientific discipline, or perhaps you can do a mediocre job at trying, but perhaps if you scope to a specific one you can do a very good job. I’ve found the same to be true with software - it’s often better to do one or few things very well than more things kind of mediocre.A Provider of Identifiers?I’m skeptical when I hear that people want to apply our traditional model of publication (e.g., having a DOI) to software. The reason isn’t because I don’t value means to support reproducibility (and knowing the exact version of something that was used) but rather that we already have means to tag specific versions of software, and means that fit into a well-established ecosystem: package managers, versions, and releases. To think that a single frozen version of software is “the correct unit to provide” I also disagree with. Software is a living, and changing entity, and when it truly does “freeze” and stops being worked on, unlike a DOI in the academic space, this is sort of its death. The correct entrypoint for a piece of software, in my opinion, is the current version on version control, from where you could decide to pin a particular release or install a particular version from a package manager. But to provide a single frozen DOI that is wrapping some other version / release of the software? It doesn’t make sense to me. It’s adding additional complexity that’s not needed. So my opinion (as I’ve shared before) is that we should be thinking more about preserving specific timepoints in package managers, and not adding on an artificially created layer of “DOI” that seems (in my opinion) more of a reflection of our need to shove things into an academic framework we are comfortable with than anything else.So (I hope) that the purpose of a research software registry would not just be to provide DOIs. That doesn’t help me get my work done at the end of the day. All that said, I don’t think there can be a singular answer for purpose. I think the purpose ultimately comes down to the institution (or community) and the specific goals of the registry. For this reason there is no one answer for what a registry should look like or provide, and it is (or will be) challenging to define attributes that “any registry should have.”What is my purpose?You cut butter!Just kidding :_) I’ve been ruminating on this idea for quite some time, and namely because I’m motivated to build a new kind of research software registry, but first I need to convince myself of a meaningful purpose. While I don’t have my convincing answer yet (but I do have a sense of direction) the way I’ve been thinking about this is to provide a set of questions or use cases that seem plausible. It seems like most people are asking “What kind of information should we have in a registry” but I think this isn’t exactly the question I’m interested in - I want to know:  What do you want to do next with the software you find?This is important because it’s going to drive the context and purpose of the registry. Here are a few examples:  I want to quickly try this out → a registry that can deploy a developer environment  I want to find if this is in a package manager → a reproducible install  I want to use this with a workflow manager → this is some kind of workflow hub    I want to see inputs / outputs / entrypoints → support to workflow tools    I want to install this on HPC → I want a module deployment or similar    I want to cite this → use case akin to CITATION.cff    I want to understand dependencies of an ecosystem → a registry deploying something akin to citelang    I want to see all my options to do X → a domain or categorical registry    I want to see new and noteworthy libraries → a registry with advanced filtering and ranking    I want to see change over time → a registry with a layer of analysis tools  Indeed many of the above contexts require additional information. For example, if we want to be able to ask what software is specifically used to perform X, we need a set of functions that are common to a domain, and then to annotate specific software (or even functions) that do it. If we want to then ask “Which of these is the best?” we need to then generate benchmarks to measure this functionality. E.g., how long does it take to run? What are the inputs and outputs and are they correct? What are resource needs? It would be an incredibly cool thing to be able to ask these questions, but an enormous amount of work for any particular scientific domain. As an example of thinking about functional needs, we might look to brain imaging, which is arguably a subfield of neuroinformatics. We might define custom processing functions like thresholding, registration, normalization, or creating regions of interest, tag specific functions that can do each, and then collect and share metrics about the degree to which easy is successful to do each. Arguably, if I wanted to do this I would create wrappers to workflow managers (akin to Snakemake Wrappers) that not only measure metrics, but make it easy for people to quickly use it in their work.It needs to be easyWhether I’m thinking about being a user of a research software registry or a developer, it just needs to be easy. Here are some ideas around that.Re-inventing the wheel?I come with the experience of deploying a custom container registry (Singularity Hub) years ago, and then being involved in standards committees (the Open Container Initiative) that develop generalized specifications that now drive the most common software (container) registries. I’ve also developed registry proxies that do interesting things, along with a Python OCI registry, and I’m the main developer for oras-py (ORAS == OCI Registry as Storage). So believe me when I say that in terms of storing blobs and metadata about them, I don’t think we should re-invent the wheel. Any new registry I create is going to start with these standards. You might disagree, and that’s OK. But I think people have thought long and hard about these things, and we are stronger for working together on them over always making our own new thing.As a supplement to that, I want to point out one of the biggest challenges in our community. The majority of research software, I would argue, doesn’t get used beyond the lab it’s created for. Said lab might submit or include it in a paper, and then they get their publication and move on. This is reflective of many things, and I’ll review them here. The first is our funding model - we maybe can fund working on a piece of software only up until the funding dries out, and then it becomes an abandoned repository, if it’s made publicly available. The second is our incentive model - the academic community is focused on writing papers, so once you get there, you don’t have reason to consider the long term impact of the software. The third is communication. It is actually much easier to throw together your own library than to have to search and then try contributing to someone else’s.I say this because I don’t think the way that things are are necessarily the fault of anyone - we are all agents responding to incentives and resources available.But then on the flip side - these observations beg to ask what leads to software that is successful, on a community level? I think a few things can happen. Either someone puts time and energy into establishing community, period, meaning bringing together people that are working on common goals and explicitly asking “How can we do this together,” or what I’ve seen with more commercial open source - having enough money or power that you can create strong branding and community just by way of having the funds for it.  I’ve talked about this a few times before and it’s not necessarily bad, but it’s unfair at best. Software that maybe would not be successful by its own merit rises to the top, and really great software that doesn’t have those resources does not. That said, I’ve also seen sort of mediocre software get much better and earn its reputation, so I can’t say it’s a completely wrong dynamic.Is the answer Mooooar Metadata?As we design the “perfect set of information” we want provided for any piece of software, we need to put people first.We have to ask ourselves what are people willing to do, and generally people aren’t wanting to spend inordinate amounts of extra time defining metadata or inputs/outputs for their custom scripts. This was a point also brought up by Paula in the SciCodes meeting and I am 100% behind it. If we require extensive metadata about software, it needs to be done in an automated fashion. In practice when I think of archives for software, I’m just not that motivated to provide more than the absolute minimum to click the “submit” button.Do people know what they want?One of the hardest things about this kind of problem is that people don’t often know what they want. And actually - I’d extend that to software in general. Think of common tools like git (version control) or containers.Could most people have told you in advance about the designs for these tools? I suspect likely not.This is often the game that software developers play - we imagine new ways of doing things that scratch an itchor have a problem that we have, and then hand over our duct taped laden prototype to others and we’re  likehey, is this useful to you? And often the response in radio silence, but then sometimes it’s a resounding, “WoW, yes!”So I’m going to throw out this idea that people generally don’t know what they want until they see it, touch it and try it.This is also why I want to inspire you to take some time to think about your specific needs and motivation for wanting(on a high level) to browse and interact with research software. What are the compelling reasons for this registry,for you?This is actually really fun to think about, because what even is a research software registry? Is it a place to find software to plug into workflows? Does it provide ABI or more general function signatures to help you plug into workflows? Does it provide a citation? A container? An interactive environment? Dependency graph? Something else? This is inded why this problem is so hard - there are so many ways to thinkabout this basic concept. And that’s kind of what makes it fun too? But also what makes it hard. Personally speaking sinceI’m more interested in building things I find myself ruminating about details for a specific use case. And since I’m a developer and craving better support for developer environments, this tends to be where my brain goes. And have you noticed I haven’t givena direct answer for what is a research software registry? It’s 1. because I don’t know, and 2. because we are trying to define a registry for a kind of output that we don’t even have an agreed upon definition for yet! So perhaps the definition will happen on the level of the deployment or institution? Anyway, I hope you take the opportunity to discuss with your peers, pets, and even yourself, to try and answer this question.SummaryTo summarize, I’m spending a lot of time thinking about this, and less in an “I’m an academic that wants DOIs and metadata” and more in a “I am a software engineer that wants to build something that I actually find useful.” Might I scratch itches along the way? Sure. And I do have some early ideas that I plan to hack on before sharing publicly. In the meantime, I do hope you are interested in some of these ideas and take time to write or introspect yourself.And on a higher level, I really like this format of writing and speaking, where the speaking isn’t formal enough to be a talk that you put together and practice for weeks (I put this all together in an afternoon) but it still is a media format that literally gives a voice.",
            "content_html": "<p>This post spurred from some original thinking about <a href=\"https://rse-ops.github.io/proposals/proposals/drafts/research-software-registry/\" target=\"_blank\">research software registries</a>, and my recent discovery of the <a href=\"https://scicodes.net/\" target=\"_blank\">SciCodes Consortium</a>, which I’m excited to find (and a bit surprised I didn’t earlier given my experience with research software and registries)! Since I’ve developed registries and been involved extensively in communities that develop standards and tooling for them, I’ve naturally been ruminating over ideas for several months, and hoping to find others that are motivated to think about similar things. This is the motivation of this post - to ruminate, share my thinking, and think together about ideas. You can read the content, or listen to the ideas below.</p><div style=\"font-size: 10px; color: #cccccc; overflow: hidden; white-space: nowrap; font-family: Interstate,Lucida Grande,Lucida Sans Unicode,Lucida Sans,Garuda,Verdana,Tahoma,sans-serif; font-weight: 100;\"><a href=\"https://soundcloud.com/vsoch\" style=\"color: #cccccc; text-decoration: none;\" target=\"_blank\" title=\"vsoch\">vsoch</a> · <a href=\"https://soundcloud.com/vsoch/research-software-registries\" style=\"color: #cccccc; text-decoration: none;\" target=\"_blank\" title=\"Research Software Registries\">Research Software Registries</a></div><h2 id=\"why-do-we-want-research-software-registries\">Why do we want research software registries?</h2><p>Research software registries have value when they are deployed for a specific context. However,I’m not convinced that a research software registry, at the most basic form providing archives with DOIS and metadata, is a useful thing in and of itself. It’s adding complexity and redundancy to an already cluttered ecosystem. The reason is because the source of truth of software is usually the source code in version control, e.g., the GitHub repository, which often already has support for features we need to enable easy citation (CITATION.cff), tagged releases, and programmatically accessible metadata. In this context, any kind of registry that provides another identifier and points to the first is providing redundant information. The only potential benefit is grouping and curation, which I would then argue should still point to the version control and/or a specific release as a source of truth.</p><p>I’m also not convinced that we have established an actual use case of “searching a registry for software.” What happens in labs and communities is that you establish communities around the software, and then there are established workflows or slack communities or GitHub organizations to join around that. Most labs already have chosen languages, and even software pipelines that new members extend or work on. I would even go as far to say that for some (myself included) I don’t find research software, but it finds me. It appears as a link in some social media or chat channel, and I click the link and then there are about 15 seconds during which I make a determination if the software can help me to solve a problem that I have, or if it looks easy, professional, and/or fun and I simply want to try it out. If the answer is “yes” then I add it to a list in a Google Document with other things to try out when I have time. If not, I close the lab and life moves on. But I want to point out that nowhere in this workflow do I explicitly go looking for software. The software often finds me, and then I keep a mental cache of “tools that I’ve seen” and go back to it when the use case arises.</p><p>So being able to answer this question about wanting research software registries is especially challenging because I’m not sure I’ve ever wanted one.Unless there is a specific kind of context around a registry (e.g., search for a specific name in a package manager to use, or look for an already assembled workflow) I haven’t been able to convince myself (yet) that I would find a use for one. I could be wrong about this, however, because as we know, people (myself included) are fairly bad at predicting the future, and perhaps there could be some future where “checking a research software registry” is a part of a daily workflow. I am skeptical because I think that a context is needed. Even if some central source of software ability truth was established, would it not be the case that a graduate student or researcher needs to go there with a use case or context in mind? I can’t imagine just mindlessly browsing for the sake of browsing. It’s akin to search engines - we are usually looking for something very specific. We don’t search without a purpose. The question here then is, what is the purpose?</p><h2 id=\"research-software-registries-with-a-purpose\">Research Software Registries with a Purpose</h2><p>A very good example of purpose comes down to workflows. This is the “I need to perform this specific function and I want to use what many others have done before me and not re-invent the wheel.” The minimum example of a workflow registry would be a search interface that indexes pipelines that are perhaps stored in version control. And extended version of that includes being able to provide structured inputs, outputs, and arguments, so the registry can programmatically provide this information to tools. You can then also quickly see how changing this to be general inputs/outputs of software (and functions within) and entrypoints of containers can quickly become a more generalized registry for software that could be used by any workflow manager that knows how to consume its information. However, there is a fine line here, because when we talk about I/O we are goingsquarely into workflow management territoty, and again in my opinion, we have to be careful about that scope. The closest thing that comes to mind for providing workflows as a service is something like <a href=\"https://openneuro.org/\" target=\"_blank\">openneuro</a> that has a beautiful idea of “Get your data into this standard format and we will serve it and provide other easy ways to analyze it.” This kind of success story tells me that perhaps there is something to say for developing anything related to processing or pipelines in the context of a community. You can’t create the perfect registry for every scientific discipline, or perhaps you can do a mediocre job at trying, but perhaps if you scope to a specific one you can do a very good job. I’ve found the same to be true with software - it’s often better to do one or few things very well than more things kind of mediocre.</p><h3 id=\"a-provider-of-identifiers\">A Provider of Identifiers?</h3><p>I’m skeptical when I hear that people want to apply our traditional model of publication (e.g., having a DOI) to software. The reason isn’t because I don’t value means to support reproducibility (and knowing the exact version of something that was used) but rather that we already have means to tag specific versions of software, and means that fit into a well-established ecosystem: package managers, versions, and releases. To think that a single frozen version of software is “the correct unit to provide” I also disagree with. Software is a living, and changing entity, and when it truly does “freeze” and stops being worked on, unlike a DOI in the academic space, this is sort of its death. The correct entrypoint for a piece of software, in my opinion, is the current version on version control, from where you could decide to pin a particular release or install a particular version from a package manager. But to provide a single frozen DOI that is wrapping some other version / release of the software? It doesn’t make sense to me. It’s adding additional complexity that’s not needed. So my opinion (as I’ve shared before) is that we should be thinking more about preserving specific timepoints in package managers, and not adding on an artificially created layer of “DOI” that seems (in my opinion) more of a reflection of our need to shove things into an academic framework we are comfortable with than anything else.</p><p>So (I hope) that the purpose of a research software registry would not just be to provide DOIs. That doesn’t help me get my work done at the end of the day. All that said, I don’t think there can be a singular answer for purpose. I think the purpose ultimately comes down to the institution (or community) and the specific goals of the registry. For this reason there is no one answer for what a registry should look like or provide, and it is (or will be) challenging to define attributes that “any registry should have.”</p><h3 id=\"what-is-my-purpose\">What is my purpose?</h3><p><em>You cut butter</em>!</p><p>Just kidding :_) I’ve been ruminating on this idea for quite some time, and namely because I’m motivated to build a new kind of research software registry, but first I need to convince myself of a meaningful purpose. While I don’t have my convincing answer yet (but I do have a sense of direction) the way I’ve been thinking about this is to provide a set of questions or use cases that seem plausible. It seems like most people are asking “What kind of information should we have in a registry” but I think this isn’t exactly the question I’m interested in - I want to know:</p><blockquote>  <p>What do you want to do next with the software you find?</p></blockquote><p>This is important because it’s going to drive the context and purpose of the registry. Here are a few examples:</p><ol class=\"custom-counter\">  <li><strong>I want to quickly try this out</strong> → a registry that can deploy a developer environment</li>  <li><strong>I want to find if this is in a package manager</strong> → a reproducible install</li>  <li><strong>I want to use this with a workflow manager</strong> → this is some kind of workflow hub</li>    <li><strong>I want to see inputs / outputs / entrypoints</strong> → support to workflow tools</li>    <li><strong>I want to install this on HPC</strong> → I want a module deployment or similar</li>    <li><strong>I want to cite this</strong> → use case akin to CITATION.cff</li>    <li><strong>I want to understand dependencies of an ecosystem</strong> → a registry deploying something akin to citelang</li>    <li><strong>I want to see all my options to do X</strong> → a domain or categorical registry</li>    <li><strong>I want to see new and noteworthy libraries</strong> → a registry with advanced filtering and ranking</li>    <li><strong>I want to see change over time</strong> → a registry with a layer of analysis tools</li>  </ol><p>Indeed many of the above contexts require additional information. For example, if we want to be able to ask what software is specifically used to perform X, we need a set of functions that are common to a domain, and then to annotate specific software (or even functions) that do it. If we want to then ask “Which of these is the best?” we need to then generate benchmarks to measure this functionality. E.g., how long does it take to run? What are the inputs and outputs and are they correct? What are resource needs? It would be an incredibly cool thing to be able to ask these questions, but an enormous amount of work for any particular scientific domain. As an example of thinking about functional needs, we might look to brain imaging, which is arguably a subfield of neuroinformatics. We might define custom processing functions like thresholding, registration, normalization, or creating regions of interest, tag specific functions that can do each, and then collect and share metrics about the degree to which easy is successful to do each. Arguably, if I wanted to do this I would create wrappers to workflow managers (akin to Snakemake Wrappers) that not only measure metrics, but make it easy for people to quickly use it in their work.</p><h2 id=\"it-needs-to-be-easy\">It needs to be easy</h2><p>Whether I’m thinking about being a user of a research software registry or a developer, it just needs to be easy. Here are some ideas around that.</p><h3 id=\"re-inventing-the-wheel\">Re-inventing the wheel?</h3><p>I come with the experience of deploying a custom container registry (Singularity Hub) years ago, and then being involved in standards committees (the Open Container Initiative) that develop generalized specifications that now drive the most common software (container) registries. I’ve also developed registry proxies that do interesting things, along with a Python OCI registry, and I’m the main developer for oras-py (ORAS == OCI Registry as Storage). So believe me when I say that in terms of storing blobs and metadata about them, I don’t think we should re-invent the wheel. Any new registry I create is going to start with these standards. You might disagree, and that’s OK. But I think people have thought long and hard about these things, and we are stronger for working together on them over always making our own new thing.</p><p>As a supplement to that, I want to point out one of the biggest challenges in our community. The majority of research software, I would argue, doesn’t get used beyond the lab it’s created for. Said lab might submit or include it in a paper, and then they get their publication and move on. This is reflective of many things, and I’ll review them here. The first is our funding model - we maybe can fund working on a piece of software only up until the funding dries out, and then it becomes an abandoned repository, if it’s made publicly available. The second is our incentive model - the academic community is focused on writing papers, so once you get there, you don’t have reason to consider the long term impact of the software. The third is communication. It is actually much easier to throw together your own library than to have to search and then try contributing to someone else’s.I say this because I don’t think the way that things are are necessarily the fault of anyone - we are all agents responding to incentives and resources available.</p><p>But then on the flip side - these observations beg to ask what leads to software that is successful, on a community level? I think a few things can happen. Either someone puts time and energy into establishing community, period, meaning bringing together people that are working on common goals and explicitly asking “How can we do this together,” or what I’ve seen with more commercial open source - having enough money or power that you can create strong branding and community just by way of having the funds for it.  I’ve talked about this a <a href=\"https://vsoch.github.io/2019/transparency/\" target=\"_blank\">few times before</a> and it’s not necessarily bad, but it’s unfair at best. Software that maybe would not be successful by its own merit rises to the top, and really great software that doesn’t have those resources does not. That said, I’ve also seen sort of mediocre software get much better and earn its reputation, so I can’t say it’s a completely wrong dynamic.</p><h3 id=\"is-the-answer-mooooar-metadata\">Is the answer Mooooar Metadata?</h3><p>As we design the “perfect set of information” we want provided for any piece of software, we need to put people first.We have to ask ourselves what are people willing to do, and generally people aren’t wanting to spend inordinate amounts of extra time defining metadata or inputs/outputs for their custom scripts. This was a point also brought up by <a href=\"https://twitter.com/orchid00\" target=\"_blank\">Paula</a> in the SciCodes meeting and I am 100% behind it. If we require extensive metadata about software, it needs to be done in an automated fashion. In practice when I think of archives for software, I’m just not that motivated to provide more than the absolute minimum to click the “submit” button.</p><h2 id=\"do-people-know-what-they-want\">Do people know what they want?</h2><p>One of the hardest things about this kind of problem is that people don’t often know what they want. And actually - I’d extend that to software in general. Think of common tools like git (version control) or containers.Could most people have told you in advance about the designs for these tools? I suspect likely not.This is often the game that software developers play - we imagine new ways of doing things that scratch an itchor have a problem that we have, and then hand over our duct taped laden prototype to others and we’re  likehey, is this useful to you? And often the response in radio silence, but then sometimes it’s a resounding, “WoW, yes!”So I’m going to throw out this idea that people generally don’t know what they want until they see it, touch it and try it.This is also why I want to inspire you to take some time to think about your specific needs and motivation for wanting(on a high level) to browse and interact with research software. What are the compelling reasons for this registry,for you?</p><p>This is actually really fun to think about, because what even is a research software registry? Is it a place to find software to plug into workflows? Does it provide ABI or more general function signatures to help you plug into workflows? Does it provide a citation? A container? An interactive environment? Dependency graph? Something else? This is inded why this problem is so hard - there are so many ways to thinkabout this basic concept. And that’s kind of what makes it fun too? But also what makes it hard. Personally speaking sinceI’m more interested in building things I find myself ruminating about details for a specific use case. And since I’m a developer and craving better support for developer environments, this tends to be where my brain goes. And have you noticed I haven’t givena direct answer for what is a research software registry? It’s 1. because I don’t know, and 2. because we are trying to define a registry for a kind of output that we don’t even have an agreed upon definition for yet! So perhaps the definition will happen on the level of the deployment or institution? Anyway, I hope you take the opportunity to discuss with your peers, pets, and even yourself, to try and answer this question.</p><h2 id=\"summary\">Summary</h2><p>To summarize, I’m spending a lot of time thinking about this, and less in an “I’m an academic that wants DOIs and metadata” and more in a “I am a software engineer that wants to build something that I actually find useful.” Might I scratch itches along the way? Sure. And I do have some early ideas that I plan to hack on before sharing publicly. In the meantime, I do hope you are interested in some of these ideas and take time to write or introspect yourself.</p><p>And on a higher level, I really like this format of writing and speaking, where the speaking isn’t formal enough to be a talk that you put together and practice for weeks (I put this all together in an afternoon) but it still is a media format that literally gives a voice.</p>",
            "url": "https://hpc.social/personal-blog/2022/research-software-registries/",
            
            
            
            
            
            "date_published": "2022-06-19T13:15:00-06:00",
            "date_modified": "2022-06-19T13:15:00-06:00",
            
                "author": "Vanessasaurus"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/life-and-leaving-nersc/",
            "title": "Life and leaving NERSC",
            "summary": null,
            "content_text": "When word started to spread that I was leaving my job at NERSC for Microsoft, a lot of people either directly or indirectly attributed my decision to being one motivated by money.  Rationalizing my decision to leave is certainly a lot easier with this \"Glenn was lured away with bags of cash\" narrative, but that wasn't really a factor when I chose to move on.  Rather, my decision is a reflection of where I see the world of HPC going in the coming decade and where I personally wanted to position myself.  For my own therapeutic reasons (and perhaps the benefit of anyone interested in what it's like to work within, and subsequently leave, the DOE HPC complex), I'll try to write it all out here.Working at NERSCFirst things first: NERSC has been a wonderful place to work.&lt;div style=\"text-align: center;\"&gt;A typical view from outside NERSC’s facility in Berkeley after work during the winter months.  Yes, it really does look like this.&lt;/div&gt;&lt;p&gt;When I started in mid-2015, I came in with about three years of prior work experience (two at SDSC doing user support and one at a biotech startup) and knew a little bit about a lot of things in HPC.  But I didn’t really know the basics of I/O or storage–I couldn’t tell you what “POSIX I/O” really meant or how GPFS worked.  The fact that I got to help author NERSC’s ten-year strategy around storage in just two years, was invited to present my view on how to bridge the gap between HPC and enterprise storage at Samsung’s North American headquarters a year later, and was trusted to oversee the design and execution of the world’s first 35 petabyte all-flash Lustre file system through my first four years is a testament to how much opportunity is available to learn and grow at NERSC.&lt;/p&gt;There are a couple of reasons for this.Stable fundingPerhaps foremost, NERSC (and DOE's Leadership Computing Facilities, ALCF and OLCF) enjoy healthy budgets and financial stability since worldwide leadership in scientific advancement is generally a national priority by both major political parties in the US.  This means that, regardless of who is president and which party holds majorities in Congress, the DOE HPC facilities can pay their employees and deploy new supercomputers.  This solid funding makes it much easier to invest in staff development and long-term planning; I was able to become a resident I/O expert at NERSC because I was never forced to chase after the funding du jour to make ends meet.  Congress trusts NERSC to allocate its funding responsibly, and NERSC prioritized letting me learn as much as I could without distraction.Instant credibility and accessSecond, having a NERSC affiliation gives you instant credibility and access in many cases.  It's not necessarily fair, but it's definitely true.  Within my first year at NERSC, I was invited to give a presentation about I/O performance monitoring in Paris because the organizer wanted a lineup of speakers from all the big players in HPC.  I had never been to Europe at that point in my life, but being the I/O guy from NERSC (and being able to present well!) was enough to get me there.  And it was during that trip to Paris that I got to meet--and literally have conversation over dinner with--more industry bigshots that I can remember.  And that trip to Paris was not an outlier; pandemic aside, NERSC let me go to Europe at least once or twice every year I've worked there.&lt;div style=\"text-align: center;\"&gt;The first photo I ever took of Notre Dame on the first day I’d ever set foot in Europe.  NERSC sent me there less than a year after I started.&lt;/div&gt;&lt;p&gt;Of course, this is not to say that every employee at a DOE HPC facility is wining and dining in Paris every summer.  Many of these opportunities are earned by showing the value of the work you’re doing, just like at any job.  But owing to healthy budgets, travel expenses are rarely the limiting factor in chasing after these opportunities.  In addition, going out into the world and talking about what you do is part of the job at a DOE facility; being a leader in the field of HPC is part of the mission of NERSC, ALCF, and OLCF, so doing high-risk, first-of-a-kind work and telling the world about it is uniquely valued within DOE in a way that it is not in industry.&lt;/p&gt;Smart peopleA product of these two factors (stable budget and instant credibility) results in coworkers and colleagues who are generally very experienced and capable.  There's an interesting mix of laissez-faire management and rigorous process-driven management as a result.Staff are generally given the freedom to choose their own destiny and focus on work that they enjoy much like in any academic environment; it's not hard to pick up passion projects or even move between groups if things get stale on a day-to-day basis.  Since everyone is working on their own slices of HPC, there's also easy access to world experts in different areas of technology if you need one.  For example, I recall once reviewing a storage system that appeared to rely on multiplexing two 12G SAS links over a single 24G SAS.  After one email and a few hours, a coworker confirmed, complete with a citation to the SCSI standards, that this was totally possible.  Even if someone in-house didn't know the answer, I had direct access to an engineering manager at a leading storage vendor who owed me a favor and definitely would've known the answer.  It's really, really hard to find as many smart people in arm's reach in most other HPC centers. At the same time, there is rigorous federal oversight on major projects and procurements to ensure that taxpayer dollars are responsibly spent.  This is a double-edged sword because all of the reporting and reviews that go into massive capital projects make forward progress very slow at times.  All DOE HPC facilities review and re-review everything about these giant supercomputers before making a decision, so by the time the public sees a press release about a new supercomputer, lab staff have spent literal years going over every detail and risk.  It sometimes may not seem that way (how many problems has Aurora had?), but rest assured that every schedule slip or technology change the public hears was preceded by countless hours of meetings about risk and cost minimization.  On the flip-side though, you have the opportunity to learn every gory detail about the system directly from the people who designed it.PayIn true millennial fashion, I think it's important to have an open discussion about the pay.  DOE labs pay more than any other HPC facility in the world as far as I am aware, and even in the San Francisco Bay Area, salary at NERSC is comparable to the base salaries offered by all the big tech companies.  You can get an idea of what entry-level salaries (think: first job after postdoc or a few years out of undergrad) by searching H1B Visa postings, and anecdotally, I'd wager that a typical HPC job at NERSC pays about 2x that of the same job at a typical US university and 3x-4x that of the same job at a British or European university.  All the labs pay about the same to boot, so an HPC job at somewhere like Oak Ridge can afford you a relatively luxurious lifestyle.Don't get me wrong though; affording to buy a Bay Area house on a single NERSC salary alone would be tough in the same way that buying a Bay Area house on any single salary would be.  And while NERSC's compensation is comparable to the base salary of the big tech companies, that base is about all you can get since DOE labs cannot offer equity or substantial bonuses.  This is less of a gap if you're just starting out, but anyone who's looked at compensation structures in tech knows that stock-based compensation, not base salary, dominates total compensation as you move up.So, if money wasn't an issue for me and NERSC is such a great place to work, why would I ever leave?The road ahead for HPCOn one hand, HPC's future has never been brighter thanks to how much life (and money!) the AI industry is bringing to the development of HPC technologies.  We have new all-flash file systems, gigantic GPUs, awesome CPU memory technologies, and mixed-precision techniques in the HPC space that were all directly driven by developments primarily intended for AI workloads.  On the other hand, leadership HPC appears to be engaging in unsustainable brinkmanship while midrange HPC is having its value completely undercut by cloud vendors.  I've not been shy about my overall anxiety about where HPC is going because of this, but I'll elaborate now that the exascale race has been won.The future of leadership HPCWithout some monumental breakthrough in transistor technology, there is only one path forward in continuing to build faster and faster supercomputers in the next decade: pour more and more energy (and dissipate more and more heat) into larger and larger (and more and more) GPUs.The goal post for exascale power keeps moving because that's been the easiest way to hit the mythical exaflop milestone; while the original goal was 20 MW, Frontier is coming in at 29 MW and Aurora at \"under 60 MW.\"  Not only is this just a lot of power to feed into a single room, but the cost and effort of actually building this infrastructure is newsworthy in and of itself these days.  At the current trajectory, the cost of building a new data center and extensive power and cooling infrastructure for every new leadership supercomputer is going to become prohibitive very soon.HPC data centers situated in places where the cost of electricity and real estate (stacked atop the risk of earthquake or wildfire) further skew the economics of just adding more power are going to run up against this first.  It used to be easy to dismiss these practicality concerns by arguing that colocating scientists with supercomputers created immeasurable synergy and exchange of ideas, but the fact that science never stopped during the work-from-home days of the pandemic have taken a lot of air out of that argument.My guess is that all the 50-60 MW data centers being built for the exascale supercomputers will be the last of their kind, and that there will be no public appetite to keep doubling down.Given this, DOE's leadership computing facilities are facing an existential threat: how do you define leadership computing after exascale if you can't just add another 50% more power into your facility?  How do you justify spending another $600 million for a supercomputer that uses the same power but only delivers 15% more performance?  You can pour similarly huge amounts of money into application modernization to accelerate science, but at the end of the day, you'd still be buying a lot of hardware that's not a lot faster.The future of places like NERSCNERSC is probably a little better off since its lack of an exascale machine today gives it at least one more turn of the crank before it hits a hard power limit in its data center.  That gives it the ability to deploy at least one more system after Perlmutter that is significantly (at least 2x) more capable but draws significantly more power.  However, compared to Frontier and Aurora, such a system may still look rather silly when it lands in the same way that Perlmutter looks a bit silly compared Summit, which was funded by the same agency but deployed years earlier.And therein lies the dilemma of centers like NERSC--how do you position yourself now so that by the time you deploy an HPC system that is close to maxing out on power, it is sufficiently different from a pure-FLOPS leadership system that it can solve problems that the leadership systems cannot?The easy go-to solution is to craft a story around \"data-centric\" supercomputing.  We did this when I was at the San Diego Supercomputer Center when we were budget-limited and had to differentiate our $12 million Comet supercomputer from TACC's $30 million Stampede.  You invest more in the file system than you would for a pure-FLOPS play, you provide low-cost but high-value onramps like Jupyter and science gateways to enable new science communities that have modest computing needs, and you fiddle with policies like allocations and queue priority to better suit interactive and urgent computing workloads.  From a productivity standpoint, this is can be a great story since users will always respond well to lower queue wait times and less frustrations with the file system.  From a system architect's standpoint, though, this is really boring.  The innovation happens in policies and software, not clever hardware or design, so there's very little that's new for a system designer to think about in this case.A more innovative approach is to start thinking about how to build a system that does more than just run batch jobs.  Perhaps it gives you a private, fast file system where you can store all your data in a way indistinguishable from your personal laptop.  Perhaps it gives you a convenient place to run a Jupyter notebook that has immediate access to a powerful GPU.  Or perhaps it gives you all the tools to set up an automated process where all you have to do is upload a file to trigger an automatic data analysis and reduction pipeline that returns its output to a shiny HTTP interface.  Such a system may not be able to crank out an exaflop using HPL, but does that matter if it's the only system in the country that supports such automation?There are interesting system architecture questions in the latter case, so as a system designer, I much prefer it over the \"data-centric\" angle to non-exaflop supercomputing strategies.  But there remains a problem.The problem: cloudSuch a \"more than just batch jobs\" supercomputer actually already exists.  It's called the cloud, and it's far, far ahead of where state-of-the-art large-scale HPC is today--it pioneered the idea of providing an integrated platform where you can twist the infrastructure and its services to exactly fit what you want to get done.  Triggering data analysis based on the arrival of new data has been around for the better part of a decade in the form of serverless computing frameworks like Azure Functions.  If you need to run a Jupyter notebook on a server that has a beefy GPU on it, just pop a few quarters into your favorite cloud provider.  And if you don't even want to worry about what infrastructure you need to make your Jupyter-based machine learning workload go fast, the cloud providers all have integrated machine learning development environments that hide all of the underlying infrastructure.And therein lies the problem: the definition of \"innovation\" as non-exaflop HPC runs up against this power wall might actually mean \"catching up to the cloud.\"This is not to say that NERSC-like HPC centers are entirely behind the cloud; all the DOE HPC facilities have bigger, faster, and more convenient parallel file systems that are generally always on and where data is always somewhere \"fast.\"  They also provide familiar, managed software environments and more egalitarian support to small- to mid-scale science projects.  DOE HPC also takes the most risk in deploying unproven technologies to shake them out before they become available to the wide market.However, those gaps are beginning to close.  You can stick a full Cray EX system, identical to what you might find at NERSC or OLCF, inside Azure nowadays and avoid that whole burdensome mess of building out a 50 MW data center.  You can also integrate such a system with all the rich infrastructure features the cloud has to offer like triggered functions.  And when it comes to being first to market for risky HPC hardware, the cloud has already caught up in many ways--Microsoft deployed AMD Milan-X CPUs in their data centers before any HPC shop did, and more recently, Microsoft invested in AMD MI-200 GPUs before Frontier had a chance to shake them out.Given this steep trajectory, I see only two scenarios for large-scale, non-exaflop HPC facilities in the 10+ year horizon:They develop, adopt, steal, or squish cloud technologies into their supercomputers to make them functionally equivalent to cloud HPC deployments.  They may be a little friendlier to scientific users since cloud functionality wasn't designed for scientific computing alone, but they also may not be as stable, mature, or feature-rich as their cloud cousins.They find better overall economics in eventually moving to massive, long-term, billion-dollar deals where flagship HPC systems and their \"more than just batch jobs\" features are colocated inside cloud datacenters sited at economically advantageous (that is, cheap power, cooling, and labor) locations in the country.There's also grey area in between where national HPC facilities consolidate their physical infrastructure in cheap areas to manage costs but still self-manage their infrastructure rather than fully outsource to a commercial cloud.  CSCS has hinted at this model as their future plan since they cannot build 100 MW datacenters in Switzerland, and this is proof that leading HPC facilities around the world see the writing on the wall and need to maneuver now to ensure they remain relevant beyond the next decade.  Unfortunately, the politics of consolidating the physical infrastructure across the DOE HPC sites would likely be mired in Congressional politics and take at least a decade to work out.  Since serious work towards this hasn't started yet, I don't envision such a grey-area solution emerging before all the DOE facilities hit their power limit.Hopefully I've painted a picture of how I perceive the road ahead for large-scale HPC facilities and you can guess which one I think will win out.Final thoughtsI have every confidence that there will still be DOE HPC facilities in ten years and that they will still be staffed by some of the brightest minds in HPC.  And even if a cloud-based HPC facility ultimately consumes centers like NERSC, I don't think many people would be out of work.  The vast majority of what DOE's HPC people do is think carefully about technology trends, maintain a deep understanding of user requirements, provide excellent support to its thousands of users, and keep complex supercomputers running well.  Those jobs don't go away if the supercomputer is in the cloud; it's just the physical location, the hands doing physical hardware swaps, and the breadth of vendor interactions that may change.For me as a system architect though, it's become too hard for me to catch up to all the new technologies and techniques HPC needs for the future while also building up other staff to be masters of today's I/O challenges.  I found myself at a fork in the road.  One path would mean catching up on a technical level and then getting in front of where the future of HPC lies before it gets there.  The other path would mean trying to steer the entire DOE HPC ship in the right direction, as long as that may take, and have faith that the people I bring along can race far enough ahead to tell me if we're still going where we need to go.  Perhaps a bit selfishly, I chose the former.  I'm just not ready to give up on racing ahead myself yet, and the only way I could hope to catch up was to make it a full-time job.I don't claim to know the future, and a lot of what I've laid out is all speculative at best.  NERSC, ALCF, or OLCF very well may build another round of data centers to keep the DOE HPC party going for another decade.  However, there's no denying that the stakes keep getting higher with every passing year.That all said, DOE has pulled off stranger things in the past, and it still has a bunch of talented people to make the best of whatever the future holds.",
            "content_html": "<p>When word started to spread that I was leaving my job at NERSC for Microsoft, a lot of people either directly or indirectly attributed my decision to being one motivated by money.  Rationalizing my decision to leave is certainly a lot easier with this \"Glenn was lured away with bags of cash\" narrative, but that wasn't really a factor when I chose to move on.  Rather, my decision is a reflection of where I see the world of HPC going in the coming decade and where I personally wanted to position myself.  For my own therapeutic reasons (and perhaps the benefit of anyone interested in what it's like to work within, and subsequently leave, the DOE HPC complex), I'll try to write it all out here.<span></span></p><p></p><h2 style=\"text-align: left;\">Working at NERSC</h2><p>First things first: NERSC has been a wonderful place to work.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p><b>&lt;div style=\"text-align: center;\"&gt;<b><span style=\"font-size: x-small;\">A typical view from outside NERSC’s facility in Berkeley after work during the winter months.  Yes, it really does look like this.</span></b>&lt;/div&gt;</b>&lt;p&gt;When I started in mid-2015, I came in with about three years of prior work experience (two at SDSC doing user support and one at a biotech startup) and knew a little bit about a lot of things in HPC.  But I didn’t really know the basics of I/O or storage–I couldn’t tell you what “POSIX I/O” really meant or how GPFS worked.  The fact that I got to help author <a href=\"https://www.nersc.gov/news-publications/nersc-news/nersc-center-news/2017/new-storage-2020-report-outlines-future-hpc-storage-vision/\">NERSC’s ten-year strategy around storage</a> in just two years, was invited to present <a href=\"https://insidehpc.com/2019/08/designing-future-flash-storage-systems-for-hpc-and-beyond/\">my view on how to bridge the gap between HPC and enterprise storage</a> at Samsung’s North American headquarters a year later, and was trusted to oversee <a href=\"https://www.nextplatform.com/2021/06/07/a-35-petabyte-all-flash-balancing-act/\">the design and execution of the world’s first 35 petabyte all-flash Lustre file system</a> through my first four years is a testament to how much opportunity is available to learn and grow at NERSC.&lt;/p&gt;</p><p>There are a couple of reasons for this.</p><h3 style=\"text-align: left;\">Stable funding</h3><p>Perhaps foremost, NERSC (and DOE's Leadership Computing Facilities, ALCF and OLCF) enjoy healthy budgets and financial stability since worldwide leadership in scientific advancement is generally a national priority by both major political parties in the US.  This means that, regardless of who is president and which party holds majorities in Congress, the DOE HPC facilities can pay their employees and deploy new supercomputers.  This solid funding makes it much easier to invest in staff development and long-term planning; I was able to become a resident I/O expert at NERSC because I was never forced to chase after the funding du jour to make ends meet.  Congress trusts NERSC to allocate its funding responsibly, and NERSC prioritized letting me learn as much as I could without distraction.</p><h3 style=\"text-align: left;\">Instant credibility and access</h3><p>Second, <a href=\"https://twitter.com/hpcprogrammer/status/1061278775353196544?s=20&amp;t=_YGQXWvykuCElqltJ-x09Q\">having a NERSC affiliation gives you instant credibility and access</a> in many cases.  It's not necessarily fair, but it's definitely true.  Within my first year at NERSC, I was invited to give <a href=\"https://archive.siam.org/meetings/pp16/pp16_program.pdf\">a presentation about I/O performance monitoring in Paris</a> because the organizer wanted a lineup of speakers from all the big players in HPC.  I had never been to Europe at that point in my life, but being the I/O guy from NERSC (and being able to present well!) was enough to get me there.  And it was during that trip to Paris that I got to meet--and literally have conversation over dinner with--<a href=\"https://www.linkedin.com/in/larry-kaplan-b101936\">more</a> <a href=\"https://people.llnl.gov/tgamblin\">industry</a> <a href=\"https://en.wikipedia.org/wiki/David_E._Keyes\">bigshots</a> that I can remember.  And that trip to Paris was not an outlier; pandemic aside, NERSC let me go to Europe at least once or twice every year I've worked there.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p><b>&lt;div style=\"text-align: center;\"&gt;<b><span style=\"font-size: x-small;\">The first photo I ever took of Notre Dame on the first day I’d ever set foot in Europe.  NERSC sent me there less than a year after I started.</span></b>&lt;/div&gt;</b>&lt;p&gt;Of course, this is not to say that every employee at a DOE HPC facility is wining and dining in Paris every summer.  Many of these opportunities are earned by showing the value of the work you’re doing, just like at any job.  But owing to healthy budgets, travel expenses are rarely the limiting factor in chasing after these opportunities.  In addition, going out into the world and talking about what you do is part of the job at a DOE facility; being a leader in the field of HPC is part of the mission of NERSC, ALCF, and OLCF, so doing high-risk, first-of-a-kind work <i>and telling the world about it</i> is uniquely valued within DOE in a way that it is not in industry.&lt;/p&gt;</p><h3 style=\"text-align: left;\">Smart people</h3><p>A product of these two factors (stable budget and instant credibility) results in coworkers and colleagues who are generally very experienced and capable.  There's an interesting mix of laissez-faire management and rigorous process-driven management as a result.</p><p>Staff are generally given the freedom to choose their own destiny and focus on work that they enjoy much like in any academic environment; it's not hard to pick up passion projects or even move between groups if things get stale on a day-to-day basis.  Since everyone is working on their own slices of HPC, there's also easy access to world experts in different areas of technology if you need one.  For example, I recall once reviewing a storage system that appeared to rely on multiplexing two 12G SAS links over a single 24G SAS.  After one email and a few hours, a coworker confirmed, complete with a citation to the SCSI standards, that this was totally possible.  Even if someone in-house didn't know the answer, I had direct access to an engineering manager at a leading storage vendor who owed me a favor and definitely would've known the answer.  It's really, really hard to find as many smart people in arm's reach in most other HPC centers. </p><p>At the same time, there is rigorous federal oversight on major projects and procurements to ensure that taxpayer dollars are responsibly spent.  This is a double-edged sword because all of the reporting and reviews that go into <a href=\"https://www.energy.gov/articles/doe-build-next-generation-supercomputer-lawrence-berkeley-national-laboratory\">massive</a> <a href=\"https://www.ornl.gov/news/us-department-energy-and-cray-deliver-record-setting-frontier-supercomputer-ornl\">capital</a> <a href=\"https://www.energy.gov/articles/us-department-energy-and-intel-build-first-exascale-supercomputer\">projects</a> make forward progress very slow at times.  All DOE HPC facilities review and re-review everything about these giant supercomputers before making a decision, so by the time the public sees a press release about a new supercomputer, lab staff have spent literal years going over every detail and risk.  It sometimes may not seem that way (how many problems has Aurora had?), but rest assured that every schedule slip or technology change the public hears was preceded by countless hours of meetings about risk and cost minimization.  On the flip-side though, you have the opportunity to learn every gory detail about the system directly from the people who designed it.</p><h3 style=\"text-align: left;\">Pay</h3><p>In <a href=\"https://www.bankrate.com/banking/federal-reserve/younger-workers-sharing-salaries/\">true millennial fashion</a>, I think it's important to have an open discussion about the pay.  DOE labs pay more than any other HPC facility in the world as far as I am aware, and even in the San Francisco Bay Area, salary at NERSC is comparable to the base salaries offered by all the big tech companies.  You can get an idea of what entry-level salaries (think: first job after postdoc or a few years out of undergrad) by searching <a href=\"https://h1bdata.info/\">H1B Visa postings</a>, and anecdotally, I'd wager that a typical HPC job at NERSC pays about 2x that of the same job at a typical US university and 3x-4x that of the same job at a British or European university.  All the labs pay about the same to boot, so an HPC job at somewhere like Oak Ridge can afford you a relatively luxurious lifestyle.</p><p>Don't get me wrong though; affording to buy a Bay Area house on a single NERSC salary alone would be tough in the same way that buying a Bay Area house on any single salary would be.  And while NERSC's compensation is comparable to the <i>base</i> salary of the big tech companies, that base is about all you can get since DOE labs cannot offer equity or substantial bonuses.  This is less of a gap if you're just starting out, but anyone who's <a href=\"https://www.levels.fyi/\">looked at compensation structures in tech</a> knows that stock-based compensation, not base salary, dominates total compensation as you move up.</p><p>So, if money wasn't an issue for me and NERSC is such a great place to work, why would I ever leave?</p><h2 style=\"text-align: left;\">The road ahead for HPC</h2><p>On one hand, HPC's future has never been brighter thanks to how much life (and money!) the AI industry is bringing to the development of HPC technologies.  We have new <a href=\"https://vastdata.com/\">all-flash</a> <a href=\"https://www.weka.io/\">file systems</a>, <a href=\"https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/\">gigantic GPUs</a>, awesome <a href=\"https://www.tomshardware.com/news/intels-sapphire-rapids-to-have-64-gigabytes-of-hbm2e-memory\">CPU memory technologies</a>, and <a href=\"https://arxiv.org/abs/2205.12182\">mixed-precision techniques</a> in the HPC space that were all directly driven by developments primarily intended for AI workloads.  On the other hand, leadership HPC appears to be engaging in unsustainable brinkmanship while midrange HPC is having its value completely undercut by cloud vendors.  I've <a href=\"https://glennklockwood.blogspot.com/2020/05/exascales-long-shadow-and-hpc-being.html\">not been shy about my overall anxiety about where HPC is going</a> because of this, but I'll elaborate now that the exascale race has been won.</p><h3 style=\"text-align: left;\">The future of leadership HPC</h3><p>Without some monumental breakthrough in transistor technology, there is only one path forward in continuing to build faster and faster supercomputers in the next decade: pour more and more energy (and dissipate more and more heat) into larger and larger (and more and more) GPUs.</p><p>The goal post for exascale power keeps moving because that's been the easiest way to hit the mythical exaflop milestone; while the original goal was 20 MW, <a href=\"https://www.nextplatform.com/2021/10/04/first-look-at-oak-ridges-frontier-exascaler-contrasted-to-argonnes-aurora/\">Frontier is coming in at 29 MW</a> and <a href=\"https://www.tomshardware.com/news/nvidia-amd-polaris-supercomputer-department-of-energy\">Aurora at \"under 60 MW.\"</a>  Not only is this just a lot of power to feed into a single room, but the <a href=\"https://www.olcf.ornl.gov/2020/09/23/powering-frontier/\">cost and effort</a> of actually <a href=\"https://www.llnl.gov/news/powering-llnl-prepares-exascale-massive-energy-and-water-upgrade\">building this infrastructure</a> is <a href=\"https://www.lanl.gov/asc/fous/sixty-megawatts-power-available-2025.php\">newsworthy</a> in and of itself these days.  At the current trajectory, the cost of building a new data center and extensive power and cooling infrastructure for every new leadership supercomputer is going to become prohibitive very soon.</p><p>HPC data centers situated in places where the cost of electricity and real estate (stacked atop the risk of earthquake or wildfire) further skew the economics of just adding more power are going to run up against this first.  It used to be easy to dismiss these practicality concerns by arguing that colocating scientists with supercomputers created immeasurable synergy and exchange of ideas, but the fact that science never stopped during the work-from-home days of the pandemic have taken a lot of air out of that argument.</p><p>My guess is that all the 50-60 MW data centers being built for the exascale supercomputers will be the last of their kind, and that there will be no public appetite to keep doubling down.</p><p>Given this, DOE's leadership computing facilities are facing an existential threat: how do you define leadership computing after exascale if you can't just add another 50% more power into your facility?  How do you justify spending another $600 million for a supercomputer that uses the same power but only delivers 15% more performance?  You can pour similarly huge amounts of money into application modernization to accelerate science, but at the end of the day, you'd still be buying a lot of hardware that's not a lot faster.</p><h3 style=\"text-align: left;\">The future of places like NERSC</h3><p>NERSC is probably a little better off since its lack of an exascale machine today gives it at least one more turn of the crank before it hits a hard power limit in its data center.  That gives it the ability to deploy at least one more system after Perlmutter that is significantly (at least 2x) more capable but draws significantly more power.  However, compared to Frontier and Aurora, such a system may still look rather silly when it lands in the same way that Perlmutter looks a bit silly compared Summit, which was funded by the same agency but deployed years earlier.</p><p>And therein lies the dilemma of centers like NERSC--how do you position yourself now so that by the time you deploy an HPC system that is close to maxing out on power, it is sufficiently different from a pure-FLOPS leadership system that it can solve problems that the leadership systems cannot?</p><p>The easy go-to solution is to craft a story around \"data-centric\" supercomputing.  We did this when I was at the San Diego Supercomputer Center when we were budget-limited and had to differentiate our $12 million Comet supercomputer from TACC's $30 million Stampede.  You invest more in the file system than you would for a pure-FLOPS play, you provide low-cost but high-value onramps like Jupyter and science gateways to enable new science communities that have modest computing needs, and you fiddle with policies like allocations and queue priority to better suit interactive and urgent computing workloads.  From a productivity standpoint, this is can be a great story since users will always respond well to lower queue wait times and less frustrations with the file system.  From a system architect's standpoint, though, this is really boring.  The innovation happens in policies and software, not clever hardware or design, so there's very little that's new for a system designer to think about in this case.</p><p>A more innovative approach is to start thinking about how to build a system that does more than just run batch jobs.  Perhaps it gives you a private, fast file system where you can store all your data in a way indistinguishable from your personal laptop.  Perhaps it gives you a convenient place to run a Jupyter notebook that has immediate access to a powerful GPU.  Or perhaps it gives you all the tools to set up an automated process where all you have to do is upload a file to trigger an automatic data analysis and reduction pipeline that returns its output to a shiny HTTP interface.  Such a system may not be able to crank out an exaflop using HPL, but does that matter if it's the only system in the country that supports such automation?</p><p>There <i>are</i> interesting system architecture questions in the latter case, so as a system designer, I much prefer it over the \"data-centric\" angle to non-exaflop supercomputing strategies.  But there remains a problem.</p><h3 style=\"text-align: left;\">The problem: cloud</h3><p>Such a \"more than just batch jobs\" supercomputer actually already exists.  It's called the cloud, and it's far, far ahead of where state-of-the-art large-scale HPC is today--it pioneered the idea of providing an integrated platform where you can twist the infrastructure and its services to exactly fit what you want to get done.  Triggering data analysis based on the arrival of new data has been around for the better part of a decade in the form of serverless computing frameworks like <a href=\"https://docs.microsoft.com/en-us/learn/modules/execute-azure-function-with-triggers/2-determine-best-trigger\">Azure Functions</a>.  If you need to run a Jupyter notebook on a server that has a beefy GPU on it, just pop a few quarters into your favorite cloud provider.  And if you don't even want to worry about what infrastructure you need to make your Jupyter-based machine learning workload go fast, the cloud providers all have <a href=\"https://docs.microsoft.com/en-us/azure/machine-learning/overview-what-is-machine-learning-studio\">integrated machine learning development environments</a> that hide all of the underlying infrastructure.</p><p>And therein lies the problem: the definition of \"innovation\" as non-exaflop HPC runs up against this power wall might actually mean \"catching up to the cloud.\"</p><p>This is not to say that NERSC-like HPC centers are entirely behind the cloud; all the DOE HPC facilities have bigger, faster, and more convenient parallel file systems that are generally always on and where data is always somewhere \"fast.\"  They also provide familiar, managed software environments and more egalitarian support to small- to mid-scale science projects.  DOE HPC also takes the most risk in deploying unproven technologies to shake them out before they become available to the wide market.</p><p>However, those gaps are beginning to close.  You can stick <a href=\"https://azure.microsoft.com/en-us/solutions/high-performance-computing/cray/\">a full Cray EX system, identical to what you might find at NERSC or OLCF, inside Azure</a> nowadays and avoid that whole burdensome mess of building out a 50 MW data center.  You can also integrate such a system with all the rich infrastructure features the cloud has to offer like triggered functions.  And when it comes to being first to market for risky HPC hardware, the cloud has already caught up in many ways--<a href=\"https://azure.microsoft.com/en-us/blog/azure-hbv3-virtual-machines-for-hpc-now-up-to-80-percent-faster-with-amd-milanx-cpus/\">Microsoft deployed AMD Milan-X CPUs in their data centers</a> before any HPC shop did, and more recently, <a href=\"https://www.theregister.com/2022/05/26/amd_azure_microsoft/\">Microsoft invested in AMD MI-200 GPUs</a> before Frontier had a chance to shake them out.</p><p>Given this steep trajectory, I see only two scenarios for large-scale, non-exaflop HPC facilities in the 10+ year horizon:</p><p></p><ol style=\"text-align: left;\"><li>They develop, adopt, steal, or squish cloud technologies into their supercomputers to make them functionally equivalent to cloud HPC deployments.  They may be a little friendlier to scientific users since cloud functionality wasn't designed for scientific computing alone, but they also may not be as stable, mature, or feature-rich as their cloud cousins.</li><li>They find better overall economics in eventually moving to <a href=\"https://www.hpcwire.com/2021/05/13/behind-the-met-offices-procurement-of-a-billion-dollar-microsoft-system/\">massive, long-term, billion-dollar deals</a> where flagship HPC systems and their \"more than just batch jobs\" features are colocated inside cloud datacenters sited at economically advantageous (that is, cheap power, cooling, and labor) locations in the country.</li></ol><p>There's also grey area in between where national HPC facilities consolidate their physical infrastructure in cheap areas to manage costs but still self-manage their infrastructure rather than fully outsource to a commercial cloud.  <a href=\"https://ethz.ch/en/news-and-events/eth-news/news/2021/03/we-dont-just-procure-a-new-computer.html\">CSCS has hinted at this model as their future plan</a> since they cannot build 100 MW datacenters in Switzerland, and this is proof that leading HPC facilities around the world see the writing on the wall and need to maneuver now to ensure they remain relevant beyond the next decade.  Unfortunately, the politics of consolidating the physical infrastructure across the DOE HPC sites would likely be mired in Congressional politics and take at least a decade to work out.  Since serious work towards this hasn't started yet, I don't envision such a grey-area solution emerging before all the DOE facilities hit their power limit.</p><p>Hopefully I've painted a picture of how I perceive the road ahead for large-scale HPC facilities and you can guess which one I think will win out.</p><h2 style=\"text-align: left;\">Final thoughts</h2><p>I have every confidence that there will still be DOE HPC facilities in ten years and that they will still be staffed by some of the brightest minds in HPC.  And even if a cloud-based HPC facility ultimately consumes centers like NERSC, I don't think many people would be out of work.  The vast majority of what DOE's HPC people do is think carefully about technology trends, maintain a deep understanding of user requirements, provide excellent support to its thousands of users, and keep complex supercomputers running well.  Those jobs don't go away if the supercomputer is in the cloud; it's just the physical location, the hands doing physical hardware swaps, and the breadth of vendor interactions that may change.</p><p>For me as a system architect though, it's become too hard for me to catch up to all the new technologies and techniques HPC needs for the future while also building up other staff to be masters of today's I/O challenges.  I found myself at a fork in the road.  One path would mean catching up on a technical level and then getting in front of where the future of HPC lies before it gets there.  The other path would mean trying to steer the entire DOE HPC ship in the right direction, as long as that may take, and have faith that the people I bring along can race far enough ahead to tell me if we're still going where we need to go.  Perhaps a bit selfishly, I chose the former.  I'm just not ready to give up on racing ahead myself yet, and the only way I could hope to catch up was to make it a full-time job.</p><p>I don't claim to know the future, and a lot of what I've laid out is all speculative at best.  NERSC, ALCF, or OLCF very well may build another round of data centers to keep the DOE HPC party going for another decade.  However, there's no denying that the stakes keep getting higher with every passing year.</p><p>That all said, DOE has pulled off stranger things in the past, and it still has a bunch of talented people to make the best of whatever the future holds.</p><p></p>",
            "url": "https://hpc.social/personal-blog/2022/life-and-leaving-nersc/",
            
            
            
            
            
            "date_published": "2022-05-27T06:42:00-06:00",
            "date_modified": "2022-05-27T06:42:00-06:00",
            
                "author": "Glenn K. Lockwood's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/experimenting-with-igor-s-bluestore-wal/",
            "title": "Experimenting with Igor’s Bluestore WAL",
            "summary": null,
            "content_text": "Igor Fedetov is one of the most knowledgable developers working on Ceph.  He’s started working on replacing our use of RocksDB’s write ahead log with a bluestore native implementation.  After tuning we can achieve up to 122K random write IOPS on a single OSD!  That’s nearly a 50% improvment over the current main branch and over twice as fast as Pacific!",
            "content_html": "<p>Igor Fedetov is one of the most knowledgable developers working on Ceph.  He’s started working on replacing our use of RocksDB’s write ahead log with a bluestore native implementation.  After tuning we can <a href=\"https://docs.google.com/spreadsheets/d/1zETd1Nq_CbLNSh3R2II-z8efQizUjDYfHDBIcMwGNdg/edit?usp=sharing\">achieve</a> up to 122K random write IOPS on a single OSD!  That’s nearly a 50% improvment over the current main branch and over twice as fast as Pacific!</p>",
            "url": "https://hpc.social/personal-blog/2022/experimenting-with-igor-s-bluestore-wal/",
            
            
            
            
            
            "date_published": "2022-05-26T01:00:00-06:00",
            "date_modified": "2022-05-26T01:00:00-06:00",
            
                "author": "Mark Nelson's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/interesting-links-i-clicked-this-week/",
            "title": "Interesting links I clicked this week",
            "summary": null,
            "content_text": "I watched several really interesting talks from SRECon22 Americas this week, and in particular I&#8217;d like to highlight:Principled Performance Analytics, Narayan Desai and Brent Bryan from Google. Some interesting thoughts on quantitative analysis of live performance data for monitoring and observability purposes, moving past simple percentile analysis.The &#8216;Success&#8217; in SRE is Silent, Casey Rosenthal from Verica.io. Interesting thoughts here on the visibility of reliability, qualitative analysis of systems, and why regulation and certification might not be the right thing for web systems.Building and Running a Diversity-focused Pre-internship program for SRE, from Andrew Ryan at Facebook Meta. Some good lessons-learned here from an early-career internship-like program, in its first year.Taking the 737 to the Max, Nickolas Means from Sym. A really interesting analysis of the Boeing 737 Max failures from both a technical and cultural perspective, complete with some graph tracing to understand failure modes.I also ran across some other articles that I&#8217;ve been actively recommending and sharing with friends and colleagues, including:Plato&#8217;s Dashboards, Fred Hebert at Honeycomb. This article has some great analysis of how easily-measurable metrics are often poor proxies for the information we&#8217;re actually interested in, and discussing qualitative research methods as a way to gain more insight.The End of Roe Will Bring About A Sea Change In The Encryption Debate, Rianna Pfefferkorn from the Stanford Internet Observatory. You should absolutely go read this article, but to sum up: Law enforcement in states than ban abortion is now absolutely part of the threat model that encrypted messaging defends against. No one claiming to be a progressive should be arguing in favor of &#8220;exceptional access&#8221; or other law enforcement access to encryption.",
            "content_html": "<p>I watched several really interesting talks from <a href=\"https://www.usenix.org/conference/srecon22americas/program\">SRECon22 Americas</a> this week, and in particular I&#8217;d like to highlight:</p><ul><li><a href=\"https://www.usenix.org/conference/srecon22americas/presentation/desai\">Principled Performance Analytics</a>, Narayan Desai and Brent Bryan from Google. Some interesting thoughts on quantitative analysis of live performance data for monitoring and observability purposes, moving past simple percentile analysis.</li><li><a href=\"https://www.usenix.org/conference/srecon22americas/presentation/rosenthal\">The &#8216;Success&#8217; in SRE is Silent</a>, Casey Rosenthal from Verica.io. Interesting thoughts here on the visibility of reliability, qualitative analysis of systems, and why regulation and certification might not be the right thing for web systems.</li><li><a href=\"https://www.usenix.org/conference/srecon22americas/presentation/ryan\">Building and Running a Diversity-focused Pre-internship program for SRE</a>, from Andrew Ryan at <s>Facebook</s> Meta. Some good lessons-learned here from an early-career internship-like program, in its first year.</li><li><a href=\"https://www.usenix.org/conference/srecon22americas/presentation/means\">Taking the 737 to the Max</a>, Nickolas Means from Sym. A really interesting analysis of the Boeing 737 Max failures from both a technical and cultural perspective, complete with some graph tracing to understand failure modes.</li></ul><p>I also ran across some other articles that I&#8217;ve been actively recommending and sharing with friends and colleagues, including:</p><ul><li><a href=\"https://ferd.ca/plato-s-dashboards.html\">Plato&#8217;s Dashboards</a>, Fred Hebert at Honeycomb. This article has some great analysis of how easily-measurable metrics are often poor proxies for the information we&#8217;re actually interested in, and discussing qualitative research methods as a way to gain more insight.</li><li><a href=\"https://cyberlaw.stanford.edu/blog/2022/05/end-roe-will-bring-about-sea-change-encryption-debate\">The End of Roe Will Bring About A Sea Change In The Encryption Debate</a>, Rianna Pfefferkorn from the Stanford Internet Observatory. You should absolutely go read this article, but to sum up: Law enforcement in states than ban abortion is now <em>absolutely</em> part of the threat model that encrypted messaging defends against. No one claiming to be a progressive should be arguing in favor of &#8220;exceptional access&#8221; or other law enforcement access to encryption.</li></ul><p></p>",
            "url": "https://hpc.social/personal-blog/2022/interesting-links-i-clicked-this-week/",
            
            
            
            
            
            "date_published": "2022-05-14T19:35:32-06:00",
            "date_modified": "2022-05-14T19:35:32-06:00",
            
                "author": "Thinking Out Loud"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/pipelib-simple-library-to-parse-filter-and-sort-things/",
            "title": "Pipelib- Simple Library to Parse, Filter, and Sort Things",
            "summary": null,
            "content_text": "In early April I added an “update” command to Singularity Registry HPC (see the pull request here and needed to start with a list of docker tags andparse them into version strings to sort, and still return the original tag for later use.I wound up creating a custom class and set of functions that use distutils.LooseVersion to support that, but in creating this“hard coded thing” I stepped back and had a question.  Can we more intelligentally compose custom parsing pipelines?Specifically I wanted to:Start with a list of container tags for an image from a registryFilter out anything that looks like a commit, but isn't a string (e.g., latest)Derive a major, minor, and patch version for each, and filter to newestSort!For step 3, as an example if there was a 1.2.3-commitA and 1.2.3-commitB I’d only want to keep one, and the newer one of the two,so I could ask for “unique by patch” and filter the older one out.Ultimately of course I dove right in,and this led to the creation of Pipelib, which was an itch I terribly wanted to scratch! In this quick post, I want to share the overall design, because it was really fun to make.DesignBefore we talk about the design, let me show it to you.import pipelib.steps as stepimport pipelib.pipeline as pipeline# A pipeline to process a list of stringssteps = (   # convert everything to lowercase   step.transform.ToLowercase(),   # don't include anything with \"two\"   ~step.filters.HasPatterns(filters=[\"two\"]))# Strings to processitems = ['item-ONE', 'item-TWO', 'item-two-THREE']p = pipeline.Pipeline(steps)# The updated and transformed itemsupdated = p.run(items)# ['item-one']In the above, we take a pipeline object and add steps to it. That design is fairly simple,as the Pipeline class takes an optional iterable of things to process. I say “things” becausewe can give it steps, composed steps, or even entire other pipelines. Here is an exampleof adding an entire other Pipeline!import pipelib.steps as stepimport pipelib.pipeline as pipelinefruits = [\"Orange\", \"Melon\", \"Watermelon\", \"Fruit23\"]preprocess = pipeline.Pipeline(    steps = (        # Example of chaining steps together        step.filters.HasMaxLength(length=8) &amp; step.filters.HasAllLetters(),    ))# Add this preprocess step alongside other steps (make lowercase)steps = (   step.transform.ToLowercase(),   preprocess,)# Create a new pipeline and runp = pipeline.Pipeline(steps)# We should expect orange and melon!updated = p.run(fruits)['orange', 'melon']Implementation-wise, this is also fairly simple. We can check the underlying class of the provided objectand either add a single step, or insert a set of steps given another pipeline. In fact, pipelib comes with asmall set of “pipelines” that are ready for you to use. For example, here is one tofilter out “things that look like complete or partial git commits”import pipelib.steps as stepimport pipelib.pipeline as pipeline# Pre-generated sets of steps we can useimport pipelib.pipelines as pipelinespipeline.Pipeline(    pipelines.git.RemoveCommits).run([\"832b1c\", \"832b1c645e562d5cc6e376e5a3e058c02a40d92a\", \"123-abcd\"])[\"123-abcd\"]This is something I found useful because people sometimes use commits as Docker tags, and I don’t find this incredibly meaningful as a version to compare to (and want to remove them). Under the hood, it looks like this:RemoveCommits = pipeline.Pipeline(    steps=(        step.filters.HasMinLength(length=8) &amp; ~step.filters.HasAllLowerLettersNumbers(),    ))Do you also notice something interesting in the above? We are actually combining steps akin to logical operations.The above “pipeline” is actually just one step that combined other steps!pipelines.git.RemoveCommits.steps[HasMinLength_AND_NotHasAllLowerLettersNumbers]Let’s step back and talk about some concepts that allow this.ConceptsPipelineAs we’ve seen above, a pipeline is a collection of steps that take, as input, a listing of items and return a parser and filtered list.StepA step is some action in a pipeline. The way this works is that we have different kinds of steps, and this makes them easyto implement and even test. A boolean step is akin to a filter, and is expected to return True or False to indicate if the item passes, e.g., False means it’s filtered out. Boolean steps are neat because they afford different kinds of logic and combination.Logical OperationsLet’s say that we have a step that checks that an input is all letters:step.filters.HasAllLetters()For the above, anything that had a number (e.g., orange123) would be filtered out. But what if we wanted to inverse that, and allow passing of inputs that don’t have all letters (meaning we want numbers or special characters?) We can simply do that:~step.filters.HasAllLetters()Implementation wise, this was really fun to do! For Python to respect the logical operator ~ I simply define the “invert” function for the BooleanStep class.def __invert__(self):    \"\"\"    We can say \"~step\" and reverse the logic.    \"\"\"    self.reverse = True    return selfIt sets an attribute “reverse” to True, and returns itself, that way we use the same step, but with this variable set to be true.What does that do? In the “run” function of the BooleanStep we basically retrieve an outcome from the underlying step (True or False) and simply reverse it given that boolean is True! Again, it’s very simple, and allows for doing things like this:from pipelib.pipeline import Pipelineimport pipelib.steps as stepsPipeline(~steps.filters.HasAllLetters()).run([\"I-have-special-characters\", \"Idonot\"])['I-have-special-characters']Pipeline(steps.filters.HasAllLetters()).run([\"I-have-special-characters\", \"Idonot\"])['Idonot']What if we wanted to combine steps? E.g., what if I want to say “has all letters” OR “has minimum length 10?” If we put the stepsside by side we would only be able to support an AND - allowing passing through of entries that have all letters and the minimum length of 10.Pipelib supports both those operators - AND and OR as follows:&gt; step = steps.filters.HasAllLetters() &amp; steps.filters.HasMinLength(length=10)&gt; stepHasAllLetters_AND_HasMinLengthPipeline(step).run([\"thisonewillpass\", \"thisoneno\", \"notthisone2\"])['thisonewillpass']For both cases above, we are using the “and” and “or functions, respectively, and:Checking for class compatibility (both must be BooleanStep)Creating a list of composed steps to added to a class attribute \"composed\"Add the previous run functions too, naming based on the step class nameDefine a new run function that loops through the composed set, runs, updates and returns a shared resultName the class based on the combined names of the composed classesFor step 4 above, the operation (AND or OR) will vary depending on if the initial call was to “and” or “or”.The main difference between the two is that “OR” starts with a default of False (otherwise it would always return True)and AND starts with a default of True (otherwise it would always return False).And since we are always taking the first class “composed” attribute, this means that you can composesteps with other steps as many times as you like - a new check is simply added to the front or back ofthe list. The result (returned) is the new class that is ready to run. Here is what an OR looks like:&gt; step = steps.filters.HasAllLetters() | steps.filters.HasMinLength(length=10)&gt; stepHasAllLetters_OR_HasMinLengthPipeline(step).run([\"thisonewillpass\", \"veryshort\", \"12345\"])['thisonewillpass', 'veryshort']If you are interested in this function, you can see the entire thing here.Transformation OperationsA base step can be thought of as a transformation. Instead of expecting a boolean to be returned, we areinstead expecting a new value or None. In this respect the transform step can also act as a boolean as a returnof “None” will be removed from the list, however in most cases a transform is intended to perform an operation on the item passed. Here is an example of a transformation operation:Pipeline(steps.transform.ToLowercase()).run([\"AHHHH\"])['ahhhh']Sort OperationsA sort operation is a step that is one level up. Instead of operating on individual items, the stepre-defines a the higher level “run” function and does operations across the iterable.A good example from Pipelib is the use case that originally inspired me - to start with a messylist of Docker tags, do some parsing to derive versions, and return back a sorted list.pipeline.Pipeline(steps.container.ContainerTagSort(ascending=False)).run([\"1.2.3\", \"0.1.0\", \"8.3.2\"])['8.3.2', '1.2.3', '0.1.0']pipeline.Pipeline(steps.container.ContainerTagSort(ascending=True)).run([\"1.2.3\", \"0.1.0\", \"8.3.2\"])['0.1.0', '1.2.3', '8.3.2']In the above we also demonstrate that steps can take parameters, such as the order of a sort!This particular sorting step also allows you to say you want to return unique major, minor, or patchversions.pipeline.Pipeline(steps.container.ContainerTagSort(unique_major=True)).run([\"1.2.3\", \"1.1.0\", \"8.3.2\"])['8.3.2', '1.2.3']And if you wanted to do a more comprehensive clean up and sort, you could do something like this.WrapperPipelib needed a way to be able to pass around some parsed version of an item, but still maintainthe original. For example, let’s say I’m parsing Docker tags into something that resembles a loosesemantic version, I might have filtered 1.2.3-boop to be just 1.2.3, but at the end of theday I need the original tag to pull. Pipelib accomplishes this via wrappers.A wrapper is conceptually that - an internal wrapper class to an item that allows for storingan original value, and still doing operations to change a current state. Wrappers are used inside steps and allow for things like sorting and comparison. You probably don’t need to worry about wrappersunless you want to develop for pipelib. By default, wrappers and “extracted away” to return the basictypes. However, you can ask Pipelib to not do this unwrapping, and then you can get backthe derived and original values:tags  = [\"1.2.3\", \"1.1.0\", \"8.3.2\"]updated = pipeline.Pipeline(steps.container.ContainerTagSort()).run(tags, unwrap=False)# Notice that this just looks like a set of strings...updated['8.3.2', '1.2.3']# But actually we have wrappers, that each have an _original attributetype(updated[0])pipelib.wrappers.version.VersionWrapperConclusionI’ve had so much fun making this library! Like many of my projects it’s probably not super useful,but if you see a cool use case please let me know! I’m also happy to develop custom pipelines or stepsfor a use case that you might be interested in. Please don’t hesitate to ask me for help, I’m always runningout of fun things to do :)  Why should I care?Arguably you could just hard code this kind of filtering and sorting, but I think theidea of being able to customize and assemble steps is a cool one. If the steps are providedin a library it might might it slightly easier, or your work more reproducible because someone else can use the steps. And if you don’t care? That’s okay too. I recognize this wasmostly a fun project, and yet-another-itch I really wanted to scratch because I’ve nevermade a design like this before, either in terms of the idea or underlying testing and automation.",
            "content_html": "<p>In early April I added an “update” command to Singularity Registry HPC (<a href=\"https://github.com/singularityhub/singularity-hpc/pull/538\" target=\"_blank\">see the pull request here</a> and needed to start with a list of docker tags andparse them into version strings to sort, and still return the original tag for later use.I wound up creating a <a href=\"https://github.com/singularityhub/singularity-hpc/blob/main/shpc/main/container/update/versions.py\" target=\"_blank\">custom class and set of functions</a> that use <a href=\"https://github.com/python/cpython/blob/bd030b633f98ea5d9f93ef0105a51d2faf67070d/Lib/distutils/version.py#L269\" target=\"_blank\">distutils.LooseVersion</a> to support that, but in creating this“hard coded thing” I stepped back and had a question.</p><blockquote>  <p>Can we more intelligentally compose custom parsing pipelines?</p></blockquote><p>Specifically I wanted to:</p><ol class=\"custom-counter\"><li>Start with a list of container tags for an image from a registry</li><li>Filter out anything that looks like a commit, but isn't a string (e.g., latest)</li><li>Derive a major, minor, and patch version for each, and filter to newest</li><li>Sort!</li></ol><p>For step 3, as an example if there was a <code class=\"language-plaintext highlighter-rouge\">1.2.3-commitA</code> and <code class=\"language-plaintext highlighter-rouge\">1.2.3-commitB</code> I’d only want to keep one, and the newer one of the two,so I could ask for “unique by patch” and filter the older one out.Ultimately of course I <a href=\"https://twitter.com/vsoch/status/1516197732708282369\" target=\"_blank\">dove right in</a>,and this led to the creation of <a href=\"https://vsoch.github.io/pipelib\" target=\"_blank\">Pipelib</a>, which was an itch I terribly wanted to scratch! In this quick post, I want to share the overall design, because it was really fun to make.</p><div style=\"padding: 20px;\"><img src=\"https://raw.githubusercontent.com/vsoch/pipelib/main/docs/assets/pipelib-small.png\" /></div><h2 id=\"design\">Design</h2><p>Before we talk about the design, let me show it to you.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">import</span> <span class=\"nn\">pipelib.steps</span> <span class=\"k\">as</span> <span class=\"n\">step</span><span class=\"kn\">import</span> <span class=\"nn\">pipelib.pipeline</span> <span class=\"k\">as</span> <span class=\"n\">pipeline</span><span class=\"c1\"># A pipeline to process a list of strings</span><span class=\"n\">steps</span> <span class=\"o\">=</span> <span class=\"p\">(</span>   <span class=\"c1\"># convert everything to lowercase</span>   <span class=\"n\">step</span><span class=\"p\">.</span><span class=\"n\">transform</span><span class=\"p\">.</span><span class=\"n\">ToLowercase</span><span class=\"p\">(),</span>   <span class=\"c1\"># don't include anything with \"two\"</span>   <span class=\"o\">~</span><span class=\"n\">step</span><span class=\"p\">.</span><span class=\"n\">filters</span><span class=\"p\">.</span><span class=\"n\">HasPatterns</span><span class=\"p\">(</span><span class=\"n\">filters</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">\"two\"</span><span class=\"p\">])</span><span class=\"p\">)</span><span class=\"c1\"># Strings to process</span><span class=\"n\">items</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s\">'item-ONE'</span><span class=\"p\">,</span> <span class=\"s\">'item-TWO'</span><span class=\"p\">,</span> <span class=\"s\">'item-two-THREE'</span><span class=\"p\">]</span><span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">pipeline</span><span class=\"p\">.</span><span class=\"n\">Pipeline</span><span class=\"p\">(</span><span class=\"n\">steps</span><span class=\"p\">)</span><span class=\"c1\"># The updated and transformed items</span><span class=\"n\">updated</span> <span class=\"o\">=</span> <span class=\"n\">p</span><span class=\"p\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">items</span><span class=\"p\">)</span><span class=\"c1\"># ['item-one']</span></code></pre></div></div><p>In the above, we take a pipeline object and add steps to it. That design is fairly simple,as the Pipeline class takes an optional iterable of things to process. I say “things” becausewe can give it steps, composed steps, or even entire other pipelines. Here is an exampleof adding an entire other Pipeline!</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">import</span> <span class=\"nn\">pipelib.steps</span> <span class=\"k\">as</span> <span class=\"n\">step</span><span class=\"kn\">import</span> <span class=\"nn\">pipelib.pipeline</span> <span class=\"k\">as</span> <span class=\"n\">pipeline</span><span class=\"n\">fruits</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s\">\"Orange\"</span><span class=\"p\">,</span> <span class=\"s\">\"Melon\"</span><span class=\"p\">,</span> <span class=\"s\">\"Watermelon\"</span><span class=\"p\">,</span> <span class=\"s\">\"Fruit23\"</span><span class=\"p\">]</span><span class=\"n\">preprocess</span> <span class=\"o\">=</span> <span class=\"n\">pipeline</span><span class=\"p\">.</span><span class=\"n\">Pipeline</span><span class=\"p\">(</span>    <span class=\"n\">steps</span> <span class=\"o\">=</span> <span class=\"p\">(</span>        <span class=\"c1\"># Example of chaining steps together</span>        <span class=\"n\">step</span><span class=\"p\">.</span><span class=\"n\">filters</span><span class=\"p\">.</span><span class=\"n\">HasMaxLength</span><span class=\"p\">(</span><span class=\"n\">length</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">)</span> <span class=\"o\">&amp;</span> <span class=\"n\">step</span><span class=\"p\">.</span><span class=\"n\">filters</span><span class=\"p\">.</span><span class=\"n\">HasAllLetters</span><span class=\"p\">(),</span>    <span class=\"p\">)</span><span class=\"p\">)</span><span class=\"c1\"># Add this preprocess step alongside other steps (make lowercase)</span><span class=\"n\">steps</span> <span class=\"o\">=</span> <span class=\"p\">(</span>   <span class=\"n\">step</span><span class=\"p\">.</span><span class=\"n\">transform</span><span class=\"p\">.</span><span class=\"n\">ToLowercase</span><span class=\"p\">(),</span>   <span class=\"n\">preprocess</span><span class=\"p\">,</span><span class=\"p\">)</span><span class=\"c1\"># Create a new pipeline and run</span><span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">pipeline</span><span class=\"p\">.</span><span class=\"n\">Pipeline</span><span class=\"p\">(</span><span class=\"n\">steps</span><span class=\"p\">)</span><span class=\"c1\"># We should expect orange and melon!</span><span class=\"n\">updated</span> <span class=\"o\">=</span> <span class=\"n\">p</span><span class=\"p\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">fruits</span><span class=\"p\">)</span><span class=\"p\">[</span><span class=\"s\">'orange'</span><span class=\"p\">,</span> <span class=\"s\">'melon'</span><span class=\"p\">]</span></code></pre></div></div><p>Implementation-wise, this is also fairly simple. We can check the underlying class of the provided objectand either add a single step, or insert a set of steps given another pipeline. In fact, pipelib comes with asmall set of “pipelines” that are ready for you to use. For example, here is one tofilter out “things that look like complete or partial git commits”</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">import</span> <span class=\"nn\">pipelib.steps</span> <span class=\"k\">as</span> <span class=\"n\">step</span><span class=\"kn\">import</span> <span class=\"nn\">pipelib.pipeline</span> <span class=\"k\">as</span> <span class=\"n\">pipeline</span><span class=\"c1\"># Pre-generated sets of steps we can use</span><span class=\"kn\">import</span> <span class=\"nn\">pipelib.pipelines</span> <span class=\"k\">as</span> <span class=\"n\">pipelines</span><span class=\"n\">pipeline</span><span class=\"p\">.</span><span class=\"n\">Pipeline</span><span class=\"p\">(</span>    <span class=\"n\">pipelines</span><span class=\"p\">.</span><span class=\"n\">git</span><span class=\"p\">.</span><span class=\"n\">RemoveCommits</span><span class=\"p\">).</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"s\">\"832b1c\"</span><span class=\"p\">,</span> <span class=\"s\">\"832b1c645e562d5cc6e376e5a3e058c02a40d92a\"</span><span class=\"p\">,</span> <span class=\"s\">\"123-abcd\"</span><span class=\"p\">])</span><span class=\"p\">[</span><span class=\"s\">\"123-abcd\"</span><span class=\"p\">]</span></code></pre></div></div><p>This is something I found useful because people sometimes use commits as Docker tags, and I don’t find this incredibly meaningful as a version to compare to (and want to remove them). Under the hood, it looks like this:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">RemoveCommits</span> <span class=\"o\">=</span> <span class=\"n\">pipeline</span><span class=\"p\">.</span><span class=\"n\">Pipeline</span><span class=\"p\">(</span>    <span class=\"n\">steps</span><span class=\"o\">=</span><span class=\"p\">(</span>        <span class=\"n\">step</span><span class=\"p\">.</span><span class=\"n\">filters</span><span class=\"p\">.</span><span class=\"n\">HasMinLength</span><span class=\"p\">(</span><span class=\"n\">length</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">)</span> <span class=\"o\">&amp;</span> <span class=\"o\">~</span><span class=\"n\">step</span><span class=\"p\">.</span><span class=\"n\">filters</span><span class=\"p\">.</span><span class=\"n\">HasAllLowerLettersNumbers</span><span class=\"p\">(),</span>    <span class=\"p\">)</span><span class=\"p\">)</span></code></pre></div></div><p>Do you also notice something interesting in the above? We are actually combining steps akin to logical operations.The above “pipeline” is actually just one step that combined other steps!</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">pipelines</span><span class=\"p\">.</span><span class=\"n\">git</span><span class=\"p\">.</span><span class=\"n\">RemoveCommits</span><span class=\"p\">.</span><span class=\"n\">steps</span><span class=\"p\">[</span><span class=\"n\">HasMinLength_AND_NotHasAllLowerLettersNumbers</span><span class=\"p\">]</span></code></pre></div></div><p>Let’s step back and talk about some concepts that allow this.</p><h2 id=\"concepts\">Concepts</h2><h3 id=\"pipeline\">Pipeline</h3><p>As we’ve seen above, a pipeline is a collection of steps that take, as input, a listing of items and return a parser and filtered list.</p><h3 id=\"step\">Step</h3><p>A step is some action in a pipeline. The way this works is that we have different kinds of steps, and this makes them easyto implement and even test. A <em>boolean</em> step is akin to a filter, and is expected to return True or False to indicate if the item passes, e.g., False means it’s filtered out. Boolean steps are neat because they afford different kinds of logic and combination.</p><h4 id=\"logical-operations\">Logical Operations</h4><p>Let’s say that we have a step that checks that an input is all letters:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">step</span><span class=\"p\">.</span><span class=\"n\">filters</span><span class=\"p\">.</span><span class=\"n\">HasAllLetters</span><span class=\"p\">()</span></code></pre></div></div><p>For the above, anything that had a number (e.g., orange123) would be filtered out. But what if we wanted to inverse that, and allow passing of inputs that don’t have all letters (meaning we want numbers or special characters?) We can simply do that:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">~</span><span class=\"n\">step</span><span class=\"p\">.</span><span class=\"n\">filters</span><span class=\"p\">.</span><span class=\"n\">HasAllLetters</span><span class=\"p\">()</span></code></pre></div></div><p>Implementation wise, this was really fun to do! For Python to respect the logical operator <code class=\"language-plaintext highlighter-rouge\">~</code> I simply define the “<strong>invert</strong>” function for the BooleanStep class.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">def</span> <span class=\"nf\">__invert__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>    <span class=\"s\">\"\"\"    We can say \"~step\" and reverse the logic.    \"\"\"</span>    <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">reverse</span> <span class=\"o\">=</span> <span class=\"bp\">True</span>    <span class=\"k\">return</span> <span class=\"bp\">self</span></code></pre></div></div><p>It sets an attribute “reverse” to True, and returns itself, that way we use the same step, but with this variable set to be true.What does that do? In the “run” <a href=\"https://github.com/vsoch/pipelib/blob/69d7d4ac677a24a31ffa9322f03090cf074442c8/pipelib/steps/step.py#L217-L238\" target=\"_blank\">function</a> of the BooleanStep we basically retrieve an outcome from the underlying step (True or False) and simply reverse it given that boolean is True! Again, it’s very simple, and allows for doing things like this:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">pipelib.pipeline</span> <span class=\"kn\">import</span> <span class=\"n\">Pipeline</span><span class=\"kn\">import</span> <span class=\"nn\">pipelib.steps</span> <span class=\"k\">as</span> <span class=\"n\">steps</span><span class=\"n\">Pipeline</span><span class=\"p\">(</span><span class=\"o\">~</span><span class=\"n\">steps</span><span class=\"p\">.</span><span class=\"n\">filters</span><span class=\"p\">.</span><span class=\"n\">HasAllLetters</span><span class=\"p\">()).</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"s\">\"I-have-special-characters\"</span><span class=\"p\">,</span> <span class=\"s\">\"Idonot\"</span><span class=\"p\">])</span><span class=\"p\">[</span><span class=\"s\">'I-have-special-characters'</span><span class=\"p\">]</span><span class=\"n\">Pipeline</span><span class=\"p\">(</span><span class=\"n\">steps</span><span class=\"p\">.</span><span class=\"n\">filters</span><span class=\"p\">.</span><span class=\"n\">HasAllLetters</span><span class=\"p\">()).</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"s\">\"I-have-special-characters\"</span><span class=\"p\">,</span> <span class=\"s\">\"Idonot\"</span><span class=\"p\">])</span><span class=\"p\">[</span><span class=\"s\">'Idonot'</span><span class=\"p\">]</span></code></pre></div></div><p>What if we wanted to combine steps? E.g., what if I want to say “has all letters” OR “has minimum length 10?” If we put the stepsside by side we would only be able to support an AND - allowing passing through of entries that have all letters and the minimum length of 10.Pipelib supports both those operators - AND and OR as follows:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;</span> <span class=\"n\">step</span> <span class=\"o\">=</span> <span class=\"n\">steps</span><span class=\"p\">.</span><span class=\"n\">filters</span><span class=\"p\">.</span><span class=\"n\">HasAllLetters</span><span class=\"p\">()</span> <span class=\"o\">&amp;</span> <span class=\"n\">steps</span><span class=\"p\">.</span><span class=\"n\">filters</span><span class=\"p\">.</span><span class=\"n\">HasMinLength</span><span class=\"p\">(</span><span class=\"n\">length</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span><span class=\"o\">&gt;</span> <span class=\"n\">step</span><span class=\"n\">HasAllLetters_AND_HasMinLength</span><span class=\"n\">Pipeline</span><span class=\"p\">(</span><span class=\"n\">step</span><span class=\"p\">).</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"s\">\"thisonewillpass\"</span><span class=\"p\">,</span> <span class=\"s\">\"thisoneno\"</span><span class=\"p\">,</span> <span class=\"s\">\"notthisone2\"</span><span class=\"p\">])</span><span class=\"p\">[</span><span class=\"s\">'thisonewillpass'</span><span class=\"p\">]</span></code></pre></div></div><p>For both cases above, we are using the “<strong>and</strong>” and “<strong>or</strong> functions, respectively, and:</p><ol class=\"custom-counter\"><li>Checking for class compatibility (both must be BooleanStep)</li><li>Creating a list of composed steps to added to a class attribute \"composed\"</li><li>Add the previous run functions too, naming based on the step class name</li><li>Define a new run function that loops through the composed set, runs, updates and returns a shared result</li><li>Name the class based on the combined names of the composed classes</li></ol><p>For step 4 above, the operation (AND or OR) will vary depending on if the initial call was to “<strong>and</strong>” or “<strong>or</strong>”.The main difference between the two is that “OR” starts with a default of False (otherwise it would always return True)and AND starts with a default of True (otherwise it would always return False).And since we are always taking the first class “composed” attribute, this means that you can composesteps with other steps as many times as you like - a new check is simply added to the front or back ofthe list. The result (returned) is the new class that is ready to run. Here is what an OR looks like:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;</span> <span class=\"n\">step</span> <span class=\"o\">=</span> <span class=\"n\">steps</span><span class=\"p\">.</span><span class=\"n\">filters</span><span class=\"p\">.</span><span class=\"n\">HasAllLetters</span><span class=\"p\">()</span> <span class=\"o\">|</span> <span class=\"n\">steps</span><span class=\"p\">.</span><span class=\"n\">filters</span><span class=\"p\">.</span><span class=\"n\">HasMinLength</span><span class=\"p\">(</span><span class=\"n\">length</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span><span class=\"o\">&gt;</span> <span class=\"n\">step</span><span class=\"n\">HasAllLetters_OR_HasMinLength</span><span class=\"n\">Pipeline</span><span class=\"p\">(</span><span class=\"n\">step</span><span class=\"p\">).</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"s\">\"thisonewillpass\"</span><span class=\"p\">,</span> <span class=\"s\">\"veryshort\"</span><span class=\"p\">,</span> <span class=\"s\">\"12345\"</span><span class=\"p\">])</span><span class=\"p\">[</span><span class=\"s\">'thisonewillpass'</span><span class=\"p\">,</span> <span class=\"s\">'veryshort'</span><span class=\"p\">]</span></code></pre></div></div><p>If you are interested in this function, you can see the entire thing <a href=\"https://github.com/vsoch/pipelib/blob/832b1c645e562d5cc6e376e5a3e058c02a40d92a/pipelib/steps/step.py#L177-L241\" target=\"_blank\">here</a>.</p><h4 id=\"transformation-operations\">Transformation Operations</h4><p>A base step can be thought of as a transformation. Instead of expecting a boolean to be returned, we areinstead expecting a new value or None. In this respect the transform step can also act as a boolean as a returnof “None” will be removed from the list, however in most cases a transform is intended to perform an operation on the item passed. Here is an example of a transformation operation:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">Pipeline</span><span class=\"p\">(</span><span class=\"n\">steps</span><span class=\"p\">.</span><span class=\"n\">transform</span><span class=\"p\">.</span><span class=\"n\">ToLowercase</span><span class=\"p\">()).</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"s\">\"AHHHH\"</span><span class=\"p\">])</span><span class=\"p\">[</span><span class=\"s\">'ahhhh'</span><span class=\"p\">]</span></code></pre></div></div><h4 id=\"sort-operations\">Sort Operations</h4><p>A sort operation is a step that is one level up. Instead of operating on individual items, the stepre-defines a the higher level “run” function and does operations across the iterable.A good example from Pipelib is the use case that originally inspired me - to start with a messylist of Docker tags, do some parsing to derive versions, and return back a sorted list.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">pipeline</span><span class=\"p\">.</span><span class=\"n\">Pipeline</span><span class=\"p\">(</span><span class=\"n\">steps</span><span class=\"p\">.</span><span class=\"n\">container</span><span class=\"p\">.</span><span class=\"n\">ContainerTagSort</span><span class=\"p\">(</span><span class=\"n\">ascending</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">)).</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"s\">\"1.2.3\"</span><span class=\"p\">,</span> <span class=\"s\">\"0.1.0\"</span><span class=\"p\">,</span> <span class=\"s\">\"8.3.2\"</span><span class=\"p\">])</span><span class=\"p\">[</span><span class=\"s\">'8.3.2'</span><span class=\"p\">,</span> <span class=\"s\">'1.2.3'</span><span class=\"p\">,</span> <span class=\"s\">'0.1.0'</span><span class=\"p\">]</span><span class=\"n\">pipeline</span><span class=\"p\">.</span><span class=\"n\">Pipeline</span><span class=\"p\">(</span><span class=\"n\">steps</span><span class=\"p\">.</span><span class=\"n\">container</span><span class=\"p\">.</span><span class=\"n\">ContainerTagSort</span><span class=\"p\">(</span><span class=\"n\">ascending</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)).</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"s\">\"1.2.3\"</span><span class=\"p\">,</span> <span class=\"s\">\"0.1.0\"</span><span class=\"p\">,</span> <span class=\"s\">\"8.3.2\"</span><span class=\"p\">])</span><span class=\"p\">[</span><span class=\"s\">'0.1.0'</span><span class=\"p\">,</span> <span class=\"s\">'1.2.3'</span><span class=\"p\">,</span> <span class=\"s\">'8.3.2'</span><span class=\"p\">]</span></code></pre></div></div><p>In the above we also demonstrate that steps can take parameters, such as the order of a sort!This particular sorting step also allows you to say you want to return unique major, minor, or patchversions.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">pipeline</span><span class=\"p\">.</span><span class=\"n\">Pipeline</span><span class=\"p\">(</span><span class=\"n\">steps</span><span class=\"p\">.</span><span class=\"n\">container</span><span class=\"p\">.</span><span class=\"n\">ContainerTagSort</span><span class=\"p\">(</span><span class=\"n\">unique_major</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)).</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"s\">\"1.2.3\"</span><span class=\"p\">,</span> <span class=\"s\">\"1.1.0\"</span><span class=\"p\">,</span> <span class=\"s\">\"8.3.2\"</span><span class=\"p\">])</span><span class=\"p\">[</span><span class=\"s\">'8.3.2'</span><span class=\"p\">,</span> <span class=\"s\">'1.2.3'</span><span class=\"p\">]</span></code></pre></div></div><p>And if you wanted to do a more comprehensive clean up and sort, you could do <a href=\"https://vsoch.github.io/pipelib/getting_started/user-guide.html#a-real-world-example-docker-tags\" target=\"_blank\">something like this</a>.</p><h3 id=\"wrapper\">Wrapper</h3><p>Pipelib needed a way to be able to pass around some parsed version of an item, but still maintainthe original. For example, let’s say I’m parsing Docker tags into something that resembles a loosesemantic version, I might have filtered <code class=\"language-plaintext highlighter-rouge\">1.2.3-boop</code> to be just <code class=\"language-plaintext highlighter-rouge\">1.2.3</code>, but at the end of theday I need the original tag to pull. Pipelib accomplishes this via wrappers.</p><p>A wrapper is conceptually that - an internal wrapper class to an item that allows for storingan original value, and still doing operations to change a current state. Wrappers are used inside steps and allow for things like sorting and comparison. You probably don’t need to worry about wrappersunless you want to develop for pipelib. By default, wrappers and “extracted away” to return the basictypes. However, you can ask Pipelib to not do this unwrapping, and then you can get backthe derived and original values:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">tags</span>  <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s\">\"1.2.3\"</span><span class=\"p\">,</span> <span class=\"s\">\"1.1.0\"</span><span class=\"p\">,</span> <span class=\"s\">\"8.3.2\"</span><span class=\"p\">]</span><span class=\"n\">updated</span> <span class=\"o\">=</span> <span class=\"n\">pipeline</span><span class=\"p\">.</span><span class=\"n\">Pipeline</span><span class=\"p\">(</span><span class=\"n\">steps</span><span class=\"p\">.</span><span class=\"n\">container</span><span class=\"p\">.</span><span class=\"n\">ContainerTagSort</span><span class=\"p\">()).</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">tags</span><span class=\"p\">,</span> <span class=\"n\">unwrap</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">)</span><span class=\"c1\"># Notice that this just looks like a set of strings...</span><span class=\"n\">updated</span><span class=\"p\">[</span><span class=\"s\">'8.3.2'</span><span class=\"p\">,</span> <span class=\"s\">'1.2.3'</span><span class=\"p\">]</span><span class=\"c1\"># But actually we have wrappers, that each have an _original attribute</span><span class=\"nb\">type</span><span class=\"p\">(</span><span class=\"n\">updated</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span><span class=\"n\">pipelib</span><span class=\"p\">.</span><span class=\"n\">wrappers</span><span class=\"p\">.</span><span class=\"n\">version</span><span class=\"p\">.</span><span class=\"n\">VersionWrapper</span></code></pre></div></div><h2 id=\"conclusion\">Conclusion</h2><p>I’ve had so much fun making this library! Like many of my projects it’s probably not super useful,but if you see a cool use case please let me know! I’m also happy to develop custom pipelines or stepsfor a use case that you might be interested in. Please don’t hesitate to ask me for help, I’m always runningout of fun things to do :)</p><blockquote>  <p>Why should I care?</p></blockquote><p>Arguably you could just hard code this kind of filtering and sorting, but I think theidea of being able to customize and assemble steps is a cool one. If the steps are providedin a library it might might it slightly easier, or your work more reproducible because someone else can use the steps. And if you don’t care? That’s okay too. I recognize this wasmostly a fun project, and yet-another-itch I really wanted to scratch because I’ve nevermade a design like this before, either in terms of the idea or <a href=\"https://twitter.com/vsoch/status/1521670410852442112\" target=\"_blank\">underlying testing and automation</a>.</p>",
            "url": "https://hpc.social/personal-blog/2022/pipelib-simple-library-to-parse-filter-and-sort-things/",
            
            
            
            
            
            "date_published": "2022-05-07T13:30:00-06:00",
            "date_modified": "2022-05-07T13:30:00-06:00",
            
                "author": "Vanessasaurus"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/the-research-software-ecosystem/",
            "title": "The Research Software Ecosystem",
            "summary": null,
            "content_text": "We recently published the Research Software Encyclopedia and also have added several new parsers for obtaining new data, meaning the total collectionof curated research software is greater than 1500entries. In honor of this collection, and of a library I’m working on called CiteLang, I wanted to do a small study to better understand:What are the most valuable dependencies in our community, across languages?What are the most valuable dependencies in our community, by language?What is the credit allocation for each repository?CiteLangTo step back for a second, let’s talk again about CiteLang. It has many functions - one of thembeing an ability to assess opensource contributions via git, but it’s main purpose is to be a markdown syntax for citing software,meaning that we can:Generate basic software credit trees, graphs, and markdown summaries.Derive a new, customizable model of credit based on published packages and dependencies.Provide a way to cite software in a paper and give credit without needing DOIs.As a simple example, I can run CiteLang over this markdown file with CiteLang references:# SummaryPortability and reproducibility of complex software stacks is essential for researchers to perform their work. High Performance Computing (HPC) environments add another level of complexity, where possibly conflicting dependencies must co-exist. Although container technologies like Singularity @conda{name=singularity} make it possible to \"bring your own environment,\" without any form of central strategy to manage containers, researchers who seek reproducibility via using containers are tasked with managing their own container collection, often not taking care to ensure that a particular digest or version is used. The reproducibility of the work is at risk, as they cannot easily install and use containers, nor can they share their software with others.Singularity Registry HPC (shpc) @pypi{name=singularity-hpc} is the first of its kind to provide an easy means for a researcher to add their research software for sharing and collaboration with other researchers to an existing collection of over 200 popular scientific libraries @github{name=autamus/registry} @github{name=spack/spack, release=0.17}. The software installs containers as environment modules that are easyto use and read documentation for, and exposes aliases for commands in the container that the researcher can add to their pipeline without thinking about complex interactions with a container. The simple addition of an entry to the registry maintained by shpc comes down to adding a yaml file, and after doing this, another researcher can easily install the same software, down to the digest, to reproduce the original work.# References&lt;!--citelang start--&gt;&lt;!--citelang end--&gt;And then run citelang render paper.md to get a nice rendered table alongside your paper! What CiteLang does is find the references in the paper, they look like this:@conda{name=singularity}@pypi{name=singularity-hpc}@github{name=autamus/registry} @github{name=spack/spack, release=0.17}Each of the references above is a package manager with a package name and (optionally) a version, and we can load in the metadatafor each and then generate a table that you see here that summarizes credit across dependencies. In this model, we give some allocation of credit (default is 50%) to the main work (paper or software) citing the software, and then recursively parse dependencies up to some minimum level of credit to calculate scores. Dependencies shared across libraries are averaged together. The final table represents the credit that you give not only to the top level software, but to all nested dependencies, for the work that you did. And that’s only the basics! CiteLang takes this simple ability to parse references and extends it to automation, graphs, badges, and more! You can read more about CiteLang here.  Publish or perish? How about neither? I just need to keep writing software!But do you see what is happening above? We aren’t requiring some artificial publicationin order to cite software. We are citing it based on its actual usage, as a known dependency to some other software.In a nutshell, we don’t believe that “the traditional academic way” of citing papers makes sense for software, and insteadof using DOIs we can use package managers and metadata as a source of truth, and derive the real value of a piece of softwarebased on this ecosystem. This means that as a research software engineer, you can just keep doing what you are already doing, and ifsomeone uses CiteLang to summarize their work, given that your software is published to a package managed you’ll get credit. Thereare so many cool ideas around this! But let’s start at the beginning. We first want to show how to summarize an ecosystem.That is exactly what we are going to do in this post.The Research Software EcosytemStarting with these curated repositories from a  set of scrapers including the Journal of Open Source Software, the HAL Research Software Database, the Research Software NL Dictionary, ROpenSci, and The Molecular Sciences Software Institute, we can do a basic analysis to identify the most used (and thus valued) pieces of software in our ecosystem. My analysis plan was to:Start with the current database.For each repository, look for requirements files to parse.Derive dependency data based on this requirements file.Combine and rank to discover the top dependencies!This of course is limited to the subset of software in our database, and the ability of CiteLang to parse a requirements file.Currently we parse setup.py and requirements.txt (Python), DESCRIPTION (R), go.mod (Go), package.json (npm), and Gemfile (ruby). Based on thebreakdown of the languages found in the RSEPedia, this is a reasonable start!  But it’s also kind of sad to see that my favorite languages (Go and Rust) are barely represented in our community. Also, the aboveshould tell you that the R and Python results likely have some meaningful interpretation, but the others not so much, only because we don’t have a big enough sample. So for all of the abovesteps, for these 1500+ repositories and many languages, I wanted th entire process to be automated, always have potential for easy improvement,and run at some regular interval as new software comes into the Research Software Encyclopedia (also automated) so we can derive changes over time.If you dont’ care to read further:View the Research Software EcosystemCheck out Languages hereResults for Dependencies hereIndividual Repositories hereFor this first publication of the interface we have the following metrics:  And I’m so excited because a tiny vision I had a few years ago to provide (and use) a community research software database is comingto live! So without further adeiu, I’m just going to jump into the cool results! It will be fun to see how these change over time.PythonLadies and gents, dinosaurs and rabbits! Your Python results:  So here is the first awesome insight. Is anyone really surprised to see numpy as the number one library?The credit value here says that the average Python repository is attributing about 3% of credit to numpy, meaning it is a direct or indirect dependency. Let that sink in! Here is the irony - when is the last time you cited numpy? You probably haven’t, because you’ve cited somethingthat uses it. We don’t remember numpy despite the fact that it’s so core to everything that we do.  The fact that the most widely used library is rarely cited is huge evidence for why a manual “write papers and cite DOIs” approach just won’t work for software.What else do we see in this list? Let me name a few things. First, we can’t be so terrible at remembering to look at or visualizethings because matplotlib is second. At least for research software, this is telling us that making plots or charts is important.The next (possibly surprising) result is that documentation and testing is at least represented, and this might be a biased samplebecause we include repositories that are peer reviewed (e.g., JoSS) and documentation and testing is necessary for that. Given this need for Python, sphinx and pytest come up as leaders to provide that. So here is another nugget of insight:  Some of us are so  busy focusing on domain-specific software that we forget the importance of the “less sexy” research software that helps us test, document, view things, or even create simple data structures.This kind of “base” software has always been what I’ve been most interested in, and ironically what people tell me time and time again“That’s not research software.” Oh really? So something that is entirely powering the research community is not research software?Of course I have my own strong opinions about a taxonomy for research software, but I would encourage those of you who are very dismissive to take a step back andconsider what you are really saying.The next insight is that we see a lot of libraries for data formats (e.g., pyaml, h5py, lxml, and more lower in the list) and this is an attestment to how important being able to read, serialize, and save data is.The final insight is the fact that requests is high in the list. For those of you not familiar, requests is a library for doing that, makinghttp requests to get content from some webby place. This is an attestment to the fact that our work is increasingly relying on external APIs,automation, or other resources provided on the web.You can see the full Python results here.RI’m less of an R programmer these days, but I think that these results also make sense.  We don’t see any huge leaders in the same way as we see numpy in Python, but not surprisingly the leader packagefor the R language is, well, R! I at first thought this was a bug, but actually R DESCRIPTION files that we parse do commonly include a pinned version of R:Depends: R (&gt;= 3.4.1), TailRank, ...And so we actually can give credit to the language proper! If you don’t feel this is appropriate, feel free to skip this line and considerthe top package jsonlite. This is also why I think json would be represented in Python if it wasn’t part of the standard library. Us research folks - we need our json! Overall I think we see a similar pattern here as we saw with Python. The libraries that float to the top are those that involve data structures (jsonlite, yaml), webby requests or similar (httr, curl), documentation and testing (knitr, rmarkdown) and graphics or visualization.  What does this tell us about what is undervalued in research software? Again, it’s not the domain specific libraries, but rather the core stuff that enables those libraries.You can see the full R results here.ProjectsIf you are interested in a specific project in the RSEPedia, we also provide a project-specific table and badge! You can browse projects from here, and here is an example of a badge generated for a project called  github.com/ORNL/tx2 (and on GitHub). Without even looking I can tell you we have some machine learning and/or visualization going on here (scikit-learn! umap! pandas! matplotlib)!  Notice how numpy (as an example) shows up at multiple points in the tree - when we calculate an overall credit, say, for the ecosystem, we take that into account! And we can then peek at the project-specific table and sort of verify that yes, this is a Python ML/visualization project:  And we see some surprises! Like, the slack-sdk? What? Believe it or not, that is pulled in by tqdm. The project-specific tables (and the description at the top) also give you a better sense of how CiteLang allocatescredit. The top level package is given 50%, and then the other 50% is given to all dependencies in the same fashion.We cut off at a value of 0.001, and we do that in case we might be parsing dependencies forever down to some infintesimally small amount.Finally, every project serves its own raw data  and the site is searchable, because sites should be. 😄️DiscussionI’m so happy (and a bit relieved, to be honest) to finally be able to show what I’ve been saying for years - that the most valuable software for research, and the software that is driving domain-specific research software, are the unsexy libraries that have to do with data structures, (maybe standards), documentation or testing, and data formats or retrieval. These are the packages that you aren’t going to remember to cite. Also, this set is totally leaving out the software we use on a day to day basis in our CI, which arguably isn’t research software but has done more for the research community than anything I can think of - containers, version control (git), and continuous integration. We’d be a mess without it. We need to be more thankful and aware of this, and for some of y’all that turn down your nose to anything that isn’t a domain-science library, perhaps take a pause. Next, let’s talk about limitations and hopes for the future.A Living DatabaseI wouldn’t have been happy with myself to simply publish software at one point in time and call it a day.The Research Software Encyclopedia is updated weekly, and so I’ve designed this analysis to do the same!This means that while we do cache a result for a newly added piece of software, we do continue to grow the analysis as new software is added. And since the tool will always use the newly updated CiteLang, any improvements to the parsers there will be reflected here! And if anyone wants to run the entire thing again (outside of the limit of GitHub Actions) they can clone the repository, nuke the _repos folder, and run the scripts again.Language GapsThe biggest gap in the RSEPedia is with respect to what we don’t see. First, despite being a prominent language, we don’t see anything for C++, because there isn’t a package manager with an API to use it. If you have a nifty (or even hacky) idea for how to parse a requirements file, I want to hear it. The RSEPedia has support for spack, but most research-oriented C++ projects are not going to go out of their way to publish their package there, and we get no signal of the package being in spack when we clone the repository. Sppaaaaaack (sorry, it’s a bit of a tic at this point!) 😁️We also don’t see standard modules or libraries provided within a language. E.g., I can almost guarantee you a ton of Python libraries are importing json, but since it’s not a package manager library we wouldn’t see it. I suspect citelang could come up with a way to derive credit for these libraries by way of abstract syntax trees or just parsing the source code, although I haven’t done this yet because I’m not convinced it’s something people are as interested in. If you want to say thank you for the Python standard library, there is a donate button on their contribution page (or you could contribute code). There is an even deeper level of parsing (at least for Python) that looks at function signatures, and I wrote a library called caliper in early 2021 to do that, and it’s able to generate function databases for Python software of interest. This would be cool to do for some kind of (unrelated) compatibility analysis here, but yes that’s very different.Parsing LimitationFor all requirements files except for Python, we are forced to do static parsing. While not perfect because bugs can happen for niche cases of someone defining requirements in a weird way, it’s a reasonable start. There is always room for improvement, or adding more static parsers for requirements files I have not considered yet.However, this is not the case for the Python parsing (either requirements.txt or setup.py)! For Python these results are likely very good because we wrap the pypi package manager install command to derive a list of packages and versions from either a setup.py or requirements.txt. Don’t worry - nothing is installed, we either just parse the requirements file and return the results, or we use the solveragainst a setup.py to come to an equivalent list. We originally had a static parser (and still use this as a fallback) however I talked to @alecbcs and he had this fantastic idea! Will it likely need updates as time goes on, giventhe functions are private? Sure. But I’m happy to do that to get the much more accurate listing.In practice, the only setup.py files that I was not able to parse either had a bug (e.g., trying to read a file that doesn’t exist in the repository) or they were trying to use modules outside of the standard library. For all of the cases of broken-ness, I opened issues on the respective repositories so we might have a better chance at parsing in the future! One detail is that we parse the first requirements file found. For a primary requirements file in the root of the repository, this is the best outcome. However, some repos don’t have a file in the root, and perhaps we find one in a documentation folder instead. Either way, the result represents our best effort at finding and parsing requirements given a cloned repository we don’t know the structure of in advance.Final ThoughtsHere are my final takeaways:Publication is not for Research SoftwareA system of credit that relies on software engineers to do extra manual work (to write papers) is never going to fully capture the ecosystem and give proper credit. It will only capture those that have the time and possibly privilege to take the extra time to write a paper.Publication only makes sense given that a piece of software is paired alongside a robust result, in which case fine, write the paper and also champion the software.Publication Does not Actually Capture CreditA system that also only skims the superficial top (the name of one package) and does not dig deep into a dependency tree is also going to miss insights and deserved attributions of credit. As the numpy example shows, nobody is actually citing numpy, but a ton of projects are using it somewhere in their dependency tree, so it deserves a lot of credit.We Can Do BetterI have a pet peeve. I’m frankly just tired of people writing about credit and attribution but not doing anything about it. We could extend that to other things, but it’s especially an issue for this topic. Ironically they are writing papers and improving their publication record as they write about how publication and research software is a strained process. I may not have solved this problem, but damn at least I’m trying to actually do something about it instead of spurting gas.I find this idea exciting because there are so many directions you can go with it. When I first designed the idea I imagined a database and online interface where you could essentially connect your GitHub repository, and akin to a builder service, parse your repository on some event and derive a new credit or citation graph. Or you could have some set akin to the RSEPedia that are also updated regularly. And then, by way of having that database, we could do these same queries (that currently I’m doing statically) to say “What are the most important libraries for this language? Across the ecosystem?” or “How has this changed over time?” It would be a true way to derive the value of a library without needing people to publish papers, and totally automated and integrated with package managers, which is where people already should be putting their software.Heck, if someone gave me a cloud and a little bit of funding I’d love to work on this. Are there good reasons or use cases? I don’t know, but maybe.So what do you think?",
            "content_html": "<p>We recently published <a href=\"https://openresearchsoftware.metajnl.com/articles/10.5334/jors.359/\" target=\"_blank\">the Research Software Encyclopedia</a> and also have added several new parsers for obtaining new data, meaning the total collectionof <a href=\"https://rseng.github.io/software/\" target=\"_blank\">curated research software</a> is greater than 1500entries. In honor of this collection, and of a library I’m working on called <a href=\"https://vsoch.github.io/citelang/getting_started/user-guide.html\" target=\"_blank\">CiteLang</a>, I wanted to do a small study to better understand:</p><ol class=\"custom-counter\"><li>What are the most valuable dependencies in our community, across languages?</li><li>What are the most valuable dependencies in our community, by language?</li><li>What is the credit allocation for each repository?</li></ol><h2 id=\"citelang\">CiteLang</h2><p>To step back for a second, let’s talk again about CiteLang. It has many functions - one of thembeing an ability to <a href=\"https://vsoch.github.io/2022/citelang-contrib/\" target=\"_blank\">assess opensource contributions</a> via git, but it’s main purpose is to be a markdown syntax for citing software,meaning that we can:</p><ol class=\"custom-counter\"><li>Generate basic software credit trees, graphs, and markdown summaries.</li><li>Derive a new, customizable model of credit based on published packages and dependencies.</li><li>Provide a way to cite software in a paper and give credit without needing DOIs.</li></ol><p>As a simple example, I can run CiteLang over this markdown file with CiteLang references:</p><div class=\"language-md highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"gh\"># Summary</span>Portability and reproducibility of complex software stacks is essential for researchers to perform their work. High Performance Computing (HPC) environments add another level of complexity, where possibly conflicting dependencies must co-exist. Although container technologies like Singularity @conda{name=singularity} make it possible to \"bring your own environment,\" without any form of central strategy to manage containers, researchers who seek reproducibility via using containers are tasked with managing their own container collection, often not taking care to ensure that a particular digest or version is used. The reproducibility of the work is at risk, as they cannot easily install and use containers, nor can they share their software with others.Singularity Registry HPC (shpc) @pypi{name=singularity-hpc} is the first of its kind to provide an easy means for a researcher to add their research software for sharing and collaboration with other researchers to an existing collection of over 200 popular scientific libraries @github{name=autamus/registry} @github{name=spack/spack, release=0.17}. The software installs containers as environment modules that are easyto use and read documentation for, and exposes aliases for commands in the container that the researcher can add to their pipeline without thinking about complex interactions with a container. The simple addition of an entry to the registry maintained by shpc comes down to adding a yaml file, and after doing this, another researcher can easily install the same software, down to the digest, to reproduce the original work.<span class=\"gh\"># References</span><span class=\"c\">&lt;!--citelang start--&gt;</span><span class=\"c\">&lt;!--citelang end--&gt;</span></code></pre></div></div><p>And then run <code class=\"language-plaintext highlighter-rouge\">citelang render paper.md</code> to get a <a href=\"https://gist.github.com/vsoch/41b4559d8f87eb9d6e62945e02689428\" target=\"_blank\">nice rendered table alongside your paper</a>! What CiteLang does is find the references in the paper, they look like this:</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>@conda{name=singularity}@pypi{name=singularity-hpc}@github{name=autamus/registry} @github{name=spack/spack, release=0.17}</code></pre></div></div><p>Each of the references above is a package manager with a package name and (optionally) a version, and we can load in the metadatafor each and then generate a table <a href=\"https://gist.github.com/vsoch/41b4559d8f87eb9d6e62945e02689428\" target=\"_blank\">that you see here</a> that summarizes credit across dependencies. In this model, we give some allocation of credit (default is 50%) to the main work (paper or software) citing the software, and then recursively parse dependencies up to some minimum level of credit to calculate scores. Dependencies shared across libraries are averaged together. The final table represents the credit that you give not only to the top level software, but to all nested dependencies, for the work that you did. And that’s only the basics! CiteLang takes this simple ability to parse references and extends it to automation, graphs, badges, and more! You can read more about CiteLang <a href=\"https://vsoch.github.io/citelang/getting_started/index.html\" target=\"_blank\">here</a>.</p><blockquote>  <p>Publish or perish? How about neither? I just need to keep writing software!</p></blockquote><p>But do you see what is happening above? We aren’t requiring some artificial publicationin order to cite software. We are citing it based on its actual usage, as a known dependency to some other software.In a nutshell, we don’t believe that “the traditional academic way” of citing papers makes sense for software, and insteadof using DOIs we can use package managers and metadata as a source of truth, and derive the real value of a piece of softwarebased on this ecosystem. This means that as a research software engineer, you can just keep doing what you are already doing, and ifsomeone uses CiteLang to summarize their work, given that your software is published to a package managed you’ll get credit. Thereare so many cool ideas around this! But let’s start at the beginning. We first want to show how to summarize an ecosystem.That is exactly what we are going to do in this post.</p><h2 id=\"the-research-software-ecosytem\">The Research Software Ecosytem</h2><p>Starting with these curated repositories from a <a href=\"https://rseng.github.io/rse/getting-started/scrapers/index.html\" target=\"_blank\"> set of scrapers</a> including the Journal of Open Source Software, the HAL Research Software Database, the Research Software NL Dictionary, ROpenSci, and The Molecular Sciences Software Institute, we can do a basic analysis to identify the most used (and thus valued) pieces of software in our ecosystem. My analysis plan was to:</p><ol class=\"custom-counter\"><li>Start with the current database.</li><li>For each repository, look for requirements files to parse.</li><li>Derive dependency data based on this requirements file.</li><li>Combine and rank to discover the top dependencies!</li></ol><p>This of course is limited to the subset of software in our database, and the ability of CiteLang to parse a requirements file.Currently we parse setup.py and requirements.txt (Python), DESCRIPTION (R), go.mod (Go), package.json (npm), and Gemfile (ruby). Based on the<a href=\"https://rseng.github.io/rsepedia-analysis/analysis/languages/\" target=\"_blank\">breakdown of the languages</a> found in the RSEPedia, this is a reasonable start!</p><div style=\"padding: 20px;\">  <img src=\"https://vsoch.github.io/assets/images/posts/citelang/languages.png\" /></div><p>But it’s also kind of sad to see that my favorite languages (Go and Rust) are barely represented in our community. Also, the aboveshould tell you that the R and Python results likely have some meaningful interpretation, but the others not so much, only because we don’t have a big enough sample. So for all of the abovesteps, for these 1500+ repositories and many languages, I wanted th entire process to be automated, always have potential for easy improvement,and run at some regular interval as new software comes into the Research Software Encyclopedia (also automated) so we can derive changes over time.If you dont’ care to read further:</p><ol class=\"custom-counter\"><li><a href=\"https://rseng.github.io/rsepedia-analysis/\" target=\"_blank\">View the Research Software Ecosystem</a></li><li><a href=\"https://rseng.github.io/rsepedia-analysis/analysis/languages/\" target=\"_blank\">Check out Languages here</a></li><li><a href=\"https://rseng.github.io/rsepedia-analysis/analysis/dependencies/\" target=\"_blank\">Results for Dependencies here</a></li><li><a href=\"https://rseng.github.io/rsepedia-analysis/analysis/repos/\" target=\"_blank\">Individual Repositories here</a></li></ol><p>For this first publication of the interface we have the following metrics:</p><div style=\"padding: 20px;\">  <img src=\"https://vsoch.github.io/assets/images/posts/citelang/ecosystem.png\" /></div><p>And I’m so excited because a tiny vision I had a few years ago to provide (and use) a community research software database is comingto live! So without further adeiu, I’m just going to jump into the cool results! It will be fun to see how these change over time.</p><h3 id=\"python\">Python</h3><p>Ladies and gents, dinosaurs and rabbits! Your Python results:</p><div style=\"padding: 20px;\">  <img src=\"https://vsoch.github.io/assets/images/posts/citelang/python-deps.png\" /></div><p>So here is the first awesome insight. Is anyone really surprised to see numpy as the number one library?The credit value here says that the average Python repository is attributing about 3% of credit to numpy, meaning it is a direct or indirect dependency. Let that sink in! Here is the irony - when is the last time you cited numpy? You probably haven’t, because you’ve cited somethingthat uses it. We don’t remember numpy despite the fact that it’s so core to everything that we do.</p><blockquote>  <p>The fact that the most widely used library is rarely cited is huge evidence for why a manual “write papers and cite DOIs” approach just won’t work for software.</p></blockquote><p>What else do we see in this list? Let me name a few things. First, we can’t be so terrible at remembering to look at or visualizethings because matplotlib is second. At least for research software, this is telling us that making plots or charts is important.The next (possibly surprising) result is that documentation and testing is at least represented, and this might be a biased samplebecause we include repositories that are peer reviewed (e.g., JoSS) and documentation and testing is necessary for that. Given this need for Python, sphinx and pytest come up as leaders to provide that. So here is another nugget of insight:</p><blockquote>  <p>Some of us are so  busy focusing on domain-specific software that we forget the importance of the “less sexy” research software that helps us test, document, view things, or even create simple data structures.</p></blockquote><p>This kind of “base” software has always been what I’ve been most interested in, and ironically what people tell me time and time again“That’s not research software.” Oh really? So something that is entirely powering the research community is not research software?Of course I have my own <a href=\"https://rseng.github.io/software/repository/github/0x0f0f0f/Metatheory.jl/annotate-taxonomy/\" target=\"_blank\">strong opinions</a> about a taxonomy for research software, but I would encourage those of you who are very dismissive to take a step back andconsider what you are really saying.</p><p>The next insight is that we see a lot of libraries for data formats (e.g., pyaml, h5py, lxml, and more lower in the list) and this is an attestment to how important being able to read, serialize, and save data is.</p><p>The final insight is the fact that requests is high in the list. For those of you not familiar, requests is a library for doing that, makinghttp requests to get content from some webby place. This is an attestment to the fact that our work is increasingly relying on external APIs,automation, or other resources provided on the web.</p><p>You can see <a href=\"https://rseng.github.io/rsepedia-analysis/analysis/python/\" target=\"_blank\">the full Python results here</a>.</p><h3 id=\"r\">R</h3><p>I’m less of an R programmer these days, but I think that these results also make sense.</p><div style=\"padding: 20px;\">  <img src=\"https://vsoch.github.io/assets/images/posts/citelang/r-deps.png\" /></div><p>We don’t see any huge leaders in the same way as we see numpy in Python, but not surprisingly the leader packagefor the R language is, well, R! I at first thought this was a bug, but actually R <code class=\"language-plaintext highlighter-rouge\">DESCRIPTION</code> files that we parse do commonly include a pinned version of R:</p><pre><code class=\"language-DESCRIPTION\">Depends: R (&gt;= 3.4.1), TailRank, ...</code></pre><p>And so we actually can give credit to the language proper! If you don’t feel this is appropriate, feel free to skip this line and considerthe top package jsonlite. This is also why I think json would be represented in Python if it wasn’t part of the standard library. Us research folks - we need our json! Overall I think we see a similar pattern here as we saw with Python. The libraries that float to the top are those that involve data structures (jsonlite, yaml), webby requests or similar (httr, curl), documentation and testing (knitr, rmarkdown) and graphics or visualization.  What does this tell us about what is undervalued in research software? Again, it’s not the domain specific libraries, but rather the core stuff that enables those libraries.</p><p>You can see <a href=\"https://rseng.github.io/rsepedia-analysis/analysis/R/\" target=\"_blank\">the full R results here</a>.</p><h3 id=\"projects\">Projects</h3><p>If you are interested in a specific project in the RSEPedia, we also provide a project-specific table and badge! You can <a href=\"https://rseng.github.io/rsepedia-analysis/analysis/repos/\" target=\"_blank\">browse projects from here</a>, and here is an example of a badge generated for a project called  <a href=\"https://rseng.github.io/rsepedia-analysis/repos/github/ORNL/tx2/README\" target=\"_blank\">github.com/ORNL/tx2</a> <a href=\"https://github.com/ORNL/tx2\" target=\"_blank\">(and on GitHub)</a>. Without even looking I can tell you we have some machine learning and/or visualization going on here (scikit-learn! umap! pandas! matplotlib)!</p><div style=\"padding: 20px;\">  <img src=\"https://vsoch.github.io/assets/images/posts/citelang/project.png\" /></div><p>Notice how numpy (as an example) shows up at multiple points in the tree - when we calculate an overall credit, say, for the ecosystem, we take that into account! And we can then peek at the project-specific table and sort of verify that yes, this is a Python ML/visualization project:</p><div style=\"padding: 20px;\">  <img src=\"https://vsoch.github.io/assets/images/posts/citelang/project-table.png\" /></div><p>And we see some surprises! Like, the slack-sdk? What? Believe it or not, that is pulled in by <a href=\"https://github.com/tqdm/tqdm/blob/4f208e72552c4d916aa4fe6a955349ee8b2ed353/setup.cfg#L87\" target=\"_blank\">tqdm</a>. The project-specific tables (and the description at the top) also give you a better sense of how CiteLang allocatescredit. The top level package is given 50%, and then the other 50% is given to all dependencies in the same fashion.We cut off at a value of 0.001, and we do that in case we might be parsing dependencies forever down to some infintesimally small amount.</p><p>Finally, every project serves its own <a href=\"https://rseng.github.io/rsepedia-analysis/repos/github/ORNL/tx2/data.json\" target=\"_blank\">raw data</a></p><div style=\"padding: 20px;\">  <img src=\"https://vsoch.github.io/assets/images/posts/citelang/json-data.png\" /></div><p>and the site is searchable, because sites should be. 😄️</p><h2 id=\"discussion\">Discussion</h2><p>I’m so happy (and a bit relieved, to be honest) to finally be able to show what I’ve been saying for years - that the most valuable software for research, and the software that is driving domain-specific research software, are the unsexy libraries that have to do with data structures, (maybe standards), documentation or testing, and data formats or retrieval. These are the packages that you aren’t going to remember to cite. Also, this set is totally leaving out the software we use on a day to day basis in our CI, which arguably isn’t research software but has done more for the research community than anything I can think of - containers, version control (git), and continuous integration. We’d be a mess without it. We need to be more thankful and aware of this, and for some of y’all that turn down your nose to anything that isn’t a domain-science library, perhaps take a pause. Next, let’s talk about limitations and hopes for the future.</p><h2 id=\"a-living-database\">A Living Database</h2><p>I wouldn’t have been happy with myself to simply publish software at one point in time and call it a day.The Research Software Encyclopedia is updated weekly, and so I’ve designed this analysis to do the same!This means that while we do cache a result for a newly added piece of software, we do continue to grow the analysis as new software is added. And since the tool will always use the newly updated <a href=\"https://github.com/vsoch/citelang\" target=\"_blank\">CiteLang</a>, any improvements to the parsers there will be reflected here! And if anyone wants to run the entire thing again (outside of the limit of GitHub Actions) they can clone the repository, nuke the _repos folder, and run the scripts again.</p><h3 id=\"language-gaps\">Language Gaps</h3><p>The biggest gap in the RSEPedia is with respect to what we don’t see. First, despite being a prominent language, we don’t see anything for C++, because there isn’t a package manager with an API to use it. If you have a nifty (or even hacky) idea for how to parse a requirements file, <a href=\"https://github.com/vsoch/citelang/issues\" target=\"_blank\">I want to hear it</a>. The RSEPedia has support for spack, but most research-oriented C++ projects are not going to go out of their way to publish their package there, and we get no signal of the package being in spack when we clone the repository. Sppaaaaaack (sorry, it’s a bit of a tic at this point!) 😁️</p><p>We also don’t see standard modules or libraries provided within a language. E.g., I can almost guarantee you a ton of Python libraries are importing json, but since it’s not a package manager library we wouldn’t see it. I suspect citelang could come up with a way to derive credit for these libraries by way of abstract syntax trees or just parsing the source code, although I haven’t done this yet because I’m not convinced it’s something people are as interested in. If you want to say thank you for the Python standard library, there is a <a href=\"https://www.python.org/psf/contrib/\" target=\"_blank\">donate button</a> on their contribution page (or you could contribute code). There is an even deeper level of parsing (at least for Python) that looks at function signatures, and I wrote a library called <a href=\"https://github.com/vsoch/caliper\" target=\"_blank\">caliper</a> in early 2021 to do that, and it’s able to generate <a href=\"https://raw.githubusercontent.com/vsoch/caliper-metrics/main/pypi/tensorflow/functiondb/functiondb-0.12.0rc1.json\" target=\"_blank\">function databases</a> for Python software of interest. This would be cool to do for some kind of (unrelated) compatibility analysis here, but yes that’s very different.</p><h3 id=\"parsing-limitation\">Parsing Limitation</h3><p>For all requirements files except for Python, we are forced to do static parsing. While not perfect because bugs can happen for niche cases of someone defining requirements in a weird way, it’s a reasonable start. There is always room for improvement, or adding more static parsers for requirements files I have not considered yet.</p><p>However, this is not the case for the Python parsing (either requirements.txt or setup.py)! For Python these results are likely very good because we wrap the pypi package manager install command to derive a list of packages and versions from either a setup.py or requirements.txt. Don’t worry - nothing is installed, we either just parse the requirements file and return the results, or we use the solveragainst a setup.py to come to an equivalent list. We originally had a static parser (and still use this as a fallback) however I talked to <a href=\"https://github.com/alecbcs\" target=\"_blank\">@alecbcs</a> and he had this fantastic idea! Will it likely need updates as time goes on, giventhe functions are private? Sure. But I’m happy to do that to get the much more accurate listing.</p><p>In practice, the only setup.py files that I was not able to parse either had a bug (e.g., trying to read a file that doesn’t exist in the repository) or they were trying to use modules outside of the standard library. For all of the cases of broken-ness, I opened issues on the respective repositories so we might have a better chance at parsing in the future! One detail is that we parse the first requirements file found. For a primary requirements file in the root of the repository, this is the best outcome. However, some repos don’t have a file in the root, and perhaps we find one in a documentation folder instead. Either way, the result represents our best effort at finding and parsing requirements given a cloned repository we don’t know the structure of in advance.</p><h3 id=\"final-thoughts\">Final Thoughts</h3><p>Here are my final takeaways:</p><h4 id=\"publication-is-not-for-research-software\">Publication is not for Research Software</h4><p>A system of credit that relies on software engineers to do extra manual work (to write papers) is never going to fully capture the ecosystem and give proper credit. It will only capture those that have the time and possibly privilege to take the extra time to write a paper.Publication only makes sense given that a piece of software is paired alongside a robust result, in which case fine, write the paper and also champion the software.</p><h4 id=\"publication-does-not-actually-capture-credit\">Publication Does not Actually Capture Credit</h4><p>A system that also only skims the superficial top (the name of one package) and does not dig deep into a dependency tree is also going to miss insights and deserved attributions of credit. As the numpy example shows, nobody is actually citing numpy, but a ton of projects are using it somewhere in their dependency tree, so it deserves a lot of credit.</p><h4 id=\"we-can-do-better\">We Can Do Better</h4><p>I have a pet peeve. I’m frankly just tired of people writing about credit and attribution but not doing anything about it. We could extend that to other things, but it’s especially an issue for this topic. Ironically they are writing <em>papers</em> and improving their publication record as they write about how publication and research software is a strained process. I may not have solved this problem, but damn at least I’m trying to actually do something about it instead of spurting gas.</p><p>I find this idea exciting because there are so many directions you can go with it. When I first designed the idea I imagined a database and online interface where you could essentially connect your GitHub repository, and akin to a builder service, parse your repository on some event and derive a new credit or citation graph. Or you could have some set akin to the RSEPedia that are also updated regularly. And then, by way of having that database, we could do these same queries (that currently I’m doing statically) to say “What are the most important libraries for this language? Across the ecosystem?” or “How has this changed over time?” It would be a true way to derive the value of a library without needing people to publish papers, and totally automated and integrated with package managers, which is where people already should be putting their software.Heck, if someone gave me a cloud and a little bit of funding I’d love to work on this. Are there good reasons or use cases? I don’t know, but maybe.</p><p>So what do you think?</p>",
            "url": "https://hpc.social/personal-blog/2022/the-research-software-ecosystem/",
            
            
            
            
            
            "date_published": "2022-04-24T13:30:00-06:00",
            "date_modified": "2022-04-24T13:30:00-06:00",
            
                "author": "Vanessasaurus"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/spooky-allocator-issues-and-fixes/",
            "title": "Spooky Allocator Issues and Fixes",
            "summary": null,
            "content_text": "Recently we started noticing performance issues in the main branch of Ceph that ultimately were traced back to a commit last summer that changed parts of our AVL and hybrid disk allocator implementations in bluestore.  Strangly, the issue only affected some of the NVMe drives in our test lab but not others.  The quick fix was to always update and save the allocator’s cursor position so that we don’t search (and fail) over and over in fast-fit mode for every allocation request.  Another interesting offshoot of this though is that it may be much nicer to limit fast-fit searches based on time rather than byte distance or the number of iterations.",
            "content_html": "<p>Recently we started noticing performance issues in the main branch of Ceph that ultimately were traced back to a commit last summer that changed parts of our AVL and hybrid disk allocator implementations in bluestore.  Strangly, the issue only affected some of the NVMe drives in our test lab but not others.  The quick <a href=\"https://github.com/ceph/ceph/pull/45884\">fix</a> was to always update and save the allocator’s cursor position so that we don’t search (and fail) over and over in fast-fit mode for every allocation request.  Another interesting offshoot of this though is that it may be much <a href=\"https://github.com/ceph/ceph/pull/45771\">nicer</a> to limit fast-fit searches based on time rather than byte distance or the number of iterations.</p>",
            "url": "https://hpc.social/personal-blog/2022/spooky-allocator-issues-and-fixes/",
            
            
            
            
            
            "date_published": "2022-04-13T01:00:00-06:00",
            "date_modified": "2022-04-13T01:00:00-06:00",
            
                "author": "Mark Nelson's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/an-unstructured-rant-on-running-long-lived-software-services/",
            "title": "An unstructured rant on running long-lived software services",
            "summary": null,
            "content_text": "&#8211; Be kind to your colleagues. Be kind to your users. Be kind to yourself. This is a long haul and you’ll all fuck up.⁃ The natural environment for your code is production. It will run there longer than it does anywhere else. Design for prod first, and if possible, make your dev environment act like prod.⁃ Legacy code is the only code worth caring about.⁃ Users do weird stuff, but they usually have a very good reason, at least in their context. Learn from them.⁃ It’s 2022, please do structured logging.⁃ Contexts and tracing make everyone&#8217;s lives easier when it comes time to debug. At minimum, include a unique request id with every request and plumb it through the system.⁃ Do your logging in a separate thread. It sucks to find a daemon blocked and hanging because of a full disk or a down syslog server.⁃ Don’t page for individual machines going down. Do provide an easy or automated way for bad nodes to get thrown out of the system.&#8211; Be prepared for your automation to be the problem, and include circuit breakers or kill switches to stop it. I&#8217;ve seen health checks that started flagging every machine in the fleet as bad, whether it was healthy or not. We didn&#8217;t bring down prod because the code assumed if it flagged more than 15% of the fleet as bad, the problem was probably with the test, not the service.⁃ Make sure you have a way to know who your users are. If you allow anonymous access, you&#8217;ll discover in five years that a business-critical team you&#8217;ve never heard of is relying on you.⁃ Make sure you have a way to turn off access for an individual machine, user, etc. If your system does anything more expensive than sending network requests, it will be possible for a single bad client to overwhelm a distributed system with thousands of servers. Turning off their access is easier than begging them to stop.⁃ If you don’t implement QOS early on, it will be hellish to add it later, and you will certainly need it if your system lasts long enough.⁃ If you provide a client library, and your system is internal only, have it send logs to the same system as your servers. This will help trace issues back to misbehaving clients so much.⁃ Track the build time for every deployed server binary and monitor how old they are. If your CI process deploys daily, week-old binaries are a problem. Month-old binaries are a major incident.⁃ If you can get away with it (internal services): track the age of client library builds and either refuse to support builds older than X, or just cut them off entirely. It sucks to support requests from year-old clients, force them to upgrade!⁃ Despite all this, you will at some point start getting requests from an ancient software version, or otherwise malformed. Make sure these requests don’t break anything.⁃ Backups are a pain, and the tooling is often bad, but I swear they will save you one day. Take the time to invest in them.⁃ Your CI process should exercise your turnup process, your decommission process, and your backups workflow. Life will suck later if you discover one of these is broken.⁃ Third party services go down. Your service goes down too, but they probably won’t happen at the same time. Be prepared to either operate without them, or mirror them yourself⁃ Your users will never, ever care if you’re down because of a dependency. Every datacenter owned by AWS could be hit by a meteor at the same time, but your user will only ever ask “why doesn’t my service work?”⁃ Have good human relationships with your software dependencies. Know the people who develop them, keep in touch with them, make sure you understand each other. This is especially true internally but also important with external deps. In the end, software is made of people.⁃ If users don’t have personal buy-in to the security policy, they will find ways to work around them and complain about you for making their lives harder. Take the time to educate them, or you&#8217;ll be fighting them continuously.",
            "content_html": "<p>&#8211; Be kind to your colleagues. Be kind to your users. Be kind to yourself. This is a long haul and you’ll all fuck up.</p><p>⁃ The natural environment for your code is production. It will run there longer than it does anywhere else. Design for prod first, and if possible, make your dev environment act like prod.</p><p>⁃ Legacy code is the only code worth caring about.</p><p>⁃ Users do weird stuff, but they usually have a very good reason, at least in their context. Learn from them.</p><p>⁃ It’s 2022, <em>please</em> do structured logging.</p><p>⁃ Contexts and tracing make everyone&#8217;s lives easier when it comes time to debug. At minimum, include a unique request id with every request and plumb it through the system.</p><p>⁃ Do your logging in a separate thread. It sucks to find a daemon blocked and hanging because of a full disk or a down syslog server.</p><p>⁃ Don’t page for individual machines going down. Do provide an easy or automated way for bad nodes to get thrown out of the system.</p><p>&#8211; Be prepared for your automation to be the problem, and include circuit breakers or kill switches to stop it. I&#8217;ve seen health checks that started flagging every machine in the fleet as bad, whether it was healthy or not. We didn&#8217;t bring down prod because the code assumed if it flagged more than 15% of the fleet as bad, the problem was probably with the test, not the service.</p><p>⁃ Make sure you have a way to know who your users are. If you allow anonymous access, you&#8217;ll discover in five years that a business-critical team you&#8217;ve never heard of is relying on you.</p><p>⁃ Make sure you have a way to turn off access for an individual machine, user, etc. If your system does anything more expensive than sending network requests, it will be possible for a single bad client to overwhelm a distributed system with thousands of servers. Turning off their access is easier than begging them to stop.</p><p>⁃ If you don’t implement QOS early on, it will be hellish to add it later, and you will certainly need it if your system lasts long enough.</p><p>⁃ If you provide a client library, and your system is internal only, have it send logs to the same system as your servers. This will help trace issues back to misbehaving clients so much.</p><p>⁃ Track the build time for every deployed server binary and monitor how old they are. If your CI process deploys daily, week-old binaries are a problem. Month-old binaries are a major incident.</p><p>⁃ If you can get away with it (internal services): track the age of client library builds and either refuse to support builds older than X, or just cut them off entirely. It sucks to support requests from year-old clients, force them to upgrade!</p><p>⁃ Despite all this, you will at some point start getting requests from an ancient software version, or otherwise malformed. Make sure these requests don’t break anything.</p><p>⁃ Backups are a pain, and the tooling is often bad, but I swear they will save you one day. Take the time to invest in them.</p><p>⁃ Your CI process should exercise your turnup process, your decommission process, and your backups workflow. Life will suck later if you discover one of these is broken.</p><p>⁃ Third party services go down. Your service goes down too, but they probably won’t happen at the same time. Be prepared to either operate without them, or mirror them yourself</p><p>⁃ Your users will never, ever care if you’re down because of a dependency. Every datacenter owned by AWS could be hit by a meteor at the same time, but <em>your</em> user will only ever ask “why doesn’t my service work?”</p><p>⁃ Have good human relationships with your software dependencies. Know the people who develop them, keep in touch with them, make sure you understand each other. This is especially true internally but also important with external deps. In the end, software is made of people.</p><p>⁃ If users don’t have personal buy-in to the security policy, they <em>will</em> find ways to work around them and complain about you for making their lives harder. Take the time to educate them, or you&#8217;ll be fighting them continuously.</p>",
            "url": "https://hpc.social/personal-blog/2022/an-unstructured-rant-on-running-long-lived-software-services/",
            
            
            
            
            
            "date_published": "2022-03-12T16:00:00-07:00",
            "date_modified": "2022-03-12T16:00:00-07:00",
            
                "author": "Thinking Out Loud"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/what-i-ve-learned-from-looking-at-1-500-jobs-leading-research-computing-teams/",
            "title": "What I've Learned from Looking at 1,500 Jobs Leading Research Computing Teams",
            "summary": null,
            "content_text": "Job numbers continue to grow; lots of data and product management jobs; IR groups at Universities becoming bigger employers(Note: This post is adapted from #111 of the Research Computing Teams Newsletter)A year and a half ago I posted my observations on the first 500 jobs posted to the job board - we’re getting close to 1,500 now, and it’s worth taking a look to see what if anything has changed in research computing team leadership and management jobs1.There are some trends that have continued since the posting.  The jobs in industry are growing vastly beyond what I would have imagined possible when I started in research computing in the 1990s.  (The number of jobs working with biomedical data of one sort or another in particular is just astonishing.)  Rather than technical computing being a niche, it’s utterly mainstream now.  There are a lot of jobs out there, and I don’t even bother posting generic “data science manager” jobs unless they’re connected to some real complex research questions - which happens a lot, whether it’s fraud detection or improving financial modelling or supporting biomedical research.  Some really fun-looking jobs that would probably feel a lot like working at a research computing centre keep coming up at consultancies –– go visit a client and help them with their data science/data engineering/etc needs.  There’s also a growing number of data science/engineering jobs at Universities that fall under the Provost/VP Operations rather than the VPR’s side of the house — Institutional Research, looking at (say) student success in support of the teaching mission.Because of the growth in number of jobs, it is very much a candidate’s market out there.  I’m seeing postings –– especially for the traditional academic “director of research computing” jobs –— stay open for cringe-inducing periods of time.  A few in particular I’ve watched with vicarious embarrassment continue coming up in the listings for 8+ months.  That’s a bad sign for us as hiring managers - the market for individual contributors is at least as tight - but it’s amazing news for us as individuals.When I wrote that post in late 2020 it was just regulated industries like health/biotech or financial services that were developing data governance or other data management jobs, but now data management is popping up everywhere, whether it’s retail or logistics or anywhere else.   These are being joined, again first in the regulated industries, by data privacy or data risk management jobs.  Privacy-preserving data analysis jobs (and teams supporting same with software development) are also starting to be more common (and there’s a lot of cool research and technology work to be done there!)I’m also (finally!) starting to see a explicitly product management jobs in research computing, both academic and private-sector.  You see it around data management — bundling and curating of data into real data products — but also in software development, especially around analysis pipelines for some reason.Probably related to the growth of product vs project thinking, I’m starting to see a lot of “delivery manager” jobs that would have been called “project managers” just a year ago.   Projects are defined by having clearly defined start- and end-points up-front.  “Delivery” jobs seem to focus on sustained, ongoing work, more appropriate for long-lived products.These products that keep coming up often combine data, software, and systems one way or another.  That really points to weaknesses around organizing by type of skills - the research software engineering movement, for instance - as the lines between software and systems in this DevOps, infrastructure-as-code era is very fuzzy; and as data grows more and more important, data skills are needed everywhere.Especially for us as managers or leads, but especially for individual contributors as they grow their skills, it’s important to have a pretty holistic view of research computing and data and not try to break it up into silos.  The growing number of data engineering jobs is a great example.  That work often involves all three of software, systems, and data expertise.   Data engineering is getting so broad and important that not only are there different sub-fields, in large organizations there are likely to be completely distinct data engineering teams doing different work.  Trying to decide which of those jobs are “research software engineering” jobs and which aren’t is not a productive way forward, for those candidates or for us as a community.Needless to say, the growth of remote jobs has been off the charts - especially in the private sector, although the academic institutions are gamely doing what they can to keep up (often hampered by institutional policies).Late June 2022 update: At the time that I write this, there’s a slow down in hiring in tech, especially among early stage-startups.  That slowdown due to economic conditions as I write this is not, as far as I can tell, affecting these more research-oriented kinds of jobs.  The job board doesn’t have a lot of jobs from startups anyway.  For larger organizations, the biotech firms or the banking firms doing fraud detection research or the computing providers or academic groups or…  clearly do not view these roles as “nice to haves” that can wait until there’s a bit more economic certainty.            What counts as such a job?  Any job that involves leading, or mentoring people, or managing projects, programs, or products, in software, systems, or data curation/management/engineering/analysis to support the solution of research problems is a good fit.  If you are hiring for such a job, feel free to submit it to the job board. &#8617;      ",
            "content_html": "<h2 id=\"job-numbers-continue-to-grow-lots-of-data-and-product-management-jobs-ir-groups-at-universities-becoming-bigger-employers\">Job numbers continue to grow; lots of data and product management jobs; IR groups at Universities becoming bigger employers</h2><p>(Note: This post is adapted from <a href=\"https://www.researchcomputingteams.org/newsletter_issues/0111\">#111</a> of the <a href=\"https://www.researchcomputingteams.org\">Research Computing Teams Newsletter</a>)</p><p>A year and a half ago I <a href=\"https://www.dursi.ca/post/jobs_managing_research_computing_teams\">posted</a> my observations on the first 500 jobs posted to <a href=\"https://www.researchcomputingteams.org/jobs\">the job board</a> - we’re getting close to 1,500 now, and it’s worth taking a look to see what if anything has changed in research computing team leadership and management jobs<sup id=\"fnref:1\"><a class=\"footnote\" href=\"https://www.dursi.ca/feed.xml#fn:1\" rel=\"footnote\">1</a></sup>.</p><p>There are some trends that have continued since the posting.  The jobs in industry are growing vastly beyond what I would have imagined possible when I started in research computing in the 1990s.  (The number of jobs working with biomedical data of one sort or another in particular is just astonishing.)  Rather than technical computing being a niche, it’s utterly mainstream now.  There are a <em>lot</em> of jobs out there, and I don’t even bother posting generic “data science manager” jobs unless they’re connected to some real complex research questions - which happens a lot, whether it’s fraud detection or improving financial modelling or supporting biomedical research.  Some really fun-looking jobs that would probably feel a lot like working at a research computing centre keep coming up at consultancies –– go visit a client and help them with their data science/data engineering/<em>etc</em> needs.  There’s also a growing number of data science/engineering jobs at Universities that fall under the Provost/VP Operations rather than the VPR’s side of the house — Institutional Research, looking at (say) student success in support of the teaching mission.</p><p>Because of the growth in number of jobs, it is very much a candidate’s market out there.  I’m seeing postings –– <em>especially</em> for the traditional academic “director of research computing” jobs –— stay open for cringe-inducing periods of time.  A few in particular I’ve watched with vicarious embarrassment continue coming up in the listings for 8+ months.  That’s a bad sign for us as hiring managers - the market for individual contributors is at least as tight - but it’s amazing news for us as individuals.</p><p>When I wrote that post in late 2020 it was just regulated industries like health/biotech or financial services that were developing data governance or other data management jobs, but now data management is popping up everywhere, whether it’s retail or logistics or anywhere else.   These are being joined, again first in the regulated industries, by data privacy or data risk management jobs.  Privacy-preserving data analysis jobs (and teams supporting same with software development) are also starting to be more common (and there’s a <em>lot</em> of cool research and technology work to be done there!)</p><p>I’m also (finally!) starting to see a explicitly <em>product</em> management jobs in research computing, both academic and private-sector.  You see it around data management — bundling and curating of data into real data products — but also in software development, especially around analysis pipelines for some reason.</p><p>Probably related to the growth of product <em>vs</em> project thinking, I’m starting to see a lot of “delivery manager” jobs that would have been called “project managers” just a year ago.   Projects are defined by having clearly defined start- and end-points up-front.  “Delivery” jobs seem to focus on sustained, ongoing work, more appropriate for long-lived products.</p><p>These products that keep coming up often combine data, software, and systems one way or another.  That really points to weaknesses around organizing by type of skills - the research software engineering movement, for instance - as the lines between software and systems in this DevOps, infrastructure-as-code era is very fuzzy; and as data grows more and more important, data skills are needed everywhere.</p><p>Especially for us as managers or leads, but especially for individual contributors as they grow their skills, it’s important to have a pretty holistic view of research computing and data and not try to break it up into silos.  The growing number of data engineering jobs is a great example.  That work often involves all three of software, systems, and data expertise.   Data engineering is getting so broad and important that not only are there different sub-fields, in large organizations there are likely to be <a href=\"https://medium.com/data-arena/team-topologies-for-data-engineering-teams-a15c5eb3849c\">completely distinct data engineering teams</a> doing different work.  Trying to decide which of those jobs are “research software engineering” jobs and which aren’t is not a productive way forward, for those candidates or for us as a community.</p><p>Needless to say, the growth of remote jobs has been off the charts - especially in the private sector, although the academic institutions are gamely doing what they can to keep up (often hampered by institutional policies).</p><p><strong>Late June 2022 update</strong>: At the time that I write this, there’s a slow down in hiring in tech, especially among early stage-startups.  That slowdown due to economic conditions as I write this is <em>not</em>, as far as I can tell, affecting these more research-oriented kinds of jobs.  The job board doesn’t have a lot of jobs from startups anyway.  For larger organizations, the biotech firms or the banking firms doing fraud detection research or the computing providers or academic groups or…  clearly do not view these roles as “nice to haves” that can wait until there’s a bit more economic certainty.</p><hr /><div class=\"footnotes\">  <ol>    <li id=\"fn:1\">      <p>What counts as such a job?  Any job that involves leading, or mentoring people, or managing projects, programs, or products, in software, systems, or data curation/management/engineering/analysis to support the solution of research problems is a good fit.  If you are hiring for such a job, feel free to <a href=\"https://airtable.com/shrL6QGic3Mv9JFrs\">submit it to the job board</a>. <a class=\"reversefootnote\" href=\"https://www.dursi.ca/feed.xml#fnref:1\">&#8617;</a></p>    </li>  </ol></div>",
            "url": "https://hpc.social/personal-blog/2022/what-i-ve-learned-from-looking-at-1-500-jobs-leading-research-computing-teams/",
            
            
            
            
            
            "date_published": "2022-02-26T00:00:00-07:00",
            "date_modified": "2022-02-26T00:00:00-07:00",
            
                "author": "Jonathan Dursi's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/a-supportive-job-interview-story/",
            "title": "A supportive job interview story",
            "summary": null,
            "content_text": "(adapted from an old lobste.rs comment)My favorite interview ever was a systems interview that didn’t go as planned. This was for an SRE position, and while I expected the interview to be a distributed systems discussion, the interviewer instead wanted to talk kernel internals.I was not at all prepared for this, and admitted it up front. The interviewer said something along the lines of, “well, why don’t we see how it goes anyway?”He then proceeded to teach me a ton about how filesystem drivers work in Linux, in the form of leading me carefully through the interview question he was “asking” me. The interviewer was incredibly encouraging throughout, and we had a good discussion about why certain design decisions worked the way they did.I ended the interview (a) convinced I had bombed it, but (b) having had an excellent time anyway and having learned a bunch of new things. I later learned the interviewer had recommended to hire me based on how our conversation had gone, though I didn’t end up taking the job for unrelated reasons having to do with relocation.I’ve given a number of similar interviews since, on system design or general sysadmin skills. I’ve always tried to go into these thinking about both where I could learn, and where I could teach, and how either outcome would give the candidate a chance to shine.",
            "content_html": "<p>(adapted from an <a href=\"https://lobste.rs/s/1bwpi8/have_you_ever_had_given_really_good#c_1r7cs6\">old lobste.rs comment</a>)</p><p>My favorite interview ever was a systems interview that didn’t go as planned. This was for an SRE position, and while I expected the interview to be a distributed systems discussion, the interviewer instead wanted to talk kernel internals.</p><p>I was not <em>at all</em> prepared for this, and admitted it up front. The interviewer said something along the lines of, “well, why don’t we see how it goes anyway?”</p><p>He then proceeded to teach me a <em>ton</em> about how filesystem drivers work in Linux, in the form of leading me carefully through the interview question he was “asking” me. The interviewer was incredibly encouraging throughout, and we had a good discussion about why certain design decisions worked the way they did.</p><p>I ended the interview (a) convinced I had bombed it, but (b) having had an excellent time anyway and having learned a bunch of new things. I later learned the interviewer had recommended to hire me based on how our conversation had gone, though I didn’t end up taking the job for unrelated reasons having to do with relocation.</p><p>I’ve given a number of similar interviews since, on system design or general sysadmin skills. I’ve always tried to go into these thinking about both where I could learn, and where I could teach, and how either outcome would give the candidate a chance to shine.</p>",
            "url": "https://hpc.social/personal-blog/2022/a-supportive-job-interview-story/",
            
            
            
            
            
            "date_published": "2022-02-25T16:00:00-07:00",
            "date_modified": "2022-02-25T16:00:00-07:00",
            
                "author": "Thinking Out Loud"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/interactive-development-containers/",
            "title": "Interactive Development Containers",
            "summary": null,
            "content_text": "I’ve recently been interested in developer workflows. Aside from being a developer, I feellike the tooling for our community, and especially for HPC or hybrid environments, is lacking.As a simple example, let’s ask a basic question:  How do I start developing here and move it over there?For the most part, creating a development container is fairly straight forward, and we can even bind sourcecode to the host to work on in one editor terminal and then build and run or test in another. However,for the moving part, it gets shoddy. Our best bet is to rebuild the container with the most updated source code, push to a registry, and then pull down somewhere else.For a container that is a binary and not layers provided by a registry, we could even scp it.If we do this right, we will have an automated build and deploy that triggers when we merge new code into main, but do you see the problem? What about the code that we wantto test that isn’t ready to merge? This is why we typically would need to manuallypush to a registry with some kind of “work in progress” tag and then pull somewhere else.Minimally we’d need to build fresh again, and then reproduce all the steps to set up our environment.Interactive Development ContainersNow I don’t have all the answers, but recently @alecbcs andI have been dreaming about what kinds of development environments we want.functionality such as:  Saving the container state without leaving it.  Loading or saving or otherwise interacting with named environments.  Inspecting or interacting with container metadata, also without leaving the container.  Moving files or sizing the container without the same.And actually I won’t even get to answering the first question in this post about moving somethingfrom one place to another, but rest assured it is an important one. This post is about some prototype or fun testing work that we’ve started around these ideas.The playground for some of these early ideas has been Paks. Paks is a Python library that I’m calling a developer wrapper for containers.Mind you, it’s more of a playground right now to experiment with ideas. But I’ve had somuch fun even this early on that I want to share what I’ve learned.WrapperBecause Paks is a wrapper, you will run containers using the paks command. Here are a few quick examples.$ paks run ubuntu$ paks run --shell /bin/sh busybox$ paks run --container-tech podman busyboxWhat is happening on the backend that took me a bit to figure out is that we will need to run a subprocess,but create a pseudo terminal to betterwatch and interact with it. This is going to happen in the “interactive_terminal” command below. But unless youwant your terminal to get wonky, we need to use termios to grab the current tty and make sure it gets restored no matter what at the end. That looks like this:    def interactive_command(self, cmd):        \"\"\"        Ensure we always restore original TTY otherwise terminal gets messed up        \"\"\"        # Controller to get history        self.hist = self.commands.history        # save original tty setting then set it to raw mode        old_tty = termios.tcgetattr(sys.stdin)        old_pty = termios.tcgetattr(sys.stdout)        try:            self._interactive_command(cmd)        finally:            termios.tcsetattr(sys.stdin, termios.TCSADRAIN, old_tty)            termios.tcsetattr(sys.stdout, termios.TCSADRAIN, old_pty)What happens if you don’t do that? Your terminal gets weird and wonky. And then in the interactivecommand function, this is where we launch a subprocess with a new pseudo terminal:        tty.setraw(sys.stdin.fileno())        # open pseudo-terminal to interact with subprocess        openpty, opentty = pty.openpty()        # use os.setsid() make it run in a new process group, or bash job control will not be enabled        p = subprocess.Popen(            cmd,            preexec_fn=os.setsid,            stdin=opentty,            stdout=opentty,            stderr=opentty,            universal_newlines=True,        )        # Welcome to Paks!        self.welcome(openpty)The setsid as a pre-exec function is ensuring the child process is a new session and won’t exit, sort of akin to a daemon. So at face value, yes it is doing exactly what you think - we are shelling into the containerand watching the command line and looking for paks-known commands. And I didn’t use a Python keylogger becauseI found that keyboard requires sudo (like really?!) and pynput is really scary because it doesn’t just get keys from the terminal - it’s watching anything you type anywhere! That gave me the heebie jeebies. I hope there is some scanner for pypi that is looking for that packageand checking it’s not being malicious.All of the above said, and all the time spent, I’m not convinced that this exact method isthe best way to be running commands from inside the container. There are other ideasthat need to be tested!StructureWe could have talked about this first, but let me show you the basic structure of paksso you get an understanding of the components.paks# Backends are different wrappers, so logically we start with podman and docker├── backends│   ├── base.py│   ├── docker.py│   ├── __init__.py│   └── podman.py# The client is what you interact with on the command line. This shows the various commands available.├── cli│   ├── config.py│   ├── env.py│   ├── __init__.py│   └── run.py# This is a central controller for things├── client.py# Here's all the built-in, interactive commands paks supports!├── commands│   ├── command.py│   ├── cp.py│   ├── env.py│   ├── history.py│   ├── __init__.py│   ├── inspect.py│   └── state.py├── defaults.py├── env.py├── logger.py# Coming soon - load your own commands!├── plugins.py├── schemas.py├── settings.py├── settings.yml├── templates.py├── utils└── version.pySo that should give you the gist - we have container wrappers (backends) and thencommands that we can issue while we are inside the container. Let’s talk about them next.Saving StateThe first thing I wanted to try with Paks was to save a container state, but not needingto open a separate terminal and save from the outside. The use case for this is that given I’m in an interactivecontainer and I’ve made some changes, I don’t want to exit and rebuild. All y’all reproducibility folkscan stop wincing, and realize that we also need more temporary or throwaway development environments like this.Reproducibilty is important, but mostly for the final production thing, and only up to a level of notgiving us pain. So how might I do this?For paks, while you are inside the container (let’s say ubuntu) you simply ask to #save:$ paks run ubuntu# touch PANCAKES# #saveSaving container...sha256:d82aaa268feb59344cf31a757ce7f5c0caa6a6bbd10b8d0af1d55cdbc50b609b[+] Building 0.2s (5/5) FINISHED...=&gt; =&gt; writing image sha256:f58ae524d8644400b33c078f19612cba7849ef8f3ea158e2291ac697a4129080=&gt; =&gt; naming to docker.io/library/busybox-savedUntagged: dockerio-busybox-joyous-hippo-3922-gloopy-peanut-9044:latestDeleted: sha256:d82aaa268feb59344cf31a757ce7f5c0caa6a6bbd10b8d0af1d55cdbc50b609bDeleted: sha256:f58ae524d8644400b33c078f19612cba7849ef8f3ea158e2291ac697a4129080Successfully saved container! ⭐️And then you can see that there is an ubuntu-saved container!$ docker images | grep ubuntuubuntu-saved                                      latest    93e336d994de   2 minutes ago   72.8MBubuntu                                            latest    54c9d81cbb44   7 days ago      72.8MBSo this has saved me some tiny bit of energy to open up another terminal, remember how to docker commit,and then also rebuild with a squash to minimize the layers (as there is a maximum number we don’t want to hit).What Paks could then eventually do is make it easy to move this entire container betweenplaces, e.g., from your local machine to HPC without a hitch. I haven’t started to work on that yetbecause this is a fun side project.EnvironmentsOne thing I do a lot is use GitHub tokens to do fun stuff with the API. I usually need tokeep this in some hidden file, then find it, open it, copy paste it, and export it in the container.And then I do that a million times when I have to run a new container. But with Paks, we can create a named environment on the host (a file to source with exports):$ paks env edit githubYou can also quickly show an environment:$ paks env show githubGITHUB_TOKEN=xxxxxxxxxxxAnd then in our container, as many times as we need, load it seamlessly!root@9ec6c3d43591:/# #envload githubLoading environment...Successfully loaded environment githubroot@9ec6c3d43591:/#  export GITHUB_TOKEN=xxxxxxxxxroot@9ec6c3d43591:/#  export GITHUB_USER=dinosaurIf only my GitHub username was dinosaur! 😁️ Is it loaded?root@9ec6c3d43591:/# env | grep GITHUBGITHUB_USER=dinosaurGITHUB_TOKEN=xxxxxxxxxOkay, so to be fair, there are a bunch of other commands for inspection and size,and I’m not going to go through them all! You can see them in the Paks user guide.And I don’t mean to say you should use this - you probably shouldn’t. But you might be interested to try it out.Parsing KeystrokesSo the most interesting part of this project has been learning about input from the terminal,and actually the reason I wanted to write this post to share what I learned. Let’s go back to the interactivefunction where we ran subprocess and created a pseudo terminal. There actually is a pretty simple wayto watch what is being typed:# This is the subprocess return code, keep going until we are done (e.g. have a return code)while p.poll() is None:    # Wait for io completion (e.g., see man select)    r, w, e = select.select([sys.stdin, openpty], [], [])    # Was it a new input?    if sys.stdin in r:        terminal_input = os.read(sys.stdin.fileno(), 10240)        new_char = terminal_input.decode(\"utf-8\")        # Do something with what you see here    # Was it a new output?    elif openpty in r:        o = os.read(openpty, 10240)        if o:            os.write(sys.stdout.fileno(), o)I learned a lot from this! Let’s talk about it.DebuggingSo the first thing I learned is that my typical “import IPython” and “IPython.embed()”isn’t going to work as easily as normal, because (at least superficially) I didn’tsee a way to have it sort of injected into the process. Anything that is interactive inthat loop is still (conceptually) running on my host. So when I use IPythonit does some weird stuff with carriage returns, but it’s still possible to interact witha little bit. So what I wound up doing so I could easily see every keypress was to writeto file in append mode:with open('/tmp/file.txt', 'a') as fd:    fd.write(new_char)This was kind of neat because I could be typing in one terminal, and then havea file open (watching it) that updates with changes, and I’d get a sense of whatis going on. I could append anything to this file to debug. And this is also reallydifferent from how we normally use subprocess, where maybe we will parse entire linesat once:p = subprocess.Popen(['python','thing.py'], stdout=subprocess.PIPE)while True:  line = p.stdout.readline()  if not line:    breakbecause we are reading on character at a time! So what we essentially need to dois keep a string that we continue appending to unless there is a newline, up or down,or left or right to indicate moving the cursor.Ascii CharactersI started to quickly see characters that my editor didn’t know - e.g., likelyescape sequences and other ascii that showed up in the little question mark box.I quickly realized that I was seeing asciicode (and some characters that couldn’t be parsed) so the solution was to look at the ordof the character and compare to a number. For example, for a backspacethe number is 127. So to act on it I might do:# if we have a backspace (ord 127)if len(new_char) == 1 and ord(new_char) == 127:    # This is our in progress line. If we have content, backspace!    if len(string_input) &gt; 0:        string_input = string_input[:-1]        # But if we don't, just write the character for the person to see and     # keep collecting new characters (continue in the loop)    if not string_input:        os.write(openpty, terminal_input)        continue    # Otherwise (not a backspace) add to our growing line to parse further!else:    string_input = string_input + new_charThe above is basically looking for a backspace, and if we find one, we removeone character from the line we are assembling. Otherwise we just add the new characterto the line.xterm sequencesAnd a similar thing happens for pressing up/down and right/left, except theterminal parses them as “[A”, “[B”, “[C”, and “[D”, respectively, and often withan escape sequence first. There are some nice tables herefor the interested reader! And this was also the point that I realized how challenging parsing input is!Along with needing to account for every character, you also need to account for platformdifferences. That’s also why I view this library as mostly for development and thinking,or at least for mostly Linux and bash shells, because I’m not sure I could ever handle them all.So for the purposes of my library, for now I decided I’m not going to handle moving left and right,nor do I want to deal with weird extra ascii characters that are added, so I just clean them up.# Get rid of left/rightstring_input = string_input.replace(\"[D\", \"\").replace(\"[C\", \"\")# Replace weird characters and escape sequencesstring_input = self.clean(string_input)Yes, that probably means some of your ninja shortcuts won’t work perfectly when running paks,and if you absolutely want one to be parsed please let me know and we can add it.NewlinesSo the gold nugget of content that Paks is interested in is when you press enter.This means you’ve finished typing something and there is some version of a newlineor carriage return. This is also a pretty variable thing depending on the platform you areon - newlines can come in very different forms! I tried to honor the two that I see most often:  \\r\\n: Windows   \\n: UNIX (e.g., Mac OSX)  \\r: Mac (pre OSX)has_newline = \"\\n\" in string_input or \"\\r\" in string_inputAt this point, we can start acting on what we see. E.g., if the user has asked for anykind of exit, I honor it.# Universal exit commandif \"exit\" in string_input and has_newline:    print(\"\\n\\rContainer exited.\\n\\r\")    return self.uri.extended_nameThe return of the name at the end is to handle cleaning up the image, which was allocateda temporary name.HistoryOne of the more interesting parts of this project was realizing that people use history, a lot.At least I do. This is going to appear as an up or down press, and only when a newline is found is some item in history re-executed. So first let’s look for exploring history with up/down. There aretwo cases - pressing up/down without a newline:# Pressing up or down, but not enterif (\"[A\" in string_input or \"[B\" in string_input) and not has_newline:    string_input = self.get_history(string_input, openpty)    os.write(openpty, terminal_input)    continueAnd with one:# Pressing up or down with enterif (\"[A\" in string_input or \"[B\" in string_input) and has_newline:    string_input = self.get_history(string_input, openpty)    os.write(openpty, terminal_input)If we don’t have a newline, we add a continue to keep parsing characters the user istyping. If we do have a newline, we let the loop keep running to keep parsing the line of history we retrieved.But let’s step back and talk about that history. We basically want to retrieve whatever line of history thatthe user is asking for, because to us it looks like up and down errors. You could imaginerestoring the previous line, and then editing it. This actually proved to be quite challenging,because I realized (by default) when we start running a container (well, ubuntu and centos)the history is stored in memory and not written to ~/.bash_history. This led to this thread and some people coming in to quickly helpand others coming in just to say “Why are you doing this with containers it makes no sense stop.” Yeah, right. If Ilistened to every person that has ever told me to stop working on something because “REASONS!” I wouldn’tultimately work on much at all.The short answer was that I needed a function to be able to get a line of history, and based on the number of times pressing up or down. For my first attempt I said “nevermind this, I’ll just save my own history!”but that got hugely complicated very fast because it turns out, we don’t just stupidly type commands over and over,we are constantly using more characters on the keyboard than letters and numbers, retrieving old things to edit,updating again, and in practice I found that I could keep up with simple parsing, but it would get out of syncfor a longer session. There also is the issue that people can tweak the amount of history saved, or how it’s saved, and there are a set of environment variables and commandsto do that. So most containers will start running and save history to memory and not file (and this makessense in case there is sensitive information) but it was problematic for me because I couldn’t parse it.For example, when someone presses up and down a bunch of times, I might see:[A[A[A[A[A[B[AThis is a reference to some previous command that I can only find in historygiven I’m parsing the input/output as I am. So my second attempt (well, maybe second throughtenth) I was trying different variations of trying to be able to parse the history.If you looked at the tweetyou’ll see we need to run:$ history -ato start writing what’s in memory to file. I didn’t want to do this on every command, because alongwith the user seeing it and the UI being awful, it was just too much. Instead, I realized that I had a smallopportunity when the user first shells into the container (and is expecting a jump in their UI) to run whateverI need and then clear the terminal. So I ran it there, right before a clear and welcome message.    def welcome(self, openpty):        \"\"\"        Welcome the user and clear terminal        \"\"\"        # Don't add commands executed to history        os.write(openpty, self.encode(\" export PROMPT_COMMAND='history -a'\\r\"))        os.write(openpty, self.encode(\" clear\\r\"))        os.write(openpty, self.encode(\" ### Welcome to PAKS! ###\\r\"))And with this method you aren’t aware of the extra commands at all! And did you notice the spaces above? That’s also another trick! Any command that you type with a leadingspace won’t be saved to history, and this is thanks to HISTCONTROL that has an ignorespace option. I think most people / containersset it to ignore space and to ignore duplicates:root@1c268386714a:/# echo $HISTCONTROLignoredups:ignorespaceThat said, I don’t explicitly try to reset this in the container, so that could be a bugif there is a container base that doesn’t do that. And I’m pretty sure centos doesn’t come with clear!I’ll likely need to work on this a bit more.  For now, please consider this only working for debian/ubuntu bases and we can inspect the other ones later!Okay, so now let’s look at the function to get history (self.hist.run). For now, just ignore the command toget the history, that’s actually done via a Paks command that we will talk about after.Here is what is going on:def get_history(self, line, openpty):    \"\"\"    Given an input with some number of up/down and newline, derive command.    \"\"\"    # Calculate the absolute change of ups/downs    up = line.count(\"[A\")    down = line.count(\"[B\")    change = up - down    # pushed down below history (maybe they are angry?)    if change &lt;= 0:       return \"\"    # Retrieve history, actually via a command run from the outside to get the file    history = self.hist.run(        container_name=self.uri.extended_name,        out=openpty,        history_file=self.settings.history_file,        user=self.settings.user,    )    history = [x for x in history.split(\"\\n\") if x]    # No history, nothing to return    if not history:        return \"\"    # The change is outside the length of history    if change &gt; len(history):        return \"\"    # here we are looking back up into history (negative index)    newline = history[-1 * change]    # Add back any characters typed AFTER the up/down presses    newline += re.split(\"(\\[A|\\[B)\", line, 1)[-1]    return newlineThe above might not be perfect, but it worked the best for everything that I tried!This allows us to issue a command that paks knows, press up to get it again, and then editit and have the command work correctly. Speaking of commands…CommandsThe core meat of paks is the commands that it recognizes. Every command has a base classthat is going to handle parsing a line (with a main command and optional args or kwargs, depending on the command),ensuring all required variables are passed (this is largely internal to the library and even a developer userdoesn’t need to think about it unless they want to change what is passed), and then providing functions for basic kinds ofexecution. So let’s step back and first look at how we find a command (or executor). Basically, once we have a newlineand we’ve parsed it per the above (looking up history and such) we can sniff it to see if it matches a knowncommand pattern:# If we have a newline (and possibly a command)if has_newline:    self.run_executor(string_input, openpty)    # Add derived line to the history    os.write(openpty, terminal_input)    string_input = \"\"The function “run_executor” is going to make this call if there is a Paks command and handle it.And no matter what, we reset our string input to be empty given that the user pressed enter, becausethey are going to start typing fresh. But before that, this function “run_executor” is going to seeif there are any known commands, and if so, to run them! That function looks like this:def run_executor(self, string_input, openpty):    \"\"\"    Given a string input, run executor    \"\"\"    # Get out early if it's not a Paks command (always starts with #)    string_input = string_input.replace(\"[A\", \"\").replace(\"[B\", \"\")    if not string_input.startswith(\"#\"):        return    # Do we have a matching executor?    executor = self.commands.get_executor(string_input, out=openpty)    if executor is not None:        # Print any message it wants to the terminal before run...        if executor.pre_message:            print(\"\\n\\r\" + executor.pre_message)        # Run it!        result = executor.run(            name=self.image,            container_name=self.uri.extended_name,            original=string_input,        )        # And any message it wants to print after        if result.message:            print(\"\\r\" + result.message)The result object holds what you would expect - a return code, some message,and the basic outputs of the call. It’s up to the executor (command) to decidewhat to show the user. Some might not show anything beyond commands that are runwith the executor. So what does that function “get_executor” look like?This is where we delive into the commands module, where there is a simple lookup ofthe starting prefixes of commands matched to Command classes:# lookup of named commands and settingsdocker_commands = {    \"#save\": SaveContainer,    \"#inspect\": InspectContainer,    \"#envload\": EnvLoad,    \"#envhost\": EnvHost,    \"#envsave\": EnvSave,    \"#cp\": Copy,    \"#size\": Size,}When I add a load functionality, all it will need to do is update this dictionary.And the reason those are “docker commands” is that you can imagine we eventuallysupport other container technologies, and the commands you run are going to vary.Each Command actually has a class attribute for the container types that are supported.Here is a snippet of the DockerCommands class attached to the client that we are calling “get_executor” on:class DockerCommands:    # Required kwargs for any docker/podman command to run    required = [\"container_name\", \"name\"]    def __init__(self, container_tech):        self.command = container_tech        self.lookup = docker_commands    def parse_name(self, cmd):        parts = cmd.split(\" \")        return parts.pop(0).replace(\"\\n\", \"\").replace(\"\\r\", \"\").strip()    def has_command(self, name):        name, _ = self.parse_name(name)        return name in self.lookup    @property    def history(self):        return History(self.command)    def get_executor(self, name, out=None):        \"\"\"        Backend is required to update history        \"\"\"        name = self.parse_name(name)        if name in self.lookup:            return self.lookup[name](self.command, required=self.required, out=out)To focus on the last function, you basically see that we parse the line (name), and thensee if it’s in our lookup. If so, we return the initialized executor, and we need to addthe output source in case it needs to interact with the current terminal. The self.commandrefers to the container technology (e.g., docker or podman in this case).Then we can look at a particular command (e.g., inspect) and see it’s pretty simple! We have definedthe supported container technologies along with optional messages, and a main run function. Here is the commandto inspect, which will dump out the json manifest and optionally take a section:class InspectContainer(Command):    supported_for = [\"docker\", \"podman\"]    pre_message = \"Inspecting Container...\"    def run(self, **kwargs):        \"\"\"        Inspect a container fully, or specific sections        \"\"\"        # Always run this first to make sure container tech is valid        self.check(**kwargs)        # These are both required for docker/podman        container_name = self.kwargs[\"container_name\"]        # inspect particular attributes provided as args        if self.args:            for section in self.args:                result = self.run_command(                    [                        self.tech,                        \"inspect\",                        \"--format\",                        \"\" % section.capitalize(),                        container_name,                    ]                )        # Otherwise just dump the whole thing        else:            result = self.run_command([self.tech, \"inspect\", container_name])            if result:                return result        return self.return_success()You’ll now know the main Paks trick - because we are still running on the host,we can issue commands to the host while we are in the container! In the above, we can just type:#inspect#inspect configAnd see the output in the terminal! This is how a lot of the interactions with the host work.It’s kind of simple and silly, but also really cool when you see it work on the container!So the run function above, just as a reminder, is called by this part:result = executor.run(    name=self.image,    container_name=self.uri.extended_name,    original=string_input,)And honestly, that’s the majority of Paks! 🎉️DiscussionPaks has  honestly been so fun to work on, despite long hours of trying to figure things out during evenings and weekends. I’m so excitedabout the ideas, and I want to share them with others because I think developer tools for containersare kind of lacking. Heck, I stayed up until like 4am writing this post. No, I don’t normally do that,I had some things on my mind, but it was an excellent use of the time, despite the fact that I woke up 4 hours later andI’m going to crash tonight (err tomorrow night… err now that I’m tweaking up the finishing touches to this post)!Next StepsI’m working on a “paks load” command that will let someone develop a Python modulewith some set of commands for their custom use case. The first thing I wanted to trywas to generate sboms for spack (e.g., “Generate sboms for this spack install in the containerand save them to my host so I can upload alongside the container to a registry). I hadsome previous work to use spack scripting, but ultimately this weekend did a pull requestto add sbom generation to spack proper. And then I’ll be able to work on the load commands.I also want to address some of the anticipated bugs I mentioned above, like properly setting “HISTCONTROL”to ensure we don’t save commands issued by the client to history, and possibly having a cleanup step on savethat removes the file. I haven’t added this yet is because if I’m developing in the container and want to say, move it from my local machine to HPC, I kind of want to have my history so I can lazily use it.But Really…We have some magic up our sleeves for what we are actually working on to inspire these ideas!I guess you’ll just have to wait for the future, because @alecbcs andI are both have vision and are a great tag team! 🎉️SecuritySo there are obviously security issues around a library like this - and I added notesto the documentation that I’ll re-iterate here. Paks is intended for use by a developerthat is in their own trusted environment, whether local or on HPC. Because there is an interactionwith the host, you wouldn’t use this in production someone to give users an ability to loadenvironments or save. You also wouldn’t want to save a development container with somethingprivate in history and push it. I’m still an advocate for, after development is done,pushing changed code to GitHub and having an automated build build, test, and deploy.Could we eventually have a production grade library to enable interactions inside thecontainer? Possibly, but it’s not Paks in Python in its current state. I thinkthat’s okay - we have to start small with ideas and go from there.Didn’t I see paks before?Yes, you did! A previous version was intended for making spack build caches on GitHub, but thatdidn’t work because you couldn’t build a spack package within a container and thenpull the same container and install it and hit the cache. I think this might work someday,hence why I haven’t completely deleted the code, but I couldn’t let a cute logo and colorscheme go to waste!So for now it’s on a separate branch but largely I am not working on it. If you want to see this branch,it’s still here!Thanks for reading friends! I hope this has been interesting and you might be inspired toalso work on better tooling for developers, even if that just means exploring the ideas.",
            "content_html": "<p>I’ve recently been interested in developer workflows. Aside from being a developer, I feellike the tooling for our community, and especially for HPC or hybrid environments, is lacking.As a simple example, let’s ask a basic question:</p><blockquote>  <p>How do I start developing here and move it over there?</p></blockquote><p>For the most part, creating a development container is fairly straight forward, and we can even bind sourcecode to the host to work on in one editor terminal and then build and run or test in another. However,for the moving part, it gets shoddy. Our best bet is to rebuild the container with the most updated source code, push to a registry, and then pull down somewhere else.For a container that is a binary and not layers provided by a registry, we could even scp it.If we do this right, we will have an automated build and deploy that triggers when we merge new code into main, but do you see the problem? What about the code that we wantto test that isn’t ready to merge? This is why we typically would need to manuallypush to a registry with some kind of “work in progress” tag and then pull somewhere else.Minimally we’d need to build fresh again, and then reproduce all the steps to set up our environment.</p><h2 id=\"interactive-development-containers\">Interactive Development Containers</h2><p>Now I don’t have all the answers, but recently <a href=\"https://github.com/alecbcs\" target=\"_blank\">@alecbcs</a> andI have been dreaming about what kinds of development environments we want.functionality such as:</p><ol class=\"custom-counter\">  <li>Saving the container state without leaving it.</li>  <li>Loading or saving or otherwise interacting with named environments.</li>  <li>Inspecting or interacting with container metadata, also without leaving the container.</li>  <li>Moving files or sizing the container without the same.</li></ol><p>And actually I won’t even get to answering the first question in this post about moving somethingfrom one place to another, but rest assured it is an important one. This post is about some prototype or fun testing work that we’ve started around these ideas.The playground for some of these early ideas has been <a href=\"https://syspack.github.io/paks/\" target=\"_blank\">Paks</a>.</p><div style=\"padding: 20px;\"> <img src=\"https://github.com/syspack/paks/raw/main/docs/assets/img/paks.png\" /></div><p>Paks is a Python library that I’m calling a developer wrapper for containers.Mind you, it’s more of a playground right now to experiment with ideas. But I’ve had somuch fun even this early on that I want to share what I’ve learned.</p><h3 id=\"wrapper\">Wrapper</h3><p>Because Paks is a wrapper, you will run containers using the paks command. Here are a few quick examples.</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>paks run ubuntu<span class=\"nv\">$ </span>paks run <span class=\"nt\">--shell</span> /bin/sh busybox<span class=\"nv\">$ </span>paks run <span class=\"nt\">--container-tech</span> podman busybox</code></pre></div></div><p>What is happening on the backend that took me a bit to figure out is that we will need to run a subprocess,but create a <a href=\"https://docs.python.org/3/library/pty.html\" target=\"_blank\">pseudo terminal</a> to betterwatch and interact with it. This is going to happen in the “interactive_terminal” command below. But unless youwant your terminal to get wonky, we need to use <a href=\"https://docs.python.org/3/library/termios.html\" target=\"_blank\">termios</a> to grab the current tty and make sure it gets restored no matter what at the end. That looks like this:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>    <span class=\"k\">def</span> <span class=\"nf\">interactive_command</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">cmd</span><span class=\"p\">):</span>        <span class=\"s\">\"\"\"        Ensure we always restore original TTY otherwise terminal gets messed up        \"\"\"</span>        <span class=\"c1\"># Controller to get history</span>        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">hist</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">commands</span><span class=\"p\">.</span><span class=\"n\">history</span>        <span class=\"c1\"># save original tty setting then set it to raw mode</span>        <span class=\"n\">old_tty</span> <span class=\"o\">=</span> <span class=\"n\">termios</span><span class=\"p\">.</span><span class=\"n\">tcgetattr</span><span class=\"p\">(</span><span class=\"n\">sys</span><span class=\"p\">.</span><span class=\"n\">stdin</span><span class=\"p\">)</span>        <span class=\"n\">old_pty</span> <span class=\"o\">=</span> <span class=\"n\">termios</span><span class=\"p\">.</span><span class=\"n\">tcgetattr</span><span class=\"p\">(</span><span class=\"n\">sys</span><span class=\"p\">.</span><span class=\"n\">stdout</span><span class=\"p\">)</span>        <span class=\"k\">try</span><span class=\"p\">:</span>            <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">_interactive_command</span><span class=\"p\">(</span><span class=\"n\">cmd</span><span class=\"p\">)</span>        <span class=\"k\">finally</span><span class=\"p\">:</span>            <span class=\"n\">termios</span><span class=\"p\">.</span><span class=\"n\">tcsetattr</span><span class=\"p\">(</span><span class=\"n\">sys</span><span class=\"p\">.</span><span class=\"n\">stdin</span><span class=\"p\">,</span> <span class=\"n\">termios</span><span class=\"p\">.</span><span class=\"n\">TCSADRAIN</span><span class=\"p\">,</span> <span class=\"n\">old_tty</span><span class=\"p\">)</span>            <span class=\"n\">termios</span><span class=\"p\">.</span><span class=\"n\">tcsetattr</span><span class=\"p\">(</span><span class=\"n\">sys</span><span class=\"p\">.</span><span class=\"n\">stdout</span><span class=\"p\">,</span> <span class=\"n\">termios</span><span class=\"p\">.</span><span class=\"n\">TCSADRAIN</span><span class=\"p\">,</span> <span class=\"n\">old_pty</span><span class=\"p\">)</span></code></pre></div></div><p>What happens if you don’t do that? Your terminal gets weird and wonky. And then in the interactivecommand function, this is where we launch a subprocess with a new pseudo terminal:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>        <span class=\"n\">tty</span><span class=\"p\">.</span><span class=\"n\">setraw</span><span class=\"p\">(</span><span class=\"n\">sys</span><span class=\"p\">.</span><span class=\"n\">stdin</span><span class=\"p\">.</span><span class=\"n\">fileno</span><span class=\"p\">())</span>        <span class=\"c1\"># open pseudo-terminal to interact with subprocess</span>        <span class=\"n\">openpty</span><span class=\"p\">,</span> <span class=\"n\">opentty</span> <span class=\"o\">=</span> <span class=\"n\">pty</span><span class=\"p\">.</span><span class=\"n\">openpty</span><span class=\"p\">()</span>        <span class=\"c1\"># use os.setsid() make it run in a new process group, or bash job control will not be enabled</span>        <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">subprocess</span><span class=\"p\">.</span><span class=\"n\">Popen</span><span class=\"p\">(</span>            <span class=\"n\">cmd</span><span class=\"p\">,</span>            <span class=\"n\">preexec_fn</span><span class=\"o\">=</span><span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">setsid</span><span class=\"p\">,</span>            <span class=\"n\">stdin</span><span class=\"o\">=</span><span class=\"n\">opentty</span><span class=\"p\">,</span>            <span class=\"n\">stdout</span><span class=\"o\">=</span><span class=\"n\">opentty</span><span class=\"p\">,</span>            <span class=\"n\">stderr</span><span class=\"o\">=</span><span class=\"n\">opentty</span><span class=\"p\">,</span>            <span class=\"n\">universal_newlines</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span>        <span class=\"p\">)</span>        <span class=\"c1\"># Welcome to Paks!</span>        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">welcome</span><span class=\"p\">(</span><span class=\"n\">openpty</span><span class=\"p\">)</span></code></pre></div></div><p>The <a href=\"https://stackoverflow.com/questions/45911705/why-use-os-setsid-in-python\" target=\"_blank\">setsid</a> as a pre-exec function is ensuring the child process is a new session and won’t exit, sort of akin to a daemon. So at face value, yes it is doing exactly what you think - we are shelling into the containerand watching the command line and looking for paks-known commands. And I didn’t use a Python keylogger becauseI found that <a href=\"https://github.com/boppreh/keyboard\" target=\"_blank\">keyboard</a> requires sudo (like really?!) and <a href=\"https://pynput.readthedocs.io/en/latest/\" target=\"_blank\">pynput</a> is really scary because it doesn’t just get keys from the terminal - it’s watching anything you type anywhere! That gave me the heebie jeebies. I hope there is some scanner for pypi that is looking for that packageand checking it’s not being malicious.</p><p>All of the above said, and all the time spent, I’m not convinced that this exact method isthe best way to be running commands from inside the container. There are other ideasthat need to be tested!</p><h3 id=\"structure\">Structure</h3><p>We could have talked about this first, but let me show you the basic structure of paksso you get an understanding of the components.</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>paks# Backends are different wrappers, so logically we start with podman and docker├── backends│   ├── base.py│   ├── docker.py│   ├── __init__.py│   └── podman.py# The client is what you interact with on the command line. This shows the various commands available.├── cli│   ├── config.py│   ├── env.py│   ├── __init__.py│   └── run.py# This is a central controller for things├── client.py# Here's all the built-in, interactive commands paks supports!├── commands│   ├── command.py│   ├── cp.py│   ├── env.py│   ├── history.py│   ├── __init__.py│   ├── inspect.py│   └── state.py├── defaults.py├── env.py├── logger.py# Coming soon - load your own commands!├── plugins.py├── schemas.py├── settings.py├── settings.yml├── templates.py├── utils└── version.py</code></pre></div></div><p>So that should give you the gist - we have container wrappers (backends) and thencommands that we can issue while we are inside the container. Let’s talk about them next.</p><h3 id=\"saving-state\">Saving State</h3><p>The first thing I wanted to try with Paks was to save a container state, but not needingto open a separate terminal and save from the outside. The use case for this is that given I’m in an interactivecontainer and I’ve made some changes, I don’t want to exit and rebuild. All y’all reproducibility folkscan stop wincing, and realize that we also need more temporary or throwaway development environments like this.Reproducibilty is important, but mostly for the final production thing, and only up to a level of notgiving us pain. So how might I do this?</p><p>For paks, while you are inside the container (let’s say ubuntu) you simply ask to <code class=\"language-plaintext highlighter-rouge\">#save</code>:</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>$ paks run ubuntu# touch PANCAKES# #saveSaving container...sha256:d82aaa268feb59344cf31a757ce7f5c0caa6a6bbd10b8d0af1d55cdbc50b609b[+] Building 0.2s (5/5) FINISHED...=&gt; =&gt; writing image sha256:f58ae524d8644400b33c078f19612cba7849ef8f3ea158e2291ac697a4129080=&gt; =&gt; naming to docker.io/library/busybox-savedUntagged: dockerio-busybox-joyous-hippo-3922-gloopy-peanut-9044:latestDeleted: sha256:d82aaa268feb59344cf31a757ce7f5c0caa6a6bbd10b8d0af1d55cdbc50b609bDeleted: sha256:f58ae524d8644400b33c078f19612cba7849ef8f3ea158e2291ac697a4129080Successfully saved container! ⭐️</code></pre></div></div><p>And then you can see that there is an ubuntu-saved container!</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>docker images | <span class=\"nb\">grep </span>ubuntuubuntu-saved                                      latest    93e336d994de   2 minutes ago   72.8MBubuntu                                            latest    54c9d81cbb44   7 days ago      72.8MB</code></pre></div></div><p>So this has saved me some tiny bit of energy to open up another terminal, remember how to docker commit,and then also rebuild with a squash to minimize the layers (as there is a maximum number we don’t want to hit).What Paks could then eventually do is make it easy to move this entire container betweenplaces, e.g., from your local machine to HPC without a hitch. I haven’t started to work on that yetbecause this is a fun side project.</p><h3 id=\"environments\">Environments</h3><p>One thing I do a lot is use GitHub tokens to do fun stuff with the API. I usually need tokeep this in some hidden file, then find it, open it, copy paste it, and export it in the container.And then I do that a million times when I have to run a new container. But with Paks, we can create a named environment on the host (a file to source with exports):</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>paks <span class=\"nb\">env </span>edit githubYou can also quickly show an environment:<span class=\"nv\">$ </span>paks <span class=\"nb\">env </span>show github<span class=\"nv\">GITHUB_TOKEN</span><span class=\"o\">=</span>xxxxxxxxxxx</code></pre></div></div><p>And then in our container, as many times as we need, load it seamlessly!</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>root@9ec6c3d43591:/# <span class=\"c\">#envload github</span>Loading environment...Successfully loaded environment githubroot@9ec6c3d43591:/#  <span class=\"nb\">export </span><span class=\"nv\">GITHUB_TOKEN</span><span class=\"o\">=</span>xxxxxxxxxroot@9ec6c3d43591:/#  <span class=\"nb\">export </span><span class=\"nv\">GITHUB_USER</span><span class=\"o\">=</span>dinosaur</code></pre></div></div><p>If only my GitHub username was dinosaur! 😁️ Is it loaded?</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>root@9ec6c3d43591:/# <span class=\"nb\">env</span> | <span class=\"nb\">grep </span>GITHUB<span class=\"nv\">GITHUB_USER</span><span class=\"o\">=</span>dinosaur<span class=\"nv\">GITHUB_TOKEN</span><span class=\"o\">=</span>xxxxxxxxx</code></pre></div></div><p>Okay, so to be fair, there are a bunch of other commands for inspection and size,and I’m not going to go through them all! You can see them <a href=\"https://syspack.github.io/paks/getting_started/user-guide.html\" target=\"_blank\">in the Paks user guide</a>.And I don’t mean to say you should use this - you probably shouldn’t. But you might be interested to try it out.</p><h3 id=\"parsing-keystrokes\">Parsing Keystrokes</h3><p>So the most interesting part of this project has been learning about input from the terminal,and actually the reason I wanted to write this post to share what I learned. Let’s go back to the interactivefunction where we ran subprocess and created a pseudo terminal. There actually is a pretty simple wayto watch what is being typed:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># This is the subprocess return code, keep going until we are done (e.g. have a return code)</span><span class=\"k\">while</span> <span class=\"n\">p</span><span class=\"p\">.</span><span class=\"n\">poll</span><span class=\"p\">()</span> <span class=\"ow\">is</span> <span class=\"bp\">None</span><span class=\"p\">:</span>    <span class=\"c1\"># Wait for io completion (e.g., see man select)</span>    <span class=\"n\">r</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">e</span> <span class=\"o\">=</span> <span class=\"n\">select</span><span class=\"p\">.</span><span class=\"n\">select</span><span class=\"p\">([</span><span class=\"n\">sys</span><span class=\"p\">.</span><span class=\"n\">stdin</span><span class=\"p\">,</span> <span class=\"n\">openpty</span><span class=\"p\">],</span> <span class=\"p\">[],</span> <span class=\"p\">[])</span>    <span class=\"c1\"># Was it a new input?</span>    <span class=\"k\">if</span> <span class=\"n\">sys</span><span class=\"p\">.</span><span class=\"n\">stdin</span> <span class=\"ow\">in</span> <span class=\"n\">r</span><span class=\"p\">:</span>        <span class=\"n\">terminal_input</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">read</span><span class=\"p\">(</span><span class=\"n\">sys</span><span class=\"p\">.</span><span class=\"n\">stdin</span><span class=\"p\">.</span><span class=\"n\">fileno</span><span class=\"p\">(),</span> <span class=\"mi\">10240</span><span class=\"p\">)</span>        <span class=\"n\">new_char</span> <span class=\"o\">=</span> <span class=\"n\">terminal_input</span><span class=\"p\">.</span><span class=\"n\">decode</span><span class=\"p\">(</span><span class=\"s\">\"utf-8\"</span><span class=\"p\">)</span>        <span class=\"c1\"># Do something with what you see here</span>    <span class=\"c1\"># Was it a new output?</span>    <span class=\"k\">elif</span> <span class=\"n\">openpty</span> <span class=\"ow\">in</span> <span class=\"n\">r</span><span class=\"p\">:</span>        <span class=\"n\">o</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">read</span><span class=\"p\">(</span><span class=\"n\">openpty</span><span class=\"p\">,</span> <span class=\"mi\">10240</span><span class=\"p\">)</span>        <span class=\"k\">if</span> <span class=\"n\">o</span><span class=\"p\">:</span>            <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">sys</span><span class=\"p\">.</span><span class=\"n\">stdout</span><span class=\"p\">.</span><span class=\"n\">fileno</span><span class=\"p\">(),</span> <span class=\"n\">o</span><span class=\"p\">)</span></code></pre></div></div><p>I learned a lot from this! Let’s talk about it.</p><h4 id=\"debugging\">Debugging</h4><p>So the first thing I learned is that my typical “import IPython” and “IPython.embed()”isn’t going to work as easily as normal, because (at least superficially) I didn’tsee a way to have it sort of injected into the process. Anything that is interactive inthat loop is still (conceptually) running on my host. So when I use IPythonit does some weird stuff with carriage returns, but it’s still possible to interact witha little bit. So what I wound up doing so I could easily see every keypress was to writeto file in append mode:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s\">'/tmp/file.txt'</span><span class=\"p\">,</span> <span class=\"s\">'a'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">fd</span><span class=\"p\">:</span>    <span class=\"n\">fd</span><span class=\"p\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">new_char</span><span class=\"p\">)</span></code></pre></div></div><p>This was kind of neat because I could be typing in one terminal, and then havea file open (watching it) that updates with changes, and I’d get a sense of whatis going on. I could append anything to this file to debug. And this is also reallydifferent from how we normally use subprocess, where maybe we will parse entire linesat once:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">subprocess</span><span class=\"p\">.</span><span class=\"n\">Popen</span><span class=\"p\">([</span><span class=\"s\">'python'</span><span class=\"p\">,</span><span class=\"s\">'thing.py'</span><span class=\"p\">],</span> <span class=\"n\">stdout</span><span class=\"o\">=</span><span class=\"n\">subprocess</span><span class=\"p\">.</span><span class=\"n\">PIPE</span><span class=\"p\">)</span><span class=\"k\">while</span> <span class=\"bp\">True</span><span class=\"p\">:</span>  <span class=\"n\">line</span> <span class=\"o\">=</span> <span class=\"n\">p</span><span class=\"p\">.</span><span class=\"n\">stdout</span><span class=\"p\">.</span><span class=\"n\">readline</span><span class=\"p\">()</span>  <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">line</span><span class=\"p\">:</span>    <span class=\"k\">break</span></code></pre></div></div><p>because we are reading on character at a time! So what we essentially need to dois keep a string that we continue appending to unless there is a newline, up or down,or left or right to indicate moving the cursor.</p><h4 id=\"ascii-characters\">Ascii Characters</h4><p>I started to quickly see characters that my editor didn’t know - e.g., likelyescape sequences and other ascii that showed up in the little question mark box.I quickly realized that I was seeing <a href=\"https://www.w3resource.com/python-exercises/python-basic-exercise-86.php\" target=\"_blank\">ascii</a>code (and some characters that couldn’t be parsed) so the solution was to look at the ordof the character and compare to a number. For example, for a backspacethe number is 127. So to act on it I might do:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># if we have a backspace (ord 127)</span><span class=\"k\">if</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">new_char</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"mi\">1</span> <span class=\"ow\">and</span> <span class=\"nb\">ord</span><span class=\"p\">(</span><span class=\"n\">new_char</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"mi\">127</span><span class=\"p\">:</span>    <span class=\"c1\"># This is our in progress line. If we have content, backspace!</span>    <span class=\"k\">if</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">string_input</span><span class=\"p\">)</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span><span class=\"p\">:</span>        <span class=\"n\">string_input</span> <span class=\"o\">=</span> <span class=\"n\">string_input</span><span class=\"p\">[:</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span>        <span class=\"c1\"># But if we don't, just write the character for the person to see and </span>    <span class=\"c1\"># keep collecting new characters (continue in the loop)</span>    <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">string_input</span><span class=\"p\">:</span>        <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">openpty</span><span class=\"p\">,</span> <span class=\"n\">terminal_input</span><span class=\"p\">)</span>        <span class=\"k\">continue</span>    <span class=\"c1\"># Otherwise (not a backspace) add to our growing line to parse further!</span><span class=\"k\">else</span><span class=\"p\">:</span>    <span class=\"n\">string_input</span> <span class=\"o\">=</span> <span class=\"n\">string_input</span> <span class=\"o\">+</span> <span class=\"n\">new_char</span></code></pre></div></div><p>The above is basically looking for a backspace, and if we find one, we removeone character from the line we are assembling. Otherwise we just add the new characterto the line.</p><h4 id=\"xterm-sequences\">xterm sequences</h4><p>And a similar thing happens for pressing up/down and right/left, except theterminal parses them as “[A”, “[B”, “[C”, and “[D”, respectively, and often withan escape sequence first. There are <a href=\"https://en.wikipedia.org/wiki/ANSI_escape_code\" target=\"_blank\">some nice tables here</a>for the interested reader! And this was also the point that I realized how challenging parsing input is!Along with needing to account for every character, you also need to account for platformdifferences. That’s also why I view this library as mostly for development and thinking,or at least for mostly Linux and bash shells, because I’m not sure I could ever handle them all.So for the purposes of my library, for now I decided I’m not going to handle moving left and right,nor do I want to deal with weird extra ascii characters that are added, so I just clean them up.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># Get rid of left/right</span><span class=\"n\">string_input</span> <span class=\"o\">=</span> <span class=\"n\">string_input</span><span class=\"p\">.</span><span class=\"n\">replace</span><span class=\"p\">(</span><span class=\"s\">\"[D\"</span><span class=\"p\">,</span> <span class=\"s\">\"\"</span><span class=\"p\">).</span><span class=\"n\">replace</span><span class=\"p\">(</span><span class=\"s\">\"[C\"</span><span class=\"p\">,</span> <span class=\"s\">\"\"</span><span class=\"p\">)</span><span class=\"c1\"># Replace weird characters and escape sequences</span><span class=\"n\">string_input</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">clean</span><span class=\"p\">(</span><span class=\"n\">string_input</span><span class=\"p\">)</span></code></pre></div></div><p>Yes, that probably means some of your ninja shortcuts won’t work perfectly when running paks,and if you absolutely want one to be parsed please let me know and we can add it.</p><h4 id=\"newlines\">Newlines</h4><p>So the gold nugget of content that Paks is interested in is when you press enter.This means you’ve finished typing something and there is some version of a newlineor carriage return. This is also a pretty variable thing depending on the platform you areon - newlines can come in very different forms! I tried to honor the two that I see most often:</p><ol class=\"custom-counter\">  <li><strong>\\r\\n</strong>: Windows </li>  <li><strong>\\n</strong>: UNIX (e.g., Mac OSX)</li>  <li><strong>\\r</strong>: Mac (pre OSX)</li></ol><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">has_newline</span> <span class=\"o\">=</span> <span class=\"s\">\"</span><span class=\"se\">\\n</span><span class=\"s\">\"</span> <span class=\"ow\">in</span> <span class=\"n\">string_input</span> <span class=\"ow\">or</span> <span class=\"s\">\"</span><span class=\"se\">\\r</span><span class=\"s\">\"</span> <span class=\"ow\">in</span> <span class=\"n\">string_input</span></code></pre></div></div><p>At this point, we can start acting on what we see. E.g., if the user has asked for anykind of exit, I honor it.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># Universal exit command</span><span class=\"k\">if</span> <span class=\"s\">\"exit\"</span> <span class=\"ow\">in</span> <span class=\"n\">string_input</span> <span class=\"ow\">and</span> <span class=\"n\">has_newline</span><span class=\"p\">:</span>    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"</span><span class=\"se\">\\n\\r</span><span class=\"s\">Container exited.</span><span class=\"se\">\\n\\r</span><span class=\"s\">\"</span><span class=\"p\">)</span>    <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">uri</span><span class=\"p\">.</span><span class=\"n\">extended_name</span></code></pre></div></div><p>The return of the name at the end is to handle cleaning up the image, which was allocateda temporary name.</p><h3 id=\"history\">History</h3><p>One of the more interesting parts of this project was realizing that people use history, a lot.At least I do. This is going to appear as an up or down press, and only when a newline is found is some item in history re-executed. So first let’s look for exploring history with up/down. There aretwo cases - pressing up/down without a newline:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># Pressing up or down, but not enter</span><span class=\"k\">if</span> <span class=\"p\">(</span><span class=\"s\">\"[A\"</span> <span class=\"ow\">in</span> <span class=\"n\">string_input</span> <span class=\"ow\">or</span> <span class=\"s\">\"[B\"</span> <span class=\"ow\">in</span> <span class=\"n\">string_input</span><span class=\"p\">)</span> <span class=\"ow\">and</span> <span class=\"ow\">not</span> <span class=\"n\">has_newline</span><span class=\"p\">:</span>    <span class=\"n\">string_input</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">get_history</span><span class=\"p\">(</span><span class=\"n\">string_input</span><span class=\"p\">,</span> <span class=\"n\">openpty</span><span class=\"p\">)</span>    <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">openpty</span><span class=\"p\">,</span> <span class=\"n\">terminal_input</span><span class=\"p\">)</span>    <span class=\"k\">continue</span></code></pre></div></div><p>And with one:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># Pressing up or down with enter</span><span class=\"k\">if</span> <span class=\"p\">(</span><span class=\"s\">\"[A\"</span> <span class=\"ow\">in</span> <span class=\"n\">string_input</span> <span class=\"ow\">or</span> <span class=\"s\">\"[B\"</span> <span class=\"ow\">in</span> <span class=\"n\">string_input</span><span class=\"p\">)</span> <span class=\"ow\">and</span> <span class=\"n\">has_newline</span><span class=\"p\">:</span>    <span class=\"n\">string_input</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">get_history</span><span class=\"p\">(</span><span class=\"n\">string_input</span><span class=\"p\">,</span> <span class=\"n\">openpty</span><span class=\"p\">)</span>    <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">openpty</span><span class=\"p\">,</span> <span class=\"n\">terminal_input</span><span class=\"p\">)</span></code></pre></div></div><p>If we don’t have a newline, we add a continue to keep parsing characters the user istyping. If we do have a newline, we let the loop keep running to keep parsing the line of history we retrieved.But let’s step back and talk about that history. We basically want to retrieve whatever line of history thatthe user is asking for, because to us it looks like up and down errors. You could imaginerestoring the previous line, and then editing it. This actually proved to be quite challenging,because I realized (by default) when we start running a container (well, ubuntu and centos)the history is stored in memory and not written to ~/.bash_history. This led to <a href=\"https://twitter.com/vsoch/status/1492377777684639748\" target=\"_blank\">this thread</a> and some people coming in to <a href=\"https://twitter.com/ajdecon/status/1492381132998033409\" target=\"_blank\">quickly help</a>and others coming in just to say “Why are you doing this with containers it makes no sense stop.” Yeah, right. If Ilistened to every person that has ever told me to stop working on something because “REASONS!” I wouldn’tultimately work on much at all.</p><p>The short answer was that I needed a function to be able to get a line of history, and based on the number of times pressing up or down. For my first attempt I said “nevermind this, I’ll just save my own history!”but that got hugely complicated very fast because it turns out, we don’t just stupidly type commands over and over,we are constantly using more characters on the keyboard than letters and numbers, retrieving old things to edit,updating again, and in practice I found that I could keep up with simple parsing, but it would get out of syncfor a longer session. There also is the issue that people can tweak the amount of history saved, or how it’s saved, and there are a set of environment <a href=\"https://www.redhat.com/sysadmin/history-command\" target=\"_blank\">variables and commands</a>to do that. So most containers will start running and save history to memory and not file (and this makessense in case there is sensitive information) but it was problematic for me because I couldn’t parse it.For example, when someone presses up and down a bunch of times, I might see:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">[</span>A[A[A[A[A[B[A</code></pre></div></div><p>This is a reference to some previous command that I can only find in historygiven I’m parsing the input/output as I am. So my second attempt (well, maybe second throughtenth) I was trying different variations of trying to be able to parse the history.If you looked at <a href=\"https://twitter.com/ajdecon/status/1492381132998033409\" target=\"_blank\">the tweet</a>you’ll see we need to run:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span><span class=\"nb\">history</span> <span class=\"nt\">-a</span></code></pre></div></div><p>to start writing what’s in memory to file. I didn’t want to do this on every command, because alongwith the user seeing it and the UI being awful, it was just too much. Instead, I realized that I had a smallopportunity when the user first shells into the container (and is expecting a jump in their UI) to run whateverI need and then clear the terminal. So I ran it there, right before a clear and welcome message.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>    <span class=\"k\">def</span> <span class=\"nf\">welcome</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">openpty</span><span class=\"p\">):</span>        <span class=\"s\">\"\"\"        Welcome the user and clear terminal        \"\"\"</span>        <span class=\"c1\"># Don't add commands executed to history</span>        <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">openpty</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">encode</span><span class=\"p\">(</span><span class=\"s\">\" export PROMPT_COMMAND='history -a'</span><span class=\"se\">\\r</span><span class=\"s\">\"</span><span class=\"p\">))</span>        <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">openpty</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">encode</span><span class=\"p\">(</span><span class=\"s\">\" clear</span><span class=\"se\">\\r</span><span class=\"s\">\"</span><span class=\"p\">))</span>        <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">openpty</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">encode</span><span class=\"p\">(</span><span class=\"s\">\" ### Welcome to PAKS! ###</span><span class=\"se\">\\r</span><span class=\"s\">\"</span><span class=\"p\">))</span></code></pre></div></div><p>And with this method you aren’t aware of the extra commands at all! And did you notice the spaces above? That’s also another trick! Any command that you type with a leadingspace won’t be saved to history, and this is thanks to <a href=\"https://unix.stackexchange.com/questions/115934/why-does-bash-have-a-histcontrol-ignorespace-option\">HISTCONTROL</a> that has an ignorespace option. I think most people / containersset it to ignore space and to ignore duplicates:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">root</span><span class=\"o\">@</span><span class=\"mi\">1</span><span class=\"n\">c268386714a</span><span class=\"p\">:</span><span class=\"o\">/</span><span class=\"c1\"># echo $HISTCONTROL</span><span class=\"n\">ignoredups</span><span class=\"p\">:</span><span class=\"n\">ignorespace</span></code></pre></div></div><p>That said, I don’t explicitly try to reset this in the container, so that could be a bugif there is a container base that doesn’t do that. And I’m pretty sure centos doesn’t come with clear!I’ll likely need to work on this a bit more.</p><blockquote>  <p>For now, please consider this only working for debian/ubuntu bases and we can inspect the other ones later!</p></blockquote><p>Okay, so now let’s look at the function to get history (self.hist.run). For now, just ignore the command toget the history, that’s actually done via a Paks command that we will talk about after.Here is what is going on:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">def</span> <span class=\"nf\">get_history</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">line</span><span class=\"p\">,</span> <span class=\"n\">openpty</span><span class=\"p\">):</span>    <span class=\"s\">\"\"\"    Given an input with some number of up/down and newline, derive command.    \"\"\"</span>    <span class=\"c1\"># Calculate the absolute change of ups/downs</span>    <span class=\"n\">up</span> <span class=\"o\">=</span> <span class=\"n\">line</span><span class=\"p\">.</span><span class=\"n\">count</span><span class=\"p\">(</span><span class=\"s\">\"[A\"</span><span class=\"p\">)</span>    <span class=\"n\">down</span> <span class=\"o\">=</span> <span class=\"n\">line</span><span class=\"p\">.</span><span class=\"n\">count</span><span class=\"p\">(</span><span class=\"s\">\"[B\"</span><span class=\"p\">)</span>    <span class=\"n\">change</span> <span class=\"o\">=</span> <span class=\"n\">up</span> <span class=\"o\">-</span> <span class=\"n\">down</span>    <span class=\"c1\"># pushed down below history (maybe they are angry?)</span>    <span class=\"k\">if</span> <span class=\"n\">change</span> <span class=\"o\">&lt;=</span> <span class=\"mi\">0</span><span class=\"p\">:</span>       <span class=\"k\">return</span> <span class=\"s\">\"\"</span>    <span class=\"c1\"># Retrieve history, actually via a command run from the outside to get the file</span>    <span class=\"n\">history</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">hist</span><span class=\"p\">.</span><span class=\"n\">run</span><span class=\"p\">(</span>        <span class=\"n\">container_name</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">uri</span><span class=\"p\">.</span><span class=\"n\">extended_name</span><span class=\"p\">,</span>        <span class=\"n\">out</span><span class=\"o\">=</span><span class=\"n\">openpty</span><span class=\"p\">,</span>        <span class=\"n\">history_file</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">settings</span><span class=\"p\">.</span><span class=\"n\">history_file</span><span class=\"p\">,</span>        <span class=\"n\">user</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">settings</span><span class=\"p\">.</span><span class=\"n\">user</span><span class=\"p\">,</span>    <span class=\"p\">)</span>    <span class=\"n\">history</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">x</span> <span class=\"k\">for</span> <span class=\"n\">x</span> <span class=\"ow\">in</span> <span class=\"n\">history</span><span class=\"p\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"s\">\"</span><span class=\"se\">\\n</span><span class=\"s\">\"</span><span class=\"p\">)</span> <span class=\"k\">if</span> <span class=\"n\">x</span><span class=\"p\">]</span>    <span class=\"c1\"># No history, nothing to return</span>    <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">history</span><span class=\"p\">:</span>        <span class=\"k\">return</span> <span class=\"s\">\"\"</span>    <span class=\"c1\"># The change is outside the length of history</span>    <span class=\"k\">if</span> <span class=\"n\">change</span> <span class=\"o\">&gt;</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">history</span><span class=\"p\">):</span>        <span class=\"k\">return</span> <span class=\"s\">\"\"</span>    <span class=\"c1\"># here we are looking back up into history (negative index)</span>    <span class=\"n\">newline</span> <span class=\"o\">=</span> <span class=\"n\">history</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span> <span class=\"o\">*</span> <span class=\"n\">change</span><span class=\"p\">]</span>    <span class=\"c1\"># Add back any characters typed AFTER the up/down presses</span>    <span class=\"n\">newline</span> <span class=\"o\">+=</span> <span class=\"n\">re</span><span class=\"p\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"s\">\"(\\[A|\\[B)\"</span><span class=\"p\">,</span> <span class=\"n\">line</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span>    <span class=\"k\">return</span> <span class=\"n\">newline</span></code></pre></div></div><p>The above might not be perfect, but it worked the best for everything that I tried!This allows us to issue a command that paks knows, press up to get it again, and then editit and have the command work correctly. Speaking of commands…</p><h3 id=\"commands\">Commands</h3><p>The core meat of paks is the commands that it recognizes. Every command has a <a href=\"https://github.com/syspack/paks/blob/ab61458a061c555434e5d3406914612fd1d60442/paks/commands/command.py#L26\" target=\"_blank\">base class</a>that is going to handle parsing a line (with a main command and optional args or kwargs, depending on the command),ensuring all required variables are passed (this is largely internal to the library and even a developer userdoesn’t need to think about it unless they want to change what is passed), and then providing functions for basic kinds ofexecution. So let’s step back and first look at how we find a command (or executor). Basically, once we have a newlineand we’ve parsed it per the above (looking up history and such) we can sniff it to see if it matches a knowncommand pattern:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># If we have a newline (and possibly a command)</span><span class=\"k\">if</span> <span class=\"n\">has_newline</span><span class=\"p\">:</span>    <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">run_executor</span><span class=\"p\">(</span><span class=\"n\">string_input</span><span class=\"p\">,</span> <span class=\"n\">openpty</span><span class=\"p\">)</span>    <span class=\"c1\"># Add derived line to the history</span>    <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">openpty</span><span class=\"p\">,</span> <span class=\"n\">terminal_input</span><span class=\"p\">)</span>    <span class=\"n\">string_input</span> <span class=\"o\">=</span> <span class=\"s\">\"\"</span></code></pre></div></div><p>The function “run_executor” is going to make this call if there is a Paks command and handle it.And no matter what, we reset our string input to be empty given that the user pressed enter, becausethey are going to start typing fresh. But before that, this function “run_executor” is going to seeif there are any known commands, and if so, to run them! That function looks like this:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">def</span> <span class=\"nf\">run_executor</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">string_input</span><span class=\"p\">,</span> <span class=\"n\">openpty</span><span class=\"p\">):</span>    <span class=\"s\">\"\"\"    Given a string input, run executor    \"\"\"</span>    <span class=\"c1\"># Get out early if it's not a Paks command (always starts with #)</span>    <span class=\"n\">string_input</span> <span class=\"o\">=</span> <span class=\"n\">string_input</span><span class=\"p\">.</span><span class=\"n\">replace</span><span class=\"p\">(</span><span class=\"s\">\"[A\"</span><span class=\"p\">,</span> <span class=\"s\">\"\"</span><span class=\"p\">).</span><span class=\"n\">replace</span><span class=\"p\">(</span><span class=\"s\">\"[B\"</span><span class=\"p\">,</span> <span class=\"s\">\"\"</span><span class=\"p\">)</span>    <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">string_input</span><span class=\"p\">.</span><span class=\"n\">startswith</span><span class=\"p\">(</span><span class=\"s\">\"#\"</span><span class=\"p\">):</span>        <span class=\"k\">return</span>    <span class=\"c1\"># Do we have a matching executor?</span>    <span class=\"n\">executor</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">commands</span><span class=\"p\">.</span><span class=\"n\">get_executor</span><span class=\"p\">(</span><span class=\"n\">string_input</span><span class=\"p\">,</span> <span class=\"n\">out</span><span class=\"o\">=</span><span class=\"n\">openpty</span><span class=\"p\">)</span>    <span class=\"k\">if</span> <span class=\"n\">executor</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"bp\">None</span><span class=\"p\">:</span>        <span class=\"c1\"># Print any message it wants to the terminal before run...</span>        <span class=\"k\">if</span> <span class=\"n\">executor</span><span class=\"p\">.</span><span class=\"n\">pre_message</span><span class=\"p\">:</span>            <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"</span><span class=\"se\">\\n\\r</span><span class=\"s\">\"</span> <span class=\"o\">+</span> <span class=\"n\">executor</span><span class=\"p\">.</span><span class=\"n\">pre_message</span><span class=\"p\">)</span>        <span class=\"c1\"># Run it!</span>        <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">executor</span><span class=\"p\">.</span><span class=\"n\">run</span><span class=\"p\">(</span>            <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">image</span><span class=\"p\">,</span>            <span class=\"n\">container_name</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">uri</span><span class=\"p\">.</span><span class=\"n\">extended_name</span><span class=\"p\">,</span>            <span class=\"n\">original</span><span class=\"o\">=</span><span class=\"n\">string_input</span><span class=\"p\">,</span>        <span class=\"p\">)</span>        <span class=\"c1\"># And any message it wants to print after</span>        <span class=\"k\">if</span> <span class=\"n\">result</span><span class=\"p\">.</span><span class=\"n\">message</span><span class=\"p\">:</span>            <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"</span><span class=\"se\">\\r</span><span class=\"s\">\"</span> <span class=\"o\">+</span> <span class=\"n\">result</span><span class=\"p\">.</span><span class=\"n\">message</span><span class=\"p\">)</span></code></pre></div></div><p>The result object holds what you would expect - a return code, some message,and the basic outputs of the call. It’s up to the executor (command) to decidewhat to show the user. Some might not show anything beyond commands that are runwith the executor. So what does that function “get_executor” look like?This is where we delive into the commands module, where there is a simple lookup ofthe starting prefixes of commands matched to Command classes:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># lookup of named commands and settings</span><span class=\"n\">docker_commands</span> <span class=\"o\">=</span> <span class=\"p\">{</span>    <span class=\"s\">\"#save\"</span><span class=\"p\">:</span> <span class=\"n\">SaveContainer</span><span class=\"p\">,</span>    <span class=\"s\">\"#inspect\"</span><span class=\"p\">:</span> <span class=\"n\">InspectContainer</span><span class=\"p\">,</span>    <span class=\"s\">\"#envload\"</span><span class=\"p\">:</span> <span class=\"n\">EnvLoad</span><span class=\"p\">,</span>    <span class=\"s\">\"#envhost\"</span><span class=\"p\">:</span> <span class=\"n\">EnvHost</span><span class=\"p\">,</span>    <span class=\"s\">\"#envsave\"</span><span class=\"p\">:</span> <span class=\"n\">EnvSave</span><span class=\"p\">,</span>    <span class=\"s\">\"#cp\"</span><span class=\"p\">:</span> <span class=\"n\">Copy</span><span class=\"p\">,</span>    <span class=\"s\">\"#size\"</span><span class=\"p\">:</span> <span class=\"n\">Size</span><span class=\"p\">,</span><span class=\"p\">}</span></code></pre></div></div><p>When I add a load functionality, all it will need to do is update this dictionary.And the reason those are “docker commands” is that you can imagine we eventuallysupport other container technologies, and the commands you run are going to vary.Each Command actually has a class attribute for the container types that are supported.Here is a snippet of the DockerCommands class attached to the client that we are calling “get_executor” on:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">class</span> <span class=\"nc\">DockerCommands</span><span class=\"p\">:</span>    <span class=\"c1\"># Required kwargs for any docker/podman command to run</span>    <span class=\"n\">required</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s\">\"container_name\"</span><span class=\"p\">,</span> <span class=\"s\">\"name\"</span><span class=\"p\">]</span>    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">container_tech</span><span class=\"p\">):</span>        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">command</span> <span class=\"o\">=</span> <span class=\"n\">container_tech</span>        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">lookup</span> <span class=\"o\">=</span> <span class=\"n\">docker_commands</span>    <span class=\"k\">def</span> <span class=\"nf\">parse_name</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">cmd</span><span class=\"p\">):</span>        <span class=\"n\">parts</span> <span class=\"o\">=</span> <span class=\"n\">cmd</span><span class=\"p\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"s\">\" \"</span><span class=\"p\">)</span>        <span class=\"k\">return</span> <span class=\"n\">parts</span><span class=\"p\">.</span><span class=\"n\">pop</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">).</span><span class=\"n\">replace</span><span class=\"p\">(</span><span class=\"s\">\"</span><span class=\"se\">\\n</span><span class=\"s\">\"</span><span class=\"p\">,</span> <span class=\"s\">\"\"</span><span class=\"p\">).</span><span class=\"n\">replace</span><span class=\"p\">(</span><span class=\"s\">\"</span><span class=\"se\">\\r</span><span class=\"s\">\"</span><span class=\"p\">,</span> <span class=\"s\">\"\"</span><span class=\"p\">).</span><span class=\"n\">strip</span><span class=\"p\">()</span>    <span class=\"k\">def</span> <span class=\"nf\">has_command</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"p\">):</span>        <span class=\"n\">name</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">parse_name</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"p\">)</span>        <span class=\"k\">return</span> <span class=\"n\">name</span> <span class=\"ow\">in</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">lookup</span>    <span class=\"o\">@</span><span class=\"nb\">property</span>    <span class=\"k\">def</span> <span class=\"nf\">history</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>        <span class=\"k\">return</span> <span class=\"n\">History</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">command</span><span class=\"p\">)</span>    <span class=\"k\">def</span> <span class=\"nf\">get_executor</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"p\">,</span> <span class=\"n\">out</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">):</span>        <span class=\"s\">\"\"\"        Backend is required to update history        \"\"\"</span>        <span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">parse_name</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"p\">)</span>        <span class=\"k\">if</span> <span class=\"n\">name</span> <span class=\"ow\">in</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">lookup</span><span class=\"p\">:</span>            <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">lookup</span><span class=\"p\">[</span><span class=\"n\">name</span><span class=\"p\">](</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">command</span><span class=\"p\">,</span> <span class=\"n\">required</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">required</span><span class=\"p\">,</span> <span class=\"n\">out</span><span class=\"o\">=</span><span class=\"n\">out</span><span class=\"p\">)</span></code></pre></div></div><p>To focus on the last function, you basically see that we parse the line (name), and thensee if it’s in our lookup. If so, we return the initialized executor, and we need to addthe output source in case it needs to interact with the current terminal. The self.commandrefers to the container technology (e.g., docker or podman in this case).</p><p>Then we can look at a particular command (e.g., inspect) and see it’s pretty simple! We have definedthe supported container technologies along with optional messages, and a main run function. Here is the commandto inspect, which will dump out the json manifest and optionally take a section:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">class</span> <span class=\"nc\">InspectContainer</span><span class=\"p\">(</span><span class=\"n\">Command</span><span class=\"p\">):</span>    <span class=\"n\">supported_for</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s\">\"docker\"</span><span class=\"p\">,</span> <span class=\"s\">\"podman\"</span><span class=\"p\">]</span>    <span class=\"n\">pre_message</span> <span class=\"o\">=</span> <span class=\"s\">\"Inspecting Container...\"</span>    <span class=\"k\">def</span> <span class=\"nf\">run</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">):</span>        <span class=\"s\">\"\"\"        Inspect a container fully, or specific sections        \"\"\"</span>        <span class=\"c1\"># Always run this first to make sure container tech is valid</span>        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">check</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span>        <span class=\"c1\"># These are both required for docker/podman</span>        <span class=\"n\">container_name</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">kwargs</span><span class=\"p\">[</span><span class=\"s\">\"container_name\"</span><span class=\"p\">]</span>        <span class=\"c1\"># inspect particular attributes provided as args</span>        <span class=\"k\">if</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">args</span><span class=\"p\">:</span>            <span class=\"k\">for</span> <span class=\"n\">section</span> <span class=\"ow\">in</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">args</span><span class=\"p\">:</span>                <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">run_command</span><span class=\"p\">(</span>                    <span class=\"p\">[</span>                        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">tech</span><span class=\"p\">,</span>                        <span class=\"s\">\"inspect\"</span><span class=\"p\">,</span>                        <span class=\"s\">\"--format\"</span><span class=\"p\">,</span>                        <span class=\"s\">\"\"</span> <span class=\"o\">%</span> <span class=\"n\">section</span><span class=\"p\">.</span><span class=\"n\">capitalize</span><span class=\"p\">(),</span>                        <span class=\"n\">container_name</span><span class=\"p\">,</span>                    <span class=\"p\">]</span>                <span class=\"p\">)</span>        <span class=\"c1\"># Otherwise just dump the whole thing</span>        <span class=\"k\">else</span><span class=\"p\">:</span>            <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">run_command</span><span class=\"p\">([</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">tech</span><span class=\"p\">,</span> <span class=\"s\">\"inspect\"</span><span class=\"p\">,</span> <span class=\"n\">container_name</span><span class=\"p\">])</span>            <span class=\"k\">if</span> <span class=\"n\">result</span><span class=\"p\">:</span>                <span class=\"k\">return</span> <span class=\"n\">result</span>        <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">return_success</span><span class=\"p\">()</span></code></pre></div></div><p>You’ll now know the main Paks trick - because we are still running on the host,we can issue commands to the host while we are in the container! In the above, we can just type:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#inspect</span><span class=\"c\">#inspect config</span></code></pre></div></div><p>And see the output in the terminal! This is how a lot of the interactions with the host work.It’s kind of simple and silly, but also really cool when you see it work on the container!So the run function above, just as a reminder, is called by this part:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">executor</span><span class=\"p\">.</span><span class=\"n\">run</span><span class=\"p\">(</span>    <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">image</span><span class=\"p\">,</span>    <span class=\"n\">container_name</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">uri</span><span class=\"p\">.</span><span class=\"n\">extended_name</span><span class=\"p\">,</span>    <span class=\"n\">original</span><span class=\"o\">=</span><span class=\"n\">string_input</span><span class=\"p\">,</span><span class=\"p\">)</span></code></pre></div></div><p>And honestly, that’s the majority of Paks! 🎉️</p><h2 id=\"discussion\">Discussion</h2><p>Paks has  honestly been so fun to work on, despite long hours of trying to figure things out during evenings and weekends. I’m so excitedabout the ideas, and I want to share them with others because I think developer tools for containersare kind of lacking. Heck, I stayed up until like 4am writing this post. No, I don’t normally do that,I had some things on my mind, but it was an excellent use of the time, despite the fact that I woke up 4 hours later andI’m going to crash tonight (err tomorrow night… err now that I’m tweaking up the finishing touches to this post)!</p><h3 id=\"next-steps\">Next Steps</h3><p>I’m working on a “paks load” command that will let someone develop a Python modulewith some set of commands for their custom use case. The first thing I wanted to trywas to generate sboms for spack (e.g., “Generate sboms for this spack install in the containerand save them to my host so I can upload alongside the container to a registry). I hadsome <a href=\"https://github.com/spack/spack-sbom\" target=\"_blank\">previous work</a> to use spack scripting, but ultimately this weekend did a <a href=\"https://github.com/spack/spack/pull/28909\" target=\"_blank\">pull request</a>to add sbom generation to spack proper. And then I’ll be able to work on the load commands.I also want to address some of the anticipated bugs I mentioned above, like properly setting “HISTCONTROL”to ensure we don’t save commands issued by the client to history, and possibly having a cleanup step on savethat removes the file. I haven’t added this yet is because if I’m developing in the container and want to say, move it from my local machine to HPC, I kind of want to have my history so I can lazily use it.</p><h3 id=\"but-really\">But Really…</h3><p>We have some magic up our sleeves for what we are actually working on to inspire these ideas!I guess you’ll just have to wait for the future, because <a href=\"https://github.com/alecbcs\" target=\"_blank\">@alecbcs</a> andI are both have vision and are a great tag team! 🎉️</p><h3 id=\"security\">Security</h3><p>So there are obviously security issues around a library like this - and I added notesto the documentation that I’ll re-iterate here. Paks is intended for use by a developerthat is in their own trusted environment, whether local or on HPC. Because there is an interactionwith the host, you wouldn’t use this in production someone to give users an ability to loadenvironments or save. You also wouldn’t want to save a development container with somethingprivate in history and push it. I’m still an advocate for, after development is done,pushing changed code to GitHub and having an automated build build, test, and deploy.Could we eventually have a production grade library to enable interactions inside thecontainer? Possibly, but it’s not Paks in Python in its current state. I thinkthat’s okay - we have to start small with ideas and go from there.</p><h3 id=\"didnt-i-see-paks-before\">Didn’t I see paks before?</h3><p>Yes, you did! A previous version was intended for making spack build caches on GitHub, but thatdidn’t work because you couldn’t build a spack package within a container and thenpull the same container and install it and hit the cache. I think this might work someday,hence why I haven’t completely deleted the code, but I couldn’t let a cute logo and colorscheme go to waste!So for now it’s on a separate branch but largely I am not working on it. If you want to see this branch,it’s still <a href=\"https://github.com/syspack/paks/tree/v1/spack\" target=\"_blank\">here</a>!</p><p>Thanks for reading friends! I hope this has been interesting and you might be inspired toalso work on better tooling for developers, even if that just means exploring the ideas.</p>",
            "url": "https://hpc.social/personal-blog/2022/interactive-development-containers/",
            
            
            
            
            
            "date_published": "2022-02-15T12:30:00-07:00",
            "date_modified": "2022-02-15T12:30:00-07:00",
            
                "author": "Vanessasaurus"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/developing-managed-vs-self-hosted-software/",
            "title": "Developing managed vs self-hosted software",
            "summary": null,
            "content_text": "I&#8217;ve done some work lately with teams that deliver their products in very different ways, and it has me thinking about how much our &#8220;best practices&#8221; depend on a product&#8217;s delivery and operations model. I&#8217;ve had a bunch of conversations about this tensionOn the one hand, some of the teams I&#8217;ve worked with build software services that are developed and operated by the same team, and where the customers (internal or external) directly make use of the operated service. These teams try to follow what I think of as &#8220;conventional&#8221; SaaS best practices:Their development workflow prioritizes iteration speed above all elseThey tend to deploy from HEAD, or close to it, in their source repositoryIn almost all cases, branches are short-lived for feature developmentThey&#8217;ve built good automated test suites and well-tuned CI/CD pipelinesReleases are very frequentThey make extensive use of observability tooling, often using third-party SaaS for thisFast roll-back is prioritized over perfect testing ahead of timeWhile their user documentation is mostly good, their operations documentation tends to be &#8220;just good enough&#8221; to onboard new team members, and a lot of it lives in SlackHowever, we also have plenty of customers who deploy our software to their own systems, whether in the cloud or on-premise. (Some of them don&#8217;t even connect to the Internet on a regular basis!) The development workflow for software aimed at these customers looks rather different:Deploys are managed by the customer, and release cycles are longerThese teams do still have CI/CD and extensive automated tests&#8230; but they may also have explicit QA steps before releasesThere tend to be lots of longer-lived version branches, and even &#8220;LTS&#8221; branches with their own roadmapsLogging is prioritized over observability, because they can&#8217;t make assumptions about the customer toolingThey put a lot more effort into operational documentation, because most operators will not also be developersFrom a developer perspective, of course, this all feels much more painful! The managed service use case feels much more comfortable to develop for, and most of the community tooling and best practices for web development seems to optimize for that model.But from a sysadmin perspective, used to mostly operating third-party software, the constraints of self-hosted development are all very familiar. And even managed service teams often rely on third-party software developed using this kind of model, relying on LTS releases of Linux distributions and pinning major versions of dependencies.The biggest challenge I&#8217;ve seen, however, is when a development team tries to target the same software at both use cases. As far as I can tell, it&#8217;s very difficult to simultaneously operate a reliable service that is being continuously developed and deployed, and to provide predictable and high-quality releases to self-hosted customers.So far, I&#8217;ve seen this tension resolved in three different ways:The internal service becomes &#8220;just another customer&#8221;, operating something close to the latest external release, resulting in a slower release cycle for the internal serviceFast development for the internal service gets prioritized, with external releases becoming less frequent and including bigger and bigger changesInternal and external diverge completely, with separate development teams taking over (and often a name change for one of them)I don&#8217;t really have a conclusion here, except that I don&#8217;t really love any of these results. /sighIf you&#8217;re reading this and have run into similar tensions, how have you seen this resolved? Have you seen any success stories in deploying the same code internally and externally? Or alternatively &#8212; any interesting stories of failure to share?  Feel free to send me an email, I&#8217;d be interested to hear from you.",
            "content_html": "<p>I&#8217;ve done some work lately with teams that deliver their products in very different ways, and it has me thinking about how much our &#8220;best practices&#8221; depend on a product&#8217;s delivery and operations model. I&#8217;ve had a bunch of conversations about this tension</p><p>On the one hand, some of the teams I&#8217;ve worked with build software services that are developed and operated by the same team, and where the customers (internal or external) directly make use of the operated service. These teams try to follow what I think of as &#8220;conventional&#8221; SaaS best practices:</p><ul><li>Their development workflow prioritizes iteration speed above all else</li><li>They tend to deploy from HEAD, or close to it, in their source repository<ul><li>In almost all cases, branches are short-lived for feature development</li></ul></li><li>They&#8217;ve built good automated test suites and well-tuned CI/CD pipelines</li><li>Releases are very frequent</li><li>They make extensive use of observability tooling, often using third-party SaaS for this</li><li>Fast roll-back is prioritized over perfect testing ahead of time</li><li>While their user documentation is mostly good, their operations documentation tends to be &#8220;just good enough&#8221; to onboard new team members, and a lot of it lives in Slack</li></ul><p>However, we also have plenty of customers who deploy our software to their own systems, whether in the cloud or on-premise. (Some of them don&#8217;t even connect to the Internet on a regular basis!) The development workflow for software aimed at these customers looks rather different:</p><ul><li>Deploys are managed by the customer, and release cycles are longer</li><li>These teams do still have CI/CD and extensive automated tests&#8230; but they may also have explicit QA steps before releases</li><li>There tend to be lots of longer-lived version branches, and even &#8220;LTS&#8221; branches with their own roadmaps</li><li>Logging is prioritized over observability, because they can&#8217;t make assumptions about the customer tooling</li><li>They put a lot more effort into operational documentation, because most operators will not also be developers</li></ul><p>From a developer perspective, of course, this all feels much more painful! The managed service use case feels much more comfortable to develop for, and most of the community tooling and best practices for web development seems to optimize for that model.</p><p>But from a sysadmin perspective, used to mostly operating third-party software, the constraints of self-hosted development are all very familiar. And even managed service teams often rely on third-party software developed using this kind of model, relying on LTS releases of Linux distributions and pinning major versions of dependencies.</p><p>The biggest challenge I&#8217;ve seen, however, is when a development team tries to target the same software at <em>both use cases</em>. As far as I can tell, it&#8217;s very difficult to simultaneously operate a reliable service that is being continuously developed and deployed, and to provide predictable and high-quality releases to self-hosted customers.</p><p>So far, I&#8217;ve seen this tension resolved in three different ways:</p><ul><li>The internal service becomes &#8220;just another customer&#8221;, operating something close to the latest external release, resulting in a slower release cycle for the internal service</li><li>Fast development for the internal service gets prioritized, with external releases becoming less frequent and including bigger and bigger changes</li><li>Internal and external diverge completely, with separate development teams taking over (and often a name change for one of them)</li></ul><p>I don&#8217;t really have a conclusion here, except that I don&#8217;t really love any of these results. /sigh</p><p>If you&#8217;re reading this and have run into similar tensions, how have you seen this resolved? Have you seen any success stories in deploying the same code internally and externally? Or alternatively &#8212; any interesting stories of failure to share? <img alt=\"😉\" class=\"wp-smiley\" src=\"https://s.w.org/images/core/emoji/14.0.0/72x72/1f609.png\" style=\"height: 1em;\" /> Feel free to <a href=\"mailto:ajdecon@ajdecon.org\">send me an email</a>, I&#8217;d be interested to hear from you.</p>",
            "url": "https://hpc.social/personal-blog/2022/developing-managed-vs-self-hosted-software/",
            
            
            
            
            
            "date_published": "2022-02-12T16:00:00-07:00",
            "date_modified": "2022-02-12T16:00:00-07:00",
            
                "author": "Thinking Out Loud"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/toy-programs-for-learning-a-new-language/",
            "title": "Toy programs for learning a new language",
            "summary": null,
            "content_text": "It used to be that I&#8217;d get interested in a new programming language, but I wouldn&#8217;t have a new project to use it for and had trouble knowing how to start. I have trouble really grasping a new language without building something in it, and &#8220;X by example&#8221; or working through a book don&#8217;t really do the job.What&#8217;s helped me lately is to build an array of &#8220;standard&#8221; toy programs that I understand reasonably well, and that I can use to explore the new language and figure out how to do something real in it.Right now, my toy program collection consists of:A link shortening service, like bit.ly or tinyurl, along with a HTTP API for adding and removing linksA 2D diffusion simulationA &#8220;system package inventory&#8221; program, that builds a list of all the RPMs/DEBs installed on a Linux machine and shoves them into a SQLite databaseThis is almost never what I&#8217;d call production-quality code. For example, when I&#8217;m writing these toy programs, I rarely write unit tests (until I start exploring the test libraries for the language!). But they&#8217;re still very valuable learning tools, and give me space to explore some very different use-cases.I almost always write all three in a given language, but the order depends a lot on what I think the new language will be good for. For example, I&#8217;ll  write the &#8220;system package inventory&#8221; program first if I think the new language might be handy for system administration tools. It&#8217;s a great way to see how well the language plays with a common Linux environment, how painful it is to use SQLite, and to get practice writing CLI tools in it. I&#8217;ll often augment the basic &#8220;scan and store&#8221; functionality with a CLI to do frequent queries, like &#8220;on what date was this package last upgraded&#8221;.On the other hand, if I think I&#8217;m going to use the new language for a bunch of numerical work, I&#8217;ll start with the diffusion simulation. When I write that, I often start with a naive implementation and then start playing with profilers and other performance tools, or try to parallelize the simulation. This is also a great excuse to dig into any plotting tools commonly used with the language.These toy programs are also handy if I want to explore new ways to integrate a service into a larger production environment. For example, I might start with the link shortening service, deploying the service itself statelessly and persisting the list of links into a PostgreSQL DB. Then I start complicating things&#8230;Let&#8217;s add logging!And tracing!It&#8217;s always a good idea to expose Prometheus metricsAnd wouldn&#8217;t it be handy to support multiple database backends?Now wrap it all in a Helm chart for handy deploymentI imagine I&#8217;m not the only person to have a standard collection of learning projects for new languages. If you do this too, what does your project list look like?",
            "content_html": "<p>It used to be that I&#8217;d get interested in a new programming language, but I wouldn&#8217;t have a new project to use it for and had trouble knowing how to start. I have trouble really grasping a new language without building something in it, and &#8220;X by example&#8221; or working through a book don&#8217;t really do the job.</p><p>What&#8217;s helped me lately is to build an array of &#8220;standard&#8221; toy programs that I understand reasonably well, and that I can use to explore the new language and figure out how to do something real in it.</p><p>Right now, my toy program collection consists of:</p><ul><li>A link shortening service, like <a href=\"https://bit.ly\">bit.ly</a> or <a href=\"https://tinyurl.com/\">tinyurl</a>, along with a HTTP API for adding and removing links</li><li>A <a href=\"https://scipython.com/book/chapter-7-matplotlib/examples/the-two-dimensional-diffusion-equation/\">2D diffusion simulation</a></li><li>A &#8220;system package inventory&#8221; program, that builds a list of all the RPMs/DEBs installed on a Linux machine and shoves them into a SQLite database</li></ul><p>This is almost never what I&#8217;d call production-quality code. For example, when I&#8217;m writing these toy programs, I rarely write unit tests (until I start exploring the test libraries for the language!). But they&#8217;re still very valuable learning tools, and give me space to explore some very different use-cases.</p><p>I almost always write all three in a given language, but the order depends a lot on what I think the new language will be good for. For example, I&#8217;ll  write the &#8220;system package inventory&#8221; program first if I think the new language might be handy for system administration tools. It&#8217;s a great way to see how well the language plays with a common Linux environment, how painful it is to use SQLite, and to get practice writing CLI tools in it. I&#8217;ll often augment the basic &#8220;scan and store&#8221; functionality with a CLI to do frequent queries, like &#8220;on what date was this package last upgraded&#8221;.</p><p>On the other hand, if I think I&#8217;m going to use the new language for a bunch of numerical work, I&#8217;ll start with the diffusion simulation. When I write that, I often start with a naive implementation and then start playing with profilers and other performance tools, or try to parallelize the simulation. This is also a great excuse to dig into any plotting tools commonly used with the language.</p><p>These toy programs are also handy if I want to explore new ways to integrate a service into a larger production environment. For example, I might start with the link shortening service, deploying the service itself statelessly and persisting the list of links into a PostgreSQL DB. Then I start complicating things&#8230;</p><ul><li>Let&#8217;s add logging!</li><li>And tracing!</li><li>It&#8217;s always a good idea to expose Prometheus metrics</li><li>And wouldn&#8217;t it be handy to support multiple database backends?</li><li>Now wrap it all in a Helm chart for handy deployment</li></ul><p>I imagine I&#8217;m not the only person to have a standard collection of learning projects for new languages. If you do this too, what does your project list look like?</p>",
            "url": "https://hpc.social/personal-blog/2022/toy-programs-for-learning-a-new-language/",
            
            
            
            
            
            "date_published": "2022-01-15T16:00:00-07:00",
            "date_modified": "2022-01-15T16:00:00-07:00",
            
                "author": "Thinking Out Loud"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/cache-age-binning-pr-finally-merged/",
            "title": "Cache Age Binning PR Finally Merged!",
            "summary": null,
            "content_text": "I’ve had this PR hanging around in various forms for years.  It’s basically the last peice of the OSD memory target code.  We can now get a “binned” view of the relative ages of items in different LRU caches and dynamically adjust target sizes for different caches.  PR is here and memory usage behavior charts are here.",
            "content_html": "<p>I’ve had this PR hanging around in various forms for years.  It’s basically the last peice of the OSD memory target code.  We can now get a “binned” view of the relative ages of items in different LRU caches and dynamically adjust target sizes for different caches.  PR is <a href=\"https://github.com/ceph/ceph/pull/43299\">here</a> and memory usage behavior charts are <a href=\"https://docs.google.com/spreadsheets/d/1lSp2cLzYmRfPILDCyLMXciIfdf0OvSFngwXukQFXIqQ/edit?usp=sharing\">here</a>.</p>",
            "url": "https://hpc.social/personal-blog/2022/cache-age-binning-pr-finally-merged/",
            
            
            
            
            
            "date_published": "2022-01-12T00:00:00-07:00",
            "date_modified": "2022-01-12T00:00:00-07:00",
            
                "author": "Mark Nelson's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/things-that-are-hard/",
            "title": "Things that are Hard",
            "summary": null,
            "content_text": "I saw a funny tweet on Twitter the other night - it was someone from a large consumer company sharingtheir vision for “the next generation shopping experience” and it was a virtual person walking through a supermarket aisle and reaching out to pick up a bottle of wine.I can’t find the specific tweet, but it said something to the effect of:  Nobody asked for this. Stop making stuff to solve problems that people don’t haveMy dear reader, it me! 😲️ This message hit me really hard, because I am definitely one to build niche tools for use cases that likely don’t exist but seem fun or interesting to me. I also feel pretty disconnected from communities that are innovating and testing new ideas.What is hard?This is a problem that a lot of us have. We build things that nobody needs. We need to focus more on the problems that people are actually facing. I would also scope that to developer workflows, which includes automation, testing, and development. Since I have a nice view into my own mental space, here is my list of things that are hard.  When I am trying to develop software and I can't open an interface with the code and environment I need  That my main interaction with a resource is via SSH  When a workflow or even container works in one place but not another  When I need to develop, build in CI, push to a registry, and pull. One mistake? Start from scratch  When I need to run a job and I have to interact with a job manager and it's hard and annoying  Logging or monitoring means looking at text files with cryptic names  Automated testing on HPC is not a thing. Build on GitHub and pray.  When I can't easily navigate code, documentation, or it's completely missing  When I set up everything the way I like it and I have to login to a new system and do it all over again  When I want to develop something that uses a cluster resource but there are no exposed APIs.  When it's impossible to compare between systems because they are special snowflakes  When I can't easily test across the systems that my software is intended for.  To scale anything I have to use a job manager, wait hours, and then again if there is one mistake  When it takes 2 hours or more to get a node allocated  When I can't really make tools for HPC because I'm trying to find workarounds for all these problemsAnd I’d add a “thing that is annoying” to be this obsessive focus on power and scale, in a competitive sense, and this raceto be in the top 500 and beat the other guy over all else. The constant need to rebuild clusters means we neverfocus on the details of how we use them. We do the same things over again. I’ve mentioned these things before, possibly many times, but I need to point it out again.  Our current developer environments are more like handcuffs than places we are enabled to thrive.The reality for me is that I tend to put myself in a new role or environment, and then think of lots of cool ways to extend a particular tool, or build something before it. This is why I’ve made a ton of visualizations, associated tools, or posts for spack - it’s literally just the thing that is right in front of me. Put something else in front of me, such as an entire infrastructure with APIs, and I’d do the same. So why can’t a nice set of developer tools be available for the resources I’m supposed to be using?Develop based on specific problemsI think I want to develop more focusing on these problems. Don’t get me wrong - I’ll definitely keep making silly projects. But my vision for the future needs to be oriented toward these pains. These in particular are the problems that I think our community needs to look at, at least given this developer perspective.I say this because I’ve seen and used the dark side - having free rein of beautiful cloud APIs to let me automate to my heart’s content! I only now, without being a part of some cloud or container cluster deployed project, am aware that I don’t have access to these development tools. What is my solution now? I largely don’t ssh into an HPC cluster until I absolutely have to - either to scale something, or reproduce a workflow on GitHub actions that works there (but then is really challenging to get it working on the cluster resource). Indeed this entire thread resulted after a frustrating evening of exactly that.What isn’t helpful? What isn’t helpful is telling me “This center / place / person has this thing that has solved this problem.” Can I easily access it, and what about the entire research software engineering community? This kind of response shuts down the conversation and makes the developer (myself for example) realize that the person I’m talking to is not interested in thinking about how to inspire change.I’ve been really frustrated recently with mentioning even an abstract idea, and getting shut down that “Oh that sounds like this other tool.”For a project to reach this “mention status” it needs to be easy to install or use, and not have a barrier of privilege that you have to work at a certain place or know people. Telling me that there is a solution that requires some convoluted steps and permissions not only implies that it is only available to those in privilege, but that the solution is not well adopted enough or shared enough to be truly a solution for our community.Inspiring VisionIf we aren’t happy with the current state of the world, what are our options? Well, we could leave our current roles to find another state that is more similar to what we want. Personally speaking, I haven’t hit that point quite yet. I want to try my hardest to formulate a vision for how I want the world to be, and then find opportunity to work on it from where I am. The wisdom here is that no specific role is perfect, and optimally we should place ourself somewhere where there are resources and open mindedness for change. it’s up to us to extend our influence as best we can to help drive some possible future. If you try that and fail? At least you tried.These are the things that are hard. I am going to try harder to have them be the focus of my thinking about the future. I want to make them easier. I’m starting to realize that possibly the reality is that I should think beyond the constraints of HPC, and more toward the kind of infrastructure that I want, and thenfigure out how to slowly integrate it as a part of our culture too. We can start with a core vision for a future that we want, and thenslowly build up tooling and community around that.Happy Friday, friends!",
            "content_html": "<p>I saw a funny tweet on Twitter the other night - it was someone from a large consumer company sharingtheir vision for “<a href=\"https://hypebeast.com/2022/1/walmart-2017-mutual-mobile-metaverse-shopping-video-resurfaces\" target=\"_blank\">the next generation shopping experience</a>” and it was a virtual person walking through a supermarket aisle and reaching out to pick up a bottle of wine.I can’t find the specific tweet, but it said something to the effect of:</p><blockquote>  <p>Nobody asked for this. Stop making stuff to solve problems that people don’t have</p></blockquote><p>My dear reader, it me! 😲️ This message hit me really hard, because I am definitely one to build niche tools for use cases that likely don’t exist but seem fun or interesting to me. I also feel pretty <a href=\"https://twitter.com/vsoch/status/1478913234136494081\" target=\"_blank\">disconnected</a> from communities that are innovating and testing new ideas.</p><h2 id=\"what-is-hard\">What is hard?</h2><p>This is a problem that a lot of us have. We build things that nobody needs. We need to focus more on the problems that people are actually facing. I would also scope that to developer workflows, which includes automation, testing, and development. Since I have a nice view into my own mental space, here is my list of things that are hard.</p><ol class=\"custom-counter\">  <li>When I am trying to develop software and I can't open an interface with the code and environment I need</li>  <li>That my main interaction with a resource is via SSH</li>  <li>When a workflow or even container works in one place but not another</li>  <li>When I need to develop, build in CI, push to a registry, and pull. One mistake? Start from scratch</li>  <li>When I need to run a job and I have to interact with a job manager and it's hard and annoying</li>  <li>Logging or monitoring means looking at text files with cryptic names</li>  <li>Automated testing on HPC is not a thing. Build on GitHub and pray.</li>  <li>When I can't easily navigate code, documentation, or it's completely missing</li>  <li>When I set up everything the way I like it and I have to login to a new system and do it all over again</li>  <li>When I want to develop something that uses a cluster resource but there are no exposed APIs.</li>  <li>When it's impossible to compare between systems because they are special snowflakes</li>  <li>When I can't easily test across the systems that my software is intended for.</li>  <li>To scale anything I have to use a job manager, wait hours, and then again if there is one mistake</li>  <li>When it takes 2 hours or more to get a node allocated</li>  <li>When I can't really make tools for HPC because I'm trying to find workarounds for all these problems</li></ol><p>And I’d add a “thing that is annoying” to be this obsessive focus on power and scale, in a competitive sense, and this raceto be in the top 500 and beat the other guy over all else. The constant need to rebuild clusters means we neverfocus on the details of how we use them. We do the same things over again. I’ve mentioned these things before, possibly many times, but I need to point it out again.</p><blockquote>  <p>Our current developer environments are more like handcuffs than places we are enabled to thrive.</p></blockquote><p>The reality for me is that I tend to put myself in a new role or environment, and then think of lots of cool ways to extend a particular tool, or build something before it. This is why I’ve made a ton of visualizations, associated tools, or posts for spack - it’s literally just the thing that is right in front of me. Put something else in front of me, such as an entire infrastructure with APIs, and I’d do the same. So why can’t a nice set of developer tools be available for the resources I’m supposed to be using?</p><h2 id=\"develop-based-on-specific-problems\">Develop based on specific problems</h2><p>I think I want to develop more focusing on these problems. Don’t get me wrong - I’ll definitely keep making silly projects. But my vision for the future needs to be oriented toward these pains. These in particular are the problems that I think our community needs to look at, at least given this developer perspective.I say this because I’ve seen and used the dark side - having free rein of beautiful cloud APIs to let me automate to my heart’s content! I only now, without being a part of some cloud or container cluster deployed project, am aware that I don’t have access to these development tools. What is my solution now? I largely don’t ssh into an HPC cluster until I absolutely have to - either to scale something, or reproduce a workflow on GitHub actions that works there (but then is really challenging to get it working on the cluster resource). Indeed <a href=\"https://twitter.com/vsoch/status/1461908217223528448\" target=\"_blank\">this entire thread</a> resulted after a frustrating evening of exactly that.</p><p>What isn’t helpful? What isn’t helpful is telling me “This center / place / person has this thing that has solved this problem.” Can I easily access it, and what about the entire research software engineering community? This kind of response shuts down the conversation and makes the developer (myself for example) realize that the person I’m talking to is not interested in thinking about how to inspire change.I’ve been really frustrated recently with mentioning even an abstract idea, and getting shut down that “Oh that sounds like this other tool.”For a project to reach this “mention status” it needs to be easy to install or use, and not have a barrier of privilege that you have to work at a certain place or know people. Telling me that there is a solution that requires some convoluted steps and permissions not only implies that it is only available to those in privilege, but that the solution is not well adopted enough or shared enough to be truly a solution for our community.</p><h2 id=\"inspiring-vision\">Inspiring Vision</h2><p>If we aren’t happy with the current state of the world, what are our options? Well, we could leave our current roles to find another state that is more similar to what we want. Personally speaking, I haven’t hit that point quite yet. I want to try my hardest to formulate a vision for how I want the world to be, and then find opportunity to work on it from where I am. The wisdom here is that no specific role is perfect, and optimally we should place ourself somewhere where there are resources and open mindedness for change. it’s up to us to extend our influence as best we can to help drive some possible future. If you try that and fail? At least you tried.</p><p>These are the things that are hard. I am going to try harder to have them be the focus of my thinking about the future. I want to make them easier. I’m starting to realize that possibly the reality is that I should think beyond the constraints of HPC, and more toward the kind of infrastructure that I want, and thenfigure out how to slowly integrate it as a part of our culture too. We can start with a core vision for a future that we want, and thenslowly build up tooling and community around that.</p><p>Happy Friday, friends!</p>",
            "url": "https://hpc.social/personal-blog/2022/things-that-are-hard/",
            
            
            
            
            
            "date_published": "2022-01-07T12:30:00-07:00",
            "date_modified": "2022-01-07T12:30:00-07:00",
            
                "author": "Vanessasaurus"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2021/researcher-s-time-has-value-too/",
            "title": "Researcher's Time Has Value, Too",
            "summary": null,
            "content_text": "..And Researchers Value Their Time(Note: This post is adapted from #102 of the Research Computing Teams Newsletter)If you followed HPC twitter in late 2021 at all, you will have seen a heartfelt thread by a well-known research software developer, one who was a key contributor to the Singularity project among others, lamenting the frankly appalling state of developer productivity in HPC - both in what tools exist, and support for them (and other tools for developers) at academic centres.  A lot of people chimed into the discussion, including one of the leading developers of the PetSC project, embedded software developers, some key people at big computing centres, all agreeing that there was a problem, but typically zooming in on one or another particular technical or procedural issue and not coming to any conclusion.I think the issue is a lot bigger than HPC software development workflows - it comes up in too many contexts to be about specific technical issues of running CI/CD pipelines on fixed infrastructure.  The only people to identify the correct underlying issue, in my opinion, were people with experience of both academia and the private sector, such as Brendan Bouffler at AWS:Too much reliance on “free” labour - postgrads and post docs who, invariably, decide that burning their time being mechanical turks for their “superiors” just sucks, so they come and work for us. And since we pay $$, we’re not gonna waste them on things that software can do.&mdash; Brendan Bouffler☁️ 🏳️‍🌈 (@boofla) November 20, 2021The same argument got made by R&amp;D research staff in the private sector.  Their time actually has value; as a result, it gets valued.In academic research computing, partly because of low salaries — especially for the endless stream of trainees — but also because we typically provide research computing systems for free, we tend to put zero value on people’s time.  Thus our “lowest-cost” approach definitely does not apply to researcher or trainee effort. If researchers have to jump through absurd hoops to get or renew their accounts, or have to distort their workflows to fit one-size-fits-all clusters and queueing systems, or postdocs have to spend hours of work by hand every month hand because tools to automate some of that work would cost $500, well, what do they expect, right?It’s not that this is an indefensible position to take, but one can’t take this position and act surprised when researchers who can afford to are seriously investigating taking their projects into the commercial cloud even though it costs 2x as much.  It turns out that people’s time is worth quite a lot to them, and is certainly worth some money.  If we were to let researchers spend their research computing and data money wherever they pleased, I think we’d find that significantly less than 100% of researchers would use “lowest price possible” as their sole criterion for choosing providers.  Core facilities like animal facilities, sequencing centres, and microscopy centres compete on dimensions other than being the cheapest option available.To be sure, there are process issues in academia which exacerbates the tendency to see people’s time as valueless - rules about capital vs operating costs, for instance - but those rules aren’t a law of nature.  If we were paying people in academia what they pay in tech, administration would suddenly discover some additional flexibility in the thresholds and criteria for considering something a capital expense if it meant we could be a bit more parsimonious with people’s time.Until then, one can’t be too surprised when the most talented and ambitious staff get routinely poached by the private sector, and when research groups start considering service providers that cost more but respect their time.",
            "content_html": "<h2 id=\"and-researchers-value-their-time\">..And Researchers Value Their Time</h2><p>(Note: This post is adapted from <a href=\"https://www.researchcomputingteams.org/newsletter_issues/0102\">#102</a> of the <a href=\"https://www.researchcomputingteams.org\">Research Computing Teams Newsletter</a>)</p><p>If you followed HPC twitter in late 2021 at all, you will have seen a <a href=\"https://twitter.com/vsoch/status/1461908217223528448\">heartfelt thread</a> by a well-known research software developer, one who was a key contributor to the Singularity project among others, lamenting the frankly appalling state of developer productivity in HPC - both in what tools exist, and support for them (and other tools for developers) at academic centres.  A <strong>lot</strong> of people <a href=\"https://twitter.com/HPC_Guru/status/1462070286983983108\">chimed into the discussion</a>, including <a href=\"https://twitter.com/five9a2/status/1462137427527675918\">one of the leading developers of the PetSC project</a>, embedded software developers, some key people at big computing centres, all agreeing that there was a problem, but typically zooming in on one or another particular technical or procedural issue and not coming to any conclusion.</p><p>I think the issue is a lot bigger than HPC software development workflows - it comes up in too many contexts to be about specific technical issues of running CI/CD pipelines on fixed infrastructure.  The only people to identify the correct underlying issue, in my opinion, were people with experience of both academia and the private sector, such as Brendan Bouffler at AWS:</p><blockquote class=\"twitter-tweet\"><p dir=\"ltr\" lang=\"en\">Too much reliance on “free” labour - postgrads and post docs who, invariably, decide that burning their time being mechanical turks for their “superiors” just sucks, so they come and work for us. And since we pay $$, we’re not gonna waste them on things that software can do.</p>&mdash; Brendan Bouffler☁️ 🏳️‍🌈 (@boofla) <a href=\"https://twitter.com/boofla/status/1462099372255203346?ref_src=twsrc%5Etfw\">November 20, 2021</a></blockquote><p>The same argument got made by R&amp;D research staff in the private sector.  Their time actually has value; as a result, it gets valued.</p><p>In academic research computing, partly because of low salaries — especially for the endless stream of trainees — but also because we typically provide research computing systems for free, we tend to put zero value on people’s time.  Thus our “lowest-cost” approach definitely does not apply to researcher or trainee effort. If researchers have to jump through absurd hoops to get or renew their accounts, or have to distort their workflows to fit one-size-fits-all clusters and queueing systems, or postdocs have to spend hours of work by hand every month hand because tools to automate some of that work would cost $500, well, what do they expect, right?</p><p>It’s not that this is an indefensible position to take, but one can’t take this position <em>and</em> act surprised when researchers who can afford to are seriously investigating taking their projects into the commercial cloud even though it costs 2x as much.  It turns out that people’s time is worth quite a lot to them, and is certainly worth some money.  If we were to <a href=\"https://www.dursi.ca/post/research-computing-funding-to-researchers\">let researchers spend their research computing and data money wherever they pleased</a>, I think we’d find that significantly less than 100% of researchers would use “lowest price possible” as their sole criterion for choosing providers.  Core facilities like animal facilities, sequencing centres, and microscopy centres compete on dimensions other than being the cheapest option available.</p><p>To be sure, there are process issues in academia which exacerbates the tendency to see people’s time as valueless - rules about capital vs operating costs, for instance - but those rules aren’t a law of nature.  If we were paying people in academia <a href=\"https://www.levels.fyi/\">what they pay in tech</a>, administration would suddenly discover some additional flexibility in the thresholds and criteria for considering something a capital expense if it meant we could be a bit more parsimonious with people’s time.</p><p>Until then, one can’t be too surprised when the most talented and ambitious staff get routinely poached by the private sector, and when research groups start considering service providers that cost more but respect their time.</p>",
            "url": "https://hpc.social/personal-blog/2021/researcher-s-time-has-value-too/",
            
            
            
            
            
            "date_published": "2021-11-23T00:00:00-07:00",
            "date_modified": "2021-11-23T00:00:00-07:00",
            
                "author": "Jonathan Dursi's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2021/ceph-crimson-2021-q3-project-update/",
            "title": "Ceph Crimson 2021 Q3 Project Update",
            "summary": null,
            "content_text": "This is the first time we’re seeing Bluestore in Crimson beating Bluestore in Classic in some (low core count) tests.  Starting to see lower tail latency as well which is a really good sign.  Top end performance will be contingent on multi-reactor support though.  Slides available here.",
            "content_html": "<p>This is the first time we’re seeing Bluestore in Crimson beating Bluestore in Classic in some (low core count) tests.  Starting to see lower tail latency as well which is a really good sign.  Top end performance will be contingent on multi-reactor support though.  Slides available <a href=\"https://docs.google.com/presentation/d/1eydyAFKRea8n-VniQzXKW8qkKM9GLVMJt2uDjipJjQA/edit?usp=sharing\">here</a>.</p>",
            "url": "https://hpc.social/personal-blog/2021/ceph-crimson-2021-q3-project-update/",
            
            
            
            
            
            "date_published": "2021-11-22T00:00:00-07:00",
            "date_modified": "2021-11-22T00:00:00-07:00",
            
                "author": "Mark Nelson's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2021/iops-are-dumb/",
            "title": "IOPS are dumb",
            "summary": null,
            "content_text": "This post is a long-form dump of some thoughts I've had while testing all-flash file systems this past year, and bits of this appear in a presentation and paper I'm presenting at PDSW'21&nbsp;about new benchmarking techniques for testing all-flash file systems.\"How many IOPS do you need?\"I'm often asked this by storage vendors, and the question drives me a little bonkers.&nbsp; I assume they ask it because their other customers bring them black-and-white IOPS requirements, but I argue that anyone would be hard-pressed to explain the scientific value of one I/O operation (versus one gigabyte) if ever called on it.&nbsp; And yet, IOPS are undeniably important; the illustrious Rob Ross devoted a whole slide dedicated to this at a recent ASCAC meeting:Rob Ross' perspective on why IOPS are now important for HPC I/OI agree with all of Rob's bullets and yet I disagree with the title of his slide; IOPS are dumb, and yet ignoring them when designing a performance-optimized parallel file system is even more dumb in contemporary times.&nbsp; So let's talk about the grey area in between that creates this dichotomy.First, bandwidth is pretty dumbIf there's one constant in HPC, it's that everyone hates I/O.&nbsp; And there's a good reason: it's a waste of time because every second you wait for I/O to complete is a second you aren't doing the math that led you to use a supercomputer in the first place.&nbsp; I/O is the time you are doing zero computing amidst a field called \"high performance computing.\"That said, everyone appreciates the product of I/O--data.&nbsp; I/O is a necessary part of preserving the results of your calculation, so nobody ever says they wish there was no I/O.&nbsp; Instead, infinitely fast I/O is what people want since it implies that 100% of a scientist's time using an HPC is spent actually performing computations while still&nbsp;preserving the results of that computation after the job has completed.Peeling back another layer of that onion, the saved results of that computation--data--has intrinsic value.&nbsp; In a typical simulation or data analysis, every byte of input or output is typically the hard-earned product of a lot of work performed by a person or machine, and it follows that if you want to both save a lot of bytes but want to spend as little time as possible performing I/O, the true value of a parallel storage system's performance is in how many bytes per second it can read or write.&nbsp; At a fundamental level, this is why I/O performance has long been gauged in terms of megabytes per second, gigabytes per second, and now terabytes per second.&nbsp; To the casual observer, a file system that can deliver 100 GB/s is more valuable than a file system that can deliver only 50 GB/s assuming all things are equal for this very reason.&nbsp; Easy.This singular metric of storage system \"goodness\" quickly breaks down once you start trying to set expectations around it though.&nbsp; For example, let's say your HPC job generates 21 TB of valuable data that must be stored, and it must be stored so frequently that we really can't tolerate more than 30 seconds writing that data out before we start feeling like \"too much time\" is being spent on I/O instead of computation.&nbsp; This turns out to be 700 GB/s--a rather arbitrary choice since that 30 seconds is a matter of subjectivity, but one that reflects the value of your 21 TB and the value of your time.&nbsp; It should follow that any file system that claims 700 GB/s of write capability should meet your requirements, and any vendor who can deliver such a system should get your business, right?Of course not.&nbsp; It's no secret that obtaining those hero bandwidths, much like obtaining Linpack-level FLOPS, requires you (the end-user) to perform I/O in exactly the right way.&nbsp; In the case of the aforementioned 700 GB/s file system, this meansHaving each MPI process write to its own file (a single shared file will get slowed down by file system lock traffic)Writing 4 MiB at a time (to exactly match the size of the network transmission buffers, remote memory buffers, RAID alignment, ...)Using 4 processes per node (enough parallelism to drive the NIC, but not too much to choke the node)Using 960 nodes (enough parallelism to drive all the file system drives, but not too much to choke the servers)I've never seen a scientific application perform this exact pattern, and consequentially, I don't expect that any scientific application has ever gotten that 700 GB/s of performance from a \"700 GB/s file system\" in practice.&nbsp; In that sense, this 700 GB/s bandwidth metric is pretty dumb since nobody actually achieves its rated performance. Of course, that hasn't prevented me from saying&nbsp;these same dumb things&nbsp;when I stump for file systems. &nbsp;The one saving grace of using bandwidth as a meaningful metric of I/O performance, though, is that&nbsp;I/O patterns are a synthetic construct&nbsp;and can be squished, stretched, and reshaped without affecting the underlying scientific data being transmitted.The value of data is in its contents, not the way it is arranged or accessed.&nbsp; There's no intrinsic scientific reason why someone should or shouldn't read their data 4 MiB at a time as long as the bits eventually get to the CPU that will perform calculations on it in the correct order.&nbsp; The only reason HPC users perform nice, 1 MiB-aligned reads and writes is because they learn (either in training or on the streets) that randomly reading a few thousand bytes at a time is very slow and works against their own interests of minimizing I/O time.&nbsp; &nbsp;This contrasts sharply with the computing side of HPC where the laws of physics generally dictate the equations that must be computed, and the order in which those computations happen dictates whether the final results accurately model some physical process or just spit out a bunch of unphysical garbage results.Because I/O patterns are not intrinsically valuable, we are free to rearrange them to best suit the strengths and weaknesses of a storage system to maximize the GB/s we can get out of it.&nbsp; This is the entire foundation of MPI-IO, which receives I/O patterns that are convenient for the physics being simulated and reorders them into patterns that are convenient for the storage system.&nbsp; So while saying a file system can deliver 700 GB/s is a bit disingenuous on an absolute scale, it does indicate what is possible if you are willing to twist your I/O pattern to exactly match the design optimum.But IOPS are particularly dumbIOPS are what happen when you take the value out of a value-based performance metric like bandwidth.&nbsp; Rather than expressing how many valuable bytes a file system can move per second, IOPS express how many arbitrary I/O operations a file system can service per second.&nbsp; And since the notion of an \"I/O operation\" is completely synthetic and can be twisted without compromising the value of the underlying data, you might already see why IOPS are a dumb metric of performance.&nbsp; They measure how quickly a file system can do something meaningless, where that meaningless thing (an I/O operation) is itself a function of the file system.&nbsp; It's like saying you can run a marathon at five steps per second--it doesn't actually indicate how long it will take you to cover the twenty six miles.IOPS as a performance measure was relatively unknown to HPC for most of history.&nbsp; Until 2012, HPC storage was dominated by hard drives which which only delivered high-value performance for large, sequential reads and writes and the notion of an \"IOP\" was antithetical to performance.&nbsp; The advent of flash introduced a new dimension of performance in its ability to read and write a lot of data at discontiguous (or even random) positions within files or across entire file systems.&nbsp; Make no mistake: you still read and write more bytes per second (i.e., get more value) from flash with a contiguous I/O pattern.&nbsp; Flash just raised the bottom end of performance in the event that you are unable or unwilling to contort your application to perform I/O in a way that is convenient for your storage media.To that end, when a vendor advertises how many IOPS they can deliver, they really are advertising how many discontiguous 4 KiB reads or writes they can deliver under the worst-case I/O pattern (fully random offsets).&nbsp; You can convert a vendor's IOPS performance back into a meaningful value metric simply by multiplying it by 4 KiB; for example, I've been presenting a slide that claims I measured 29,000 write IOPS and 1,400 read IOPS from a single ClusterStor E1000 OST array:Performance measurements of a single ClusterStor E1000 NVMe Lustre OSTIn reality, I was able to write data at 0.12 GB/s and read data at 5.7 GB/s, and stating these performance metrics as IOPS makes it clear that these data rates reflect the worst-case scenario of tiny I/Os happening at random locations rather than the best-case scenario of sequential I/Os which can happen at 27 GB/s and 41 GB/s, respectively.Where IOPS get particularly stupid is when we try to cast them as some sort of hero number analogous to the 700 GB/s bandwidth metric discussed above.&nbsp; Because IOPS reflect a worst-case performance scenario, no user should ever be asking \"how can I get the highest IOPS\" because they'd really be asking \"how can I get the best, worst-case performance?\"&nbsp; Relatedly, trying to measure the IOPS capability of a storage system gets very convoluted because it often requires twisting your I/O pattern in such unrealistic ways that heroic effort is required to get such terrible performance.&nbsp; At some point, every I/O performance engineer should find themselves questioning why they are putting so much time into defeating every optimization the file system implements to avoid this worst-case scenario.To make this a little more concrete, let's look at this slide I made in 2019 to discuss the IOPS projections of this exact same ClusterStor E1000 array:Projected performance of a ClusterStor E1000 NVMe Lustre OST based on a PCIe Gen3 platformSomehow the random read rate went from a projected 600,000 to an astonishing 1,400,000 read IOPS--which one is the correct measure of read IOPS?It turns out that they're both correct; the huge difference in measured read IOPS are the result of the the 600 KIOPS estimate coming from a measurement thatran for a much longer sustained period (180 seconds vs. 69 seconds)used fewer client nodes (21 nodes vs. of 32 nodes)wrote larger files (1,008× 8 GiB files vs. 1,024×&nbsp;384 GiB files)Unlike the IOPS measurements on individual SSDs which are measured using a standard tool (fio with libaio from a single node), there is no standard method for measuring the IOPS of a parallel file system.&nbsp; And just as the hero bandwidth number we discussed above is unattainable by real applications, any standardized IOPS test for a parallel file system would result in a relatively meaningless number.&nbsp; And yes, this includes IO-500; its numbers have little quantitative value if you want to design a parallel file system the right way.So who's to say whether a ClusterStor E1000 OST is capable of 600 kIOPS or 1,400 kIOPS?&nbsp; I argue that 1,400 kIOPS is more accurate since I/O is bursty and a three-minute-long burst of completely random reads is less likely than a one-minute long one on a production system.&nbsp; If I worked for a vendor though, I'm sure this would be taken to be a dishonest marketing number since it doesn't reflect an indefinitely sustainable level of performance.&nbsp; And perhaps courageously, the official Cray ClusterStor E1000 data sheet doesn't even wade into these waters and avoids quoting any kind of IOPS performance expectation.&nbsp; Ultimately, the true value of the random read capability is the bandwidth achievable by all of the most random workloads that will realistically be run at the same time on a file system.&nbsp; Good luck figuring that out.Write IOPS are really dumbAs I said at the outset, I cannot disagree with any of the bullets in the slide Rob presented at ASCAC.&nbsp; That first one is particularly salient--there are a new class of HPC workloads, particularly in AI, whose primary purpose is to randomly sample large datasets to train statistical models.&nbsp; If these datasets are too large to fit into memory, you cannot avoid some degree of random read I/O without introducing biases into your weights.&nbsp; For this reason, there is legitimate need for HPC to demand high random read performance from their file systems.&nbsp; Casting this requirement in terms of 4 KiB random read rates to have a neat answer to the \"how many IOPS do you need\" question is dubious, but whatever.&nbsp; There's little room for intellectual purity in HPC.The same can't be said for random write rates.&nbsp; Write IOPS are a completely worthless and misleading performance metric in parallel file systems.In most cases, HPC applications approximate some aspect of the physical world, and mathematics and physics were created to describe this physical world in a structured way.&nbsp; Whether you're computing over atoms, meshes, or matrices, there is structure to the data you are writing out and the way your application traverses memory to write everything out.&nbsp; You may not write data out in a perfectly ordered way; you may have more atoms on one MPI process than another, or you may be traversing an imbalanced graph.&nbsp; But there is almost always enough structure to scientific data to squish it into a non-random I/O pattern using middleware like MPI-IO.Granted, there are a few workloads where this is not true.&nbsp; Out-of-core sorting of short-read DNA sequences&nbsp;and in-place updates of telescope mosaics are two workloads that come to mind where you don't know where to write a small bit of data until you've computed on that small bit of data.&nbsp; In both these cases though, the files are never read and written at the same time, meaning that these random-ish writes can be cached in memory, reordered to be less random, and written out to the file asynchronously.&nbsp; And the effect of write-back caching on random write workloads is staggering.To illustrate this, consider three different ways in which IOR can be run against an all-NVMe file system to measure random 4 KiB writes:In the naïve case, we just write 4 KiB pages at random locations within a bunch of files (one file per MPI process) and report what IOR tells us the write IOPS were at the end.&nbsp; This includes only the time spent in write(2) calls.In the case where we include fsync, we call fsync(2) at the end of all the writes and include the time it takes to return along with all the time spent in write(2).In the O_DIRECT case, we open the file with direct I/O to completely bypass the client write-back cache and ensure that write(2) doesn't return until the data has been written to the file system servers.These seemingly minor changes result in write IOPS rates that differ by over 30x:Random write IOPS measured using IOR on an all-NVMe parallel file systemAgain we ask: which one is the right value for the file system's write IOPS performance?If we split apart the time spent in each phase of this I/O performance test, we immediately see that the naïve case is wildly deceptive:Breakdown of time spent in I/O calls for 4K random write IOR workloadThe reason IOR reported a 2.6 million write IOPS rate is because all those random writes actually got cached in each compute node's memory, and I/O didn't actually happen until the file was closed and all cached dirty pages were flushed.&nbsp; At the point this happens, the cache flushing process doesn't result in random writes anymore; the client reordered all of those cached writes into large, 1 MiB network requests and converted our random write workload into a sequential write workload.The same thing happens in the case where we include fsync; the only difference is that we're including the time required to flush caches in the denominator of our IOPS measurement.&nbsp; Rather frustratingly, we actually stopped issuing write(2) calls after 45 seconds, but so many writes were cached in memory during those 45 seconds that it took almost 15 minutes to reorder and write them all out during that final fsync and file close.&nbsp; What should've been 45 seconds of random writes to the file system turned into 45 seconds of random writes to memory and 850 seconds of sequential writes to the file system.The O_DIRECT case is the most straightforward since we don't cache any writes, and every one of our random writes from the application turns into a random write out to the file system.&nbsp; This cuts our measured IOPS almost in half, but otherwise leaves no surprises when we expect to only write for 45 seconds. &nbsp;Of course, we wrote far fewer bytes overall in this case since the effective bytes/sec during this 45 seconds was so low.Based on all this, it's tempting to say that the O_DIRECT case is the correct way to measure random write IOPS since it avoids write-back caches--but is it really?&nbsp; In the rare case where an application intentionally does random writes (e.g., out-of-core sort or in-place updates), what are the odds that two MPI processes on different nodes will try to write to the same part of the same file at the same time and therefore trigger cache flushing?&nbsp; Perhaps more directly, what are the odds that a scientific application would be using O_DIRECT and random writes at the same time?&nbsp; Only the most masochistic HPC user would ever purposely do something like this since it results in worst-case I/O performance; it doesn't take long for a user to realize this I/O pattern is terrible and reformulating their I/O pattern would increase their productive use of their supercomputer.So if no user in their right mind does truly unbuffered random writes, what's the point in measuring it in the first place?&nbsp; There is none.&nbsp; Measuring write IOPS is dumb.&nbsp; Using O_DIRECT to measure random write performance is dumb, and measuring write IOPS through write-back cache, while representative of most users' actual workloads, isn't actually doing 4K random I/Os and therefore isn't even measuring IOPS.Not all IOPS are always dumbThis all being said, measuring IOPS can be valuable in contexts outside of parallel file systems.&nbsp; Two cases come to mind where measuring IOPS can be a rational yard stick.1. Serving up LUNs to containers and VMsBy definition, infrastructure providers shouldn't be responsible for the applications that run inside black-box containers and VMs because they are providing storage infrastructure (block devices) and not storage services (file systems).&nbsp; Blocks in and blocks out are measured in IOPS, so the fit is natural.&nbsp; That said, HPC users care about file systems (that is, scientific applications do not perform I/O using SCSI commands directly!), so worrying about LUN performance isn't meaningful in the HPC context.2. Measuring the effect of many users doing many thingsWhile individual HPC workloads rarely perform random I/Os on purpose, if you have enough users doing many small tasks all at once, the file system itself sees a workload that approaches something random.&nbsp; The more, small, independent tasks running parallel and the farther back you stand from the overall I/O load timeline, the more random it looks.&nbsp; So, I argue that it is fair to measure the IOPS of a parallel file system for the purposes of measuring how much abuse a file system can take before it begins to impact everybody.Take, for example, these IOPS scaling I measured on a small all-flash file system using IOR:Scale-up IOPS benchmarking to demonstrate the saturation point of an all-flash file system&lt;div&gt;It looks like it takes about 4,096 concurrent random readers or writers to max out the file system.  This alone isn’t meaningful until you consider what this means in the context of the whole compute and storage platform.&lt;/div&gt;What fraction of the cluster's compute nodes corresponds to 4096 cores?&nbsp; If you've got, say, 728 dual-socket 64-core AMD Epyc processors, it would only take 32 compute nodes to max out this file system.&nbsp; And if another user wanted to use any of the remaining 696 compute nodes to, say, run a Python script that needed to read in random packages scattered across the file system, there would be no remaining IOPS capacity left at this point, and everyone would experience perceptible lag.Of course, this is the most extreme case--purely random IOPS--but you can measure the IOPS that a real workload does generate on the server side when, say, sampling a deep learning training dataset. With this, you can then figure out how much headroom that application leaves for every other random-ish workload that needs to run on the same system.Once you realize that a lot of the unglamorous parts of of scientific computing--reading dotfiles when you log in, loading shared objects when you launch a dynamically linked executable, or even just editing source code--are full of random-like reads, you can establish a quantitative basis for figuring out how badly an IOPS-intensive data analysis application may affect everyone else's interactive accesses on the same file system.This is not to say that we can easily answer the question of \"How many IOPS do you need?\" though.&nbsp; How many IOPS a workload can drive is not how many IOPS that workload needs--it's really how fast it can compute before it has run out of data to process and needs to read more in.&nbsp; The faster your compute nodes, generally, the more data they can consume.&nbsp; They still want all the IOPS you can give them so they can spend as much time computing (and not waiting for I/O) as possible, and how many IOPS your application can drive is a function of how quickly it runs given the full stack between it and the storage, including CPU, memory, and networking.If everything is dumb, now what?Give up trying to reduce I/O performance down to a single IOPS number, because it's two degrees away from being useful.&nbsp; Bandwidth is a better metric in that it's only one degree away from what actually matters, but at the end of the day, the real metric of I/O performance is how much time an application has to wait on I/O before it can resume performing meaningful computations.&nbsp; Granted, most storage vendors will give you a blank stare if you take this angle to them; telling them that your application spends 50% of its time waiting on I/O isn't going to get you a better file system from a storage company alone, so think about what the real problem could be.Is the application doing I/O in a pattern (random or otherwise) that prevents the storage system from delivering as many bytes/second as possible?&nbsp; If so, ask your vendor for a storage system that delivers more bandwidth to a wider range of I/O patterns than just perfectly aligned 1 MiB reads and writes.Is the storage system already running as well as it can, but it only takes a few compute nodes to max it out?&nbsp; If so, your storage system is too small relative to your compute system, and you should ask your vendor for more servers and drives to scale out.Is the storage system running at 100% CPU even though it's not delivering full bandwidth?&nbsp; Servicing a small I/O requires a lot more CPU than a large I/O since there are fixed computations that have to happen on every read or write regardless of how big it is.&nbsp; Ask your vendor for a better file system that doesn't eat up so much CPU, or ask for more capable servers.Alternatively, if you have a lot of users all doing different things and the file system is giving poor performance to everyone, ask your vendor for a file system with better quality of service.&nbsp; This will ensure that one big job doesn't starve out all the small ones.Is the storage system slow but you don't have the time to figure out why?&nbsp; If so, it sounds like you work for an organization that doesn't actually value data because it's not appropriately staffed.&nbsp; This isn't a storage problem!Ultimately, if solving I/O problems was as easy answering how many IOPS you need, storage wouldn't be the perpetual pain point in HPC that it has been.&nbsp; As with all things in computing, there is no shortcut and the proper way to approach this is by rolling up your sleeves and start ruling out problems.&nbsp; You can (and should!) ask for a lot from your storage vendors--flexibility in delivering bandwidth, CPU-efficient file systems, and quality of service controls are all valid requests when buying storage.&nbsp; But IOPS are not.",
            "content_html": "<div style=\"border: 1px solid black; font-size: x-small; margin-left: 2em; margin-right: 2em; padding: 1em;\">This post is a long-form dump of some thoughts I've had while testing all-flash file systems this past year, and bits of this appear in a <a href=\"http://www.pdsw.org/index.shtml\">presentation and paper I'm presenting at PDSW'21</a>&nbsp;about new benchmarking techniques for testing all-flash file systems.</div><p>\"How many IOPS do you need?\"</p><p>I'm often asked this by storage vendors, and the question drives me a little bonkers.&nbsp; I assume they ask it because their other customers bring them black-and-white IOPS requirements, but I argue that anyone would be hard-pressed to explain the scientific value of one I/O operation (versus one gigabyte) if ever called on it.&nbsp; And yet, IOPS are undeniably important; the illustrious Rob Ross devoted a whole slide dedicated to this at a <a href=\"https://science.osti.gov/ascr/ascac/Meetings/202109\">recent ASCAC meeting</a>:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://1.bp.blogspot.com/-uoZq9awp-3E/YVS3anWGgpI/AAAAAAABWsw/tb12XvWtTScjd42nIscFJ-6U7Dr3E_TLQCLcBGAsYHQ/s2048/rob-ross-ascac-slide.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"Rob Ross' perspective on why IOPS are now important for HPC I/O\" border=\"0\" height=\"226\" src=\"https://1.bp.blogspot.com/-uoZq9awp-3E/YVS3anWGgpI/AAAAAAABWsw/tb12XvWtTScjd42nIscFJ-6U7Dr3E_TLQCLcBGAsYHQ/w400-h226/rob-ross-ascac-slide.png\" title=\"Rob Ross' perspective on why IOPS are now important for HPC I/O\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><b><span style=\"font-size: x-small;\">Rob Ross' perspective on why IOPS are now important for HPC I/O</span></b></div><p>I agree with all of Rob's bullets and yet I disagree with the title of his slide; IOPS are dumb, and yet ignoring them when designing a performance-optimized parallel file system is even more dumb in contemporary times.&nbsp; So let's talk about the grey area in between that creates this dichotomy.<span></span></p><p></p><h2 style=\"text-align: left;\">First, bandwidth is pretty dumb</h2><p>If there's one constant in HPC, it's that everyone hates I/O.&nbsp; And there's a good reason: it's a waste of time because every second you wait for I/O to complete is a second you aren't doing the math that led you to use a supercomputer in the first place.&nbsp; I/O is the time you are doing zero computing amidst a field called \"high performance computing.\"</p><p>That said, everyone appreciates the product of I/O--data.&nbsp; I/O is a necessary part of preserving the results of your calculation, so nobody ever says they wish there was no I/O.&nbsp; Instead, infinitely fast I/O is what people want since it implies that 100% of a scientist's time using an HPC is spent actually performing computations while still&nbsp;preserving the results of that computation after the job has completed.</p><p>Peeling back another layer of that onion, the saved results of that computation--data--has intrinsic value.&nbsp; In a typical simulation or data analysis, every byte of input or output is typically the hard-earned product of a lot of work performed by a person or machine, and it follows that if you want to both save a lot of bytes but want to spend as little time as possible performing I/O, the true value of a parallel storage system's performance is in how many bytes per second it can read or write.&nbsp; At a fundamental level, this is why I/O performance has long been gauged in terms of megabytes per second, gigabytes per second, and now terabytes per second.&nbsp; To the casual observer, a file system that can deliver 100 GB/s is more valuable than a file system that can deliver only 50 GB/s assuming all things are equal for this very reason.&nbsp; Easy.</p><p>This singular metric of storage system \"goodness\" quickly breaks down once you start trying to set expectations around it though.&nbsp; For example, let's say your HPC job generates 21 TB of valuable data that must be stored, and it must be stored so frequently that we really can't tolerate more than 30 seconds writing that data out before we start feeling like \"too much time\" is being spent on I/O instead of computation.&nbsp; This turns out to be 700 GB/s--a rather arbitrary choice since that 30 seconds is a matter of subjectivity, but one that reflects the value of your 21 TB and the value of your time.&nbsp; It should follow that any <a href=\"https://www.nersc.gov/news-publications/nersc-news/nersc-center-news/2016/cori-supercomputer-now-fully-installed-at-berkeley-lab/\">file system that claims 700 GB/s of write capability</a> should meet your requirements, and any vendor who can deliver such a system should get your business, right?</p><p>Of course not.&nbsp; It's no secret that obtaining those hero bandwidths, much like obtaining Linpack-level FLOPS, requires you (the end-user) to perform I/O in exactly the right way.&nbsp; In the case of the aforementioned 700 GB/s file system, this means</p><p></p><ol style=\"text-align: left;\"><li>Having each MPI process write to its own file (a single shared file will get slowed down by file system lock traffic)</li><li>Writing 4 MiB at a time (to exactly match the size of the network transmission buffers, remote memory buffers, RAID alignment, ...)</li><li>Using 4 processes per node (enough parallelism to drive the NIC, but not too much to choke the node)</li><li>Using 960 nodes (enough parallelism to drive all the file system drives, but not too much to choke the servers)</li></ol><p></p><p>I've never seen a scientific application perform this exact pattern, and consequentially, I don't expect that any scientific application has ever gotten that 700 GB/s of performance from a \"700 GB/s file system\" in practice.&nbsp; In that sense, this 700 GB/s bandwidth metric is pretty dumb since nobody actually achieves its rated performance. Of course, that hasn't prevented me from saying&nbsp;these <a href=\"https://storageconference.us/2019/Invited/Lockwood.slides.pdf\">same</a> <a href=\"https://www.osti.gov/biblio/1798757\">dumb</a> <a href=\"https://hps.vi4io.org/_media/events/2021/iodc21-lockwood.pdf\">things</a>&nbsp;when I stump for file systems. &nbsp;The one saving grace of using bandwidth as a meaningful metric of I/O performance, though, is that&nbsp;<b>I/O patterns are a synthetic construct</b>&nbsp;and can be squished, stretched, and reshaped without affecting the underlying scientific data being transmitted.</p><p>The value of data is in its contents, not the way it is arranged or accessed.&nbsp; There's no intrinsic scientific reason why someone should or shouldn't read their data 4 MiB at a time as long as the bits eventually get to the CPU that will perform calculations on it in the correct order.&nbsp; The only reason HPC users perform nice, 1 MiB-aligned reads and writes is because they learn (either in training or on the streets) that randomly reading a few thousand bytes at a time is very slow and works against their own interests of minimizing I/O time.&nbsp; &nbsp;This contrasts sharply with the computing side of HPC where the laws of physics generally dictate the equations that must be computed, and the order in which those computations happen dictates whether the final results accurately model some physical process or just spit out a bunch of unphysical garbage results.</p><p>Because I/O patterns are not intrinsically valuable, we are free to rearrange them to best suit the strengths and weaknesses of a storage system to maximize the GB/s we can get out of it.&nbsp; This is the entire foundation of MPI-IO, which receives I/O patterns that are convenient for the physics being simulated and reorders them into patterns that are convenient for the storage system.&nbsp; So while saying a file system can deliver 700 GB/s is a bit disingenuous on an absolute scale, it does indicate what is possible if you are willing to twist your I/O pattern to exactly match the design optimum.</p><h2 style=\"text-align: left;\">But IOPS are particularly dumb</h2><p>IOPS are what happen when you take the value out of a value-based performance metric like bandwidth.&nbsp; Rather than expressing how many valuable bytes a file system can move per second, IOPS express how many arbitrary I/O operations a file system can service per second.&nbsp; And since the notion of an \"I/O operation\" is completely synthetic and can be twisted without compromising the value of the underlying data, you might already see why IOPS are a dumb metric of performance.&nbsp; They measure how quickly a file system can do something meaningless, where that meaningless thing (an I/O operation) is itself a function of the file system.&nbsp; It's like saying you can run a marathon at five steps per second--it doesn't actually indicate how long it will take you to cover the twenty six miles.</p><p>IOPS as a performance measure was relatively unknown to HPC for most of history.&nbsp; <a href=\"https://www.sdsc.edu/News%20Items/PR030512_gordon.html\">Until 2012</a>, HPC storage was dominated by hard drives which which only delivered high-value performance for large, sequential reads and writes and the notion of an \"IOP\" was antithetical to performance.&nbsp; The advent of flash introduced a new dimension of performance in its ability to read and write a lot of data at discontiguous (or even random) positions within files or across entire file systems.&nbsp; Make no mistake: you still read and write more bytes per second (i.e., get more value) from flash with a contiguous I/O pattern.&nbsp; Flash just raised the bottom end of performance in the event that you are unable or unwilling to contort your application to perform I/O in a way that is convenient for your storage media.</p><p>To that end, when a vendor advertises how many IOPS they can deliver, they really are advertising how many discontiguous 4 KiB reads or writes they can deliver under the worst-case I/O pattern (fully random offsets).&nbsp; You can convert a vendor's IOPS performance back into a meaningful value metric simply by multiplying it by 4 KiB; for example, I've been presenting a slide that claims I measured <a href=\"https://www.osti.gov/biblio/1798757\">29,000 write IOPS and 1,400 read IOPS from a single ClusterStor E1000 OST array</a>:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://1.bp.blogspot.com/-Aq07XkQ1A1U/YVU95I3cvwI/AAAAAAABWtA/2Z57P80DSeoWxeS2dRP42SQUlxaAjas0gCLcBGAsYHQ/s2048/Screen%2BShot%2B2021-09-29%2Bat%2B21.32.04.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"Performance measurements of a single ClusterStor E1000 NVMe Lustre OST\" border=\"0\" height=\"206\" src=\"https://1.bp.blogspot.com/-Aq07XkQ1A1U/YVU95I3cvwI/AAAAAAABWtA/2Z57P80DSeoWxeS2dRP42SQUlxaAjas0gCLcBGAsYHQ/w400-h206/Screen%2BShot%2B2021-09-29%2Bat%2B21.32.04.png\" title=\"Performance measurements of a single ClusterStor E1000 NVMe Lustre OST\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><b><span style=\"font-size: x-small;\">Performance measurements of a single ClusterStor E1000 NVMe Lustre OST</span></b></div><p>In reality, I was able to write data at 0.12 GB/s and read data at 5.7 GB/s, and stating these performance metrics as IOPS makes it clear that these data rates reflect the worst-case scenario of tiny I/Os happening at random locations rather than the best-case scenario of sequential I/Os which can happen at 27 GB/s and 41 GB/s, respectively.</p><p>Where IOPS get particularly stupid is when we try to cast them as some sort of hero number analogous to the 700 GB/s bandwidth metric discussed above.&nbsp; Because IOPS reflect a worst-case performance scenario, no user should ever be asking \"how can I get the highest IOPS\" because they'd really be asking \"how can I get the best, worst-case performance?\"&nbsp; Relatedly, trying to measure the <i>IOPS capability</i> of a storage system gets very convoluted because it often requires twisting your I/O pattern in such unrealistic ways that heroic effort is required to get such terrible performance.&nbsp; At some point, every I/O performance engineer should find themselves questioning why they are putting so much time into defeating every optimization the file system implements to avoid this worst-case scenario.</p><p>To make this a little more concrete, let's look at this <a href=\"https://www.lustre.org/wp-content/uploads/SC19LustreBoF_All.pdf\">slide I made in 2019 to discuss the IOPS projections of this exact same ClusterStor E1000 array</a>:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://1.bp.blogspot.com/-xpPJ4SVoNcQ/YVVGQ4qV4WI/AAAAAAABWtI/Vpl-loGSSsomakJR69dc3xReU-0D_2AzgCLcBGAsYHQ/s2048/Screen%2BShot%2B2021-09-29%2Bat%2B22.01.19.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"Projected performance of a ClusterStor E1000 NVMe Lustre OST based on a PCIe Gen3 platform\" border=\"0\" height=\"226\" src=\"https://1.bp.blogspot.com/-xpPJ4SVoNcQ/YVVGQ4qV4WI/AAAAAAABWtI/Vpl-loGSSsomakJR69dc3xReU-0D_2AzgCLcBGAsYHQ/w400-h226/Screen%2BShot%2B2021-09-29%2Bat%2B22.01.19.png\" title=\"Projected performance of a ClusterStor E1000 NVMe Lustre OST based on a PCIe Gen3 platform\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><b><span style=\"font-size: x-small;\">Projected performance of a ClusterStor E1000 NVMe Lustre OST based on a PCIe Gen3 platform</span></b></div><p>Somehow the random read rate went from a projected 600,000 to an astonishing 1,400,000 read IOPS--which one is the correct measure of read IOPS?</p><p>It turns out that they're <i>both</i> correct; the huge difference in measured read IOPS are the result of the the 600 KIOPS estimate coming from a measurement that</p><ol style=\"text-align: left;\"><li>ran for a much longer sustained period (180 seconds vs. 69 seconds)</li><li>used fewer client nodes (21 nodes vs. of 32 nodes)</li><li>wrote larger files (1,008× 8 GiB files vs. 1,024×&nbsp;384 GiB files)</li></ol><p>Unlike the IOPS measurements on individual SSDs which are measured using a standard tool (<a href=\"https://github.com/axboe/fio\">fio</a> with <a href=\"https://pagure.io/libaio\">libaio</a> from a single node), there is no standard method for measuring the IOPS of a parallel file system.&nbsp; And just as the hero bandwidth number we discussed above is unattainable by real applications, any standardized IOPS test for a parallel file system would result in a relatively meaningless number.&nbsp; And yes, this includes IO-500; <a href=\"https://www.glennklockwood.com/benchmarks/io500.html#interpreting-results\">its numbers have little quantitative value</a> if you want to design a parallel file system the right way.</p><p>So who's to say whether a ClusterStor E1000 OST is capable of 600 kIOPS or 1,400 kIOPS?&nbsp; I argue that 1,400 kIOPS is more accurate since I/O is bursty and a three-minute-long burst of completely random reads is less likely than a one-minute long one on a production system.&nbsp; If I worked for a vendor though, I'm sure this would be taken to be a dishonest marketing number since it doesn't reflect an indefinitely sustainable level of performance.&nbsp; And perhaps courageously, the <a href=\"https://www.hpe.com/psnow/doc/PSN1012842049INEN.pdf\">official Cray ClusterStor E1000 data sheet</a> doesn't even wade into these waters and avoids quoting any kind of IOPS performance expectation.&nbsp; Ultimately, the true value of the random read capability is the bandwidth achievable by all of the most random workloads that will realistically be run at the same time on a file system.&nbsp; Good luck figuring that out.</p><h2 style=\"text-align: left;\">Write IOPS are <i>really</i> dumb</h2><p>As I said at the outset, I cannot disagree with any of the bullets in the slide Rob presented at ASCAC.&nbsp; That first one is particularly salient--there <i>are</i> a new class of HPC workloads, particularly in AI, whose primary purpose is to randomly sample large datasets to train statistical models.&nbsp; If these datasets are too large to fit into memory, you cannot avoid some degree of random read I/O without introducing biases into your weights.&nbsp; For this reason, there is legitimate need for HPC to demand high random read performance from their file systems.&nbsp; Casting this requirement in terms of 4 KiB random read rates to have a neat answer to the \"how many IOPS do you need\" question is dubious, but whatever.&nbsp; There's little room for intellectual purity in HPC.</p><p>The same can't be said for random write rates.&nbsp; Write IOPS are a completely worthless and misleading performance metric in parallel file systems.</p><p>In most cases, HPC applications approximate some aspect of the physical world, and mathematics and physics were created to describe this physical world in a structured way.&nbsp; Whether you're computing over atoms, meshes, or matrices, there is structure to the data you are writing out and the way your application traverses memory to write everything out.&nbsp; You may not write data out in a perfectly ordered way; you may have more atoms on one MPI process than another, or you may be traversing an imbalanced graph.&nbsp; But there is almost always enough structure to scientific data to squish it into a non-random I/O pattern using middleware like MPI-IO.</p><p>Granted, there are a few workloads where this is not true.&nbsp; <a href=\"https://www.sdsc.edu/Events/ipp_webinars/large_scale_genomics.pdf\">Out-of-core sorting of short-read DNA sequences</a>&nbsp;and <a href=\"http://dx.doi.org/10.1016/j.future.2017.12.022\">in-place updates of telescope mosaics</a> are two workloads that come to mind where you don't know where to write a small bit of data until you've computed on that small bit of data.&nbsp; In both these cases though, the files are never read and written at the same time, meaning that these random-ish writes can be cached in memory, reordered to be less random, and written out to the file asynchronously.&nbsp; And the effect of write-back caching on random write workloads is staggering.</p><p>To illustrate this, consider three different ways in which IOR can be run against an all-NVMe file system to measure random 4 KiB writes:</p><p></p><ul style=\"text-align: left;\"><li>In the <b>naïve</b> case, we just write 4 KiB pages at random locations within a bunch of files (one file per MPI process) and report what IOR tells us the write IOPS were at the end.&nbsp; This includes only the time spent in write(2) calls.</li><li>In the case where we <b>include fsync</b>, we call fsync(2) at the end of all the writes and include the time it takes to return along with all the time spent in write(2).</li><li>In the <b>O_DIRECT</b> case, we open the file with direct I/O to completely bypass the client write-back cache and ensure that write(2) doesn't return until the data has been written to the file system servers.</li></ul><div>These seemingly minor changes result in write IOPS rates that differ by over 30x:</div><p></p><p></p><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://1.bp.blogspot.com/-zShKdPu53YE/YVVW2QbVRYI/AAAAAAABWtQ/mReqH6S2lsgF0nhAmqDdlCra7-FQoywWACLcBGAsYHQ/s565/download.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"Random write IOPS measured using IOR on an all-NVMe parallel file system\" border=\"0\" height=\"280\" src=\"https://1.bp.blogspot.com/-zShKdPu53YE/YVVW2QbVRYI/AAAAAAABWtQ/mReqH6S2lsgF0nhAmqDdlCra7-FQoywWACLcBGAsYHQ/w400-h280/download.png\" title=\"Random write IOPS measured using IOR on an all-NVMe parallel file system\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><b><span style=\"font-size: x-small;\">Random write IOPS measured using IOR on an all-NVMe parallel file system</span></b></div><p>Again we ask: which one is the right value for the file system's write IOPS performance?</p><p>If we split apart the time spent in each phase of this I/O performance test, we immediately see that the naïve case is wildly deceptive:</p><p></p><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://1.bp.blogspot.com/-7r9NLXU8Cd8/YVVW9NcK52I/AAAAAAABWtU/hRmBYygTtDUkX1Q6an3iYdbMu68Ni4TMgCLcBGAsYHQ/s565/download-1.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"Breakdown of time spent in I/O calls for 4K random write IOR workload\" border=\"0\" height=\"274\" src=\"https://1.bp.blogspot.com/-7r9NLXU8Cd8/YVVW9NcK52I/AAAAAAABWtU/hRmBYygTtDUkX1Q6an3iYdbMu68Ni4TMgCLcBGAsYHQ/w400-h274/download-1.png\" title=\"Breakdown of time spent in I/O calls for 4K random write IOR workload\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><b><span style=\"font-size: x-small;\">Breakdown of time spent in I/O calls for 4K random write IOR workload</span></b></div><p>The reason IOR reported a 2.6 million write IOPS rate is because all those random writes actually got cached in each compute node's memory, and I/O didn't actually happen until the file was closed and all cached dirty pages were flushed.&nbsp; At the point this happens, the cache flushing process doesn't result in random writes anymore; the client reordered all of those cached writes into large, 1 MiB network requests and converted our random write workload into a sequential write workload.</p><p>The same thing happens in the case where we include fsync; the only difference is that we're including the time required to flush caches in the denominator of our IOPS measurement.&nbsp; Rather frustratingly, we actually stopped issuing write(2) calls after 45 seconds, but so many writes were cached in memory during those 45 seconds that it took almost 15 minutes to reorder and write them all out during that final fsync and file close.&nbsp; What should've been 45 seconds of random writes to the file system turned into 45 seconds of random writes to memory and 850 seconds of sequential writes to the file system.</p><p>The O_DIRECT case is the most straightforward since we don't cache any writes, and every one of our random writes from the application turns into a random write out to the file system.&nbsp; This cuts our measured IOPS almost in half, but otherwise leaves no surprises when we expect to only write for 45 seconds. &nbsp;Of course, we wrote far fewer bytes overall in this case since the effective bytes/sec during this 45 seconds was so low.</p><p>Based on all this, it's tempting to say that the O_DIRECT case is the correct way to measure random write IOPS since it avoids write-back caches--but is it really?&nbsp; In the rare case where an application intentionally does random writes (e.g., out-of-core sort or in-place updates), what are the odds that two MPI processes on different nodes will try to write to the same part of the same file at the same time and therefore trigger cache flushing?&nbsp; Perhaps more directly, what are the odds that a scientific application would be using O_DIRECT <i>and</i> random writes at the same time?&nbsp; Only the most masochistic HPC user would ever purposely do something like this since it results in worst-case I/O performance; it doesn't take long for a user to realize this I/O pattern is terrible and reformulating their I/O pattern would increase their productive use of their supercomputer.</p><p>So if no user in their right mind does truly unbuffered random writes, what's the point in measuring it in the first place?&nbsp; <b>There is none.&nbsp; Measuring write IOPS is dumb</b>.&nbsp; Using O_DIRECT to measure random write performance is dumb, and measuring write IOPS through write-back cache, while representative of most users' actual workloads, isn't actually doing 4K random I/Os and therefore isn't even measuring IOPS.</p><p></p><h2>Not all IOPS are always dumb</h2><div>This all being said, measuring IOPS can be valuable in contexts outside of parallel file systems.&nbsp; Two cases come to mind where measuring IOPS can be a rational yard stick.</div><h3 style=\"text-align: left;\">1. Serving up LUNs to containers and VMs</h3><div>By definition, infrastructure providers shouldn't be responsible for the applications that run inside black-box containers and VMs because they are providing storage infrastructure (block devices) and not storage services (file systems).&nbsp; Blocks in and blocks out are measured in IOPS, so the fit is natural.&nbsp; That said, HPC users care about file systems (that is, scientific applications do not perform I/O using SCSI commands directly!), so worrying about LUN performance isn't meaningful in the HPC context.</div><h3 style=\"text-align: left;\">2. Measuring the effect of many users doing many things</h3><div>While individual HPC workloads rarely perform random I/Os on purpose, if you have enough users doing many small tasks all at once, the file system itself sees a workload that approaches something random.&nbsp; The more, small, independent tasks running parallel and the farther back you stand from the overall I/O load timeline, the more random it looks.&nbsp; So, I argue that it is fair to measure the IOPS of a parallel file system for the purposes of measuring how much abuse a file system can take before it begins to impact everybody.</div><div><br /></div><div>Take, for example, these IOPS scaling I measured on a small all-flash file system using IOR:</div><div><br /></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://1.bp.blogspot.com/-TVonp3v_RWE/YW9bGX7mCrI/AAAAAAABWwQ/IWCsgpJvZYEiOAtzfntxWgnf8ZZaZyLzwCLcBGAsYHQ/s584/Unknown-1.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img alt=\"Scale-up IOPS benchmarking to demonstrate the saturation point of an all-flash file system\" border=\"0\" height=\"289\" src=\"https://1.bp.blogspot.com/-TVonp3v_RWE/YW9bGX7mCrI/AAAAAAABWwQ/IWCsgpJvZYEiOAtzfntxWgnf8ZZaZyLzwCLcBGAsYHQ/w400-h289/Unknown-1.png\" title=\"Scale-up IOPS benchmarking to demonstrate the saturation point of an all-flash file system\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><span style=\"font-size: x-small;\"><b>Scale-up IOPS benchmarking to demonstrate the saturation point of an all-flash file system</b></span></div><p><br />&lt;div&gt;It looks like it takes about 4,096 concurrent random readers or writers to max out the file system.  This alone isn’t meaningful until you consider what this means in the context of the whole compute and storage platform.&lt;/div&gt;</p><div><br /></div><div>What fraction of the cluster's compute nodes corresponds to 4096 cores?&nbsp; If you've got, say, <a href=\"https://www.sdsc.edu/support/user_guides/expanse.html#tech_summary\">728 dual-socket 64-core AMD Epyc processors</a>, it would only take 32 compute nodes to max out this file system.&nbsp; And if another user wanted to use any of the remaining 696 compute nodes to, say, run a Python script that needed to read in random packages scattered across the file system, there would be no remaining IOPS capacity left at this point, and everyone would experience perceptible lag.</div><div><br /></div><div>Of course, this is the most extreme case--purely random IOPS--but you can measure the IOPS that a real workload does generate on the server side when, say, sampling a deep learning training dataset. With this, you can then figure out how much headroom that application leaves for every other random-ish workload that needs to run on the same system.</div><div><br /></div><div>Once you realize that a lot of the unglamorous parts of of scientific computing--reading dotfiles when you log in, loading shared objects when you launch a dynamically linked executable, or even just editing source code--are full of random-like reads, you can establish a quantitative basis for figuring out how badly an IOPS-intensive data analysis application may affect everyone else's interactive accesses on the same file system.</div><div><br /></div><div>This is not to say that we can easily answer the question of \"How many IOPS do you need?\" though.&nbsp; How many IOPS a workload can drive is not how many IOPS that workload <i>needs</i>--it's really how fast it can compute before it has run out of data to process and needs to read more in.&nbsp; The faster your compute nodes, generally, the more data they can <i>consume</i>.&nbsp; They still <i>want</i> all the IOPS you can give them so they can spend as much time computing (and not waiting for I/O) as possible, and how many IOPS your application can drive is a function of how quickly it runs given the full stack between it and the storage, including CPU, memory, and networking.</div><h2 style=\"text-align: left;\">If everything is dumb, now what?</h2><div>Give up trying to reduce I/O performance down to a single IOPS number, because it's two degrees away from being useful.&nbsp; Bandwidth is a better metric in that it's only one degree away from what actually matters, but at the end of the day, the real metric of I/O performance is how much time an application has to wait on I/O before it can resume performing meaningful computations.&nbsp; Granted, most storage vendors will give you a blank stare if you take this angle to them; telling them that your application spends 50% of its time waiting on I/O isn't going to get you a better file system from a storage company alone, so think about what the real problem could be.</div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\"><b>Is the application doing I/O in a pattern (random or otherwise) that prevents the storage system from delivering as many bytes/second as possible?</b>&nbsp; If so, ask your vendor for a storage system that delivers more bandwidth to a wider range of I/O patterns than just perfectly aligned 1 MiB reads and writes.<br /><br /></div><div style=\"text-align: left;\"><b>Is the storage system already running as well as it can, but it only takes a few compute nodes to max it out?&nbsp;</b> If so, your storage system is too small relative to your compute system, and you should ask your vendor for more servers and drives to scale out.</div><div style=\"text-align: left;\"><br /><b>Is the storage system running at 100% CPU even though it's not delivering full bandwidth?&nbsp;</b> Servicing a small I/O requires a lot more CPU than a large I/O since there are fixed computations that have to happen on every read or write regardless of how big it is.&nbsp; Ask your vendor for a better file system that doesn't eat up so much CPU, or ask for more capable servers.<br /></div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\">Alternatively, if you have a lot of users all doing different things and the file system is giving poor performance to everyone, ask your vendor for a file system with better quality of service.&nbsp; This will ensure that one big job doesn't starve out all the small ones.</div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\"><b>Is the storage system slow but you don't have the time to figure out why?&nbsp;</b> If so, it sounds like you work for an organization that doesn't actually value data because it's not appropriately staffed.&nbsp; This isn't a storage problem!</div><div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\">Ultimately, if solving I/O problems was as easy answering how many IOPS you need, storage wouldn't be the perpetual pain point in HPC that it has been.&nbsp; As with all things in computing, there is no shortcut and the proper way to approach this is by rolling up your sleeves and start ruling out problems.&nbsp; You can (and should!) ask for a lot from your storage vendors--flexibility in delivering bandwidth, CPU-efficient file systems, and quality of service controls are all valid requests when buying storage.&nbsp; But IOPS are not.</div>",
            "url": "https://hpc.social/personal-blog/2021/iops-are-dumb/",
            
            
            
            
            
            "date_published": "2021-10-24T17:56:00-06:00",
            "date_modified": "2021-10-24T17:56:00-06:00",
            
                "author": "Glenn K. Lockwood's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2021/uptodate/",
            "title": "Uptodate",
            "summary": null,
            "content_text": "I recently had an itch to scratch - and that itch was writing a library in Go.We don’t use Go much for my work, so I figured out a compelling reason to start a new personal project -a command line tool written in Go (and matching GitHub action) to help keep things up todate in a repository. Appropriately, I called it uptodate!It was hugely inspired from the binoc (short for “binoculars”)library that can also perform specific kinds of updates, but I wanted more of a focus onDocker, and to have total control so I could go wild and crazy with writing Go codewithout worrying about forcing it on the owner, alecbcs, to merge my wild ideas.UptodateUptodate is a command line tool in Go and GitHub action that makes it easy to:   Update FROM statements in Dockerfile to have the latest shas   Update build arguments that are for spack versions, GitHub releases and commits, and container hashes.   Generate a matrix of Docker builds from a single configuration file   Generate a matrix of changed files in a repository.   List Dockerfile in a repository that have been changed.With all of the above, you can imagine a workflow that first updates DockerfileFROM statements and build args, and then re-builds and deploys these containers - the assumption being that the underlying dependency such as a GitHub commitor spack version has an update. Uptodate also will take a nested structurethat I call a docker “build hierarchy” and add new folders and Dockerfile whena new tag is detected. A kind of updater in uptodate is naturally called an “updater”and this means for the docker build and docker hierarchy updaters, we can writea yaml configuration file with our preferences for versions to be added, andother metadata. You should check out the user guidefor detailed usage, or read about the GitHub actionHow does it work?I’ll give a brief overview of a few of the commands and then a quick example GitHub workflow,and I’ll recommend that you read the documentation for the latest updates on uptodate, harharhar.The examples below assumed that you’ve installed uptodate and have the binary “uptodate” in your path.DockerfileIf you have one or more Dockerfile in your repository you can run uptodate to update digests.For example:$ uptodate dockerfile .will find Dockerfile in the present working directory and subfolders and update.For digests, you might see that:FROM ubuntu:20.04is updated toFROM ubuntu:18.04@sha256:9bc830af2bef73276515a29aa896eedfa7bdf4bdbc5c1063b4c457a4bbb8cd79Note in the above we still have the digest and the tag, so subsequent updates canfurther update the sha by looking up the container based on the tag.And we can also update build arguments that match a particular format! This one,specifically:ARG uptodate_&lt;build-arg-type&gt;_&lt;build-arg-value&gt;=&lt;default&gt;The above flags the build argument for uptodate to look at using the prefix of the libraryname, and then the next string after the underscore is the kind of update, followed byspecific metadata for that updater, and of course the value! A few examples are provided below.Spack Build ArgumentsSpack is a package manager intended for HPC, and it’shuge at the lab where I work. So naturally, it made sense for uptodate to be able tolook up the latest spack versions for some package.To create an argument that matched to a spack package (and its version) you might see:ARG uptodate_spack_ace=6.5.6After the updater runs, if it finds a new version 6.5.12, the line will read:ARG uptodate_spack_ace=6.5.12This works by using the static API that is deployed alongside the Spack Packagesrepository that I designed earlier this year. So the updater will get the latest versionsas known within the last 24 hours.GitHub Release Build ArgumentIf we want an updated version from a GitHub release (let’s say the spack software itself)we might see this:ARG uptodate_github_release_spack__spack=v0.16.1The above will look for new releases from spack on GitHub and update as follows:ARG uptodate_github_release_spack__spack=v0.16.2GitHub Commit Build ArgumentSimilarity, if we want more “bleeding edge” changes we can ask for a commitfrom a specific branch, following this pattern:ARG uptodate_github_commit_&lt;org&gt;__&lt;name&gt;__&lt;branch&gt;=&lt;release-tag&gt;Here is an example of asking for updates for the develop branch.ARG uptodate_github_commit_spack__spack__develop=NAwhich wouldn’t care about the first “commit” NA as it would update to:ARG uptodate_github_commit_spack__spack__develop=be8e52fbbec8106150680fc628dc72e69e5a20beAnd then to use it in your Dockerfile, you might pop into an environment variable:ENV spack_commit=${uptodate_github_commit_spack__spack__develop}See the docs for more detailed usage and an example for the Dockerfile updater.Docker BuildThe second updater that I think is pretty useful is the Docker build updater.This updated will read a config file, an uptodate.yaml, and then follow instructionsfor version regular expressoins and different kinds of builds args to generate a matrix ofbuilds (intended for GitHub actions). For example, let’s say that we start with this configuration file:dockerbuild:  build_args:    # This is an example of a manual build arg, versions are required    llvm_version:      # The key is a shorthand used for naming (required)      key: llvm      versions:       - \"4.0.0\"       - \"5.0.1\"       - \"6.0.0\"    # This is an example of a spack build arg, the name is the package    abyss_version:      key: abyss      name: abyss      type: spack    # This will be parsed by the Dockerfile parser, name is the container name    ubuntu_version:      key: ubuntu      name: ubuntu      type: container      startat: \"16.04\"      endat: \"20.04\"      filter:         - \"^[0-9]+[.]04$\"       skips:      - \"17.04\"      - \"19.04\"You’ll see the primary section of interest is under “dockerbuild” and under thiswe have three build args for a manually defined set of versions, a version froma spack package, and a container. You could run this in a repository rootto look for these config files (and a Dockerfile that they render with inthe same directory or below it) to generate a build matrix.$ uptodate dockerbuild Or to only include changed uptodate.yaml files:$ uptodate dockerbuild --changesIf you provide a registry URI that the containers build to, we can actually checkthese containers to look at current build args (that are saved as labels and thenviewable in the image config by uptodate) to determine if an update is needed.$ uptodate dockerbuild --registry ghcr.io/rse-radiussthe container. I think this is one of the neatest features - it was just addedin evenings this last week! Check out anexample image config that has these labels!This registry URI will also be included in the output to make it easy to buildIn a GitHub action, it might be used like this:jobs:  generate:    name: Generate Build Matrix    runs-on: ubuntu-latest    outputs:      dockerbuild_matrix: $      empty_matrix: $    steps:    - uses: actions/checkout@v2      if: github.event_name == 'pull_request'      with:         fetch-depth: 0         ref: $    - uses: actions/checkout@v2      if: github.event_name != 'pull_request'      with:         fetch-depth: 0    - name: Generate Build Matrix      uses: vsoch/uptodate@main      id: dockerbuild      with:         root: .        parser: dockerbuild        flags: \"--registry ghcr.io/myreponame\"    - name: View and Check Build Matrix Result      env:        result: $      run: |        echo ${result}  build:    needs:      - generate    runs-on: ubuntu-latest    strategy:      fail-fast: false      matrix:        result: $    if: $    name: \"Build $\"    steps:    - name: Checkout Repository      uses: actions/checkout@v2    - name: Set up Docker Buildx      uses: docker/setup-buildx-action@v1    - name: Build $      id: builder      env:        container: $        prefix: $        filename: $      run: |        basedir=$(dirname $filename)        cd $basedir        ${prefix} -t ${container} .Of course you’d want to login to a registry, and then also possibly calculate metrics forthe container, so consider this a very simple example.The build matrix that is being passed between those steps has entries like this:[  {    \"name\": \"ubuntu/clang/uptodate.yaml\",    \"container_name\": \"ghcr.io/rse-radiuss/clang-ubuntu-20.04:llvm-10.0.0\",    \"filename\": \"ubuntu/clang/Dockerfile\",    \"parser\": \"dockerbuild\",    \"buildargs\": {      \"llvm_version\": \"10.0.0\",      \"ubuntu_version\": \"20.04\"    },    \"command_prefix\": \"docker build -f Dockerfile --build-arg llvm_version=10.0.0 --build-arg ubuntu_version=20.04\",    \"description\": \"ubuntu/clang llvm_version:10.0.0 ubuntu_version:20.04\"  },  ...]Git UpdaterI also like this updater because it easily generates for you a matrix of filesthat are changed, according to git. Running locally it looks like this:$ ./uptodate git /path/to/repo              _            _       _         _   _ _ __ | |_ ___   __| | __ _| |_ ___  | | | | '_ \\| __/ _ \\ / _  |/ _  | __/ _ \\ | |_| | |_) | || (_) | (_| | (_| | ||  __/  \\__,_| .__/ \\__\\___/ \\__,_|\\__,_|\\__\\___|       |_|                          git  ⭐️ Changed Files ⭐️    .github/workflows/build-matrices.yaml: ModifyAnd would generate a matrix for a GitHub action too:[  {    \"name\": \"Modify\",    \"filename\": \"cli/dockerbuild.go\"  },  {    \"name\": \"Modify\",    \"filename\": \"parsers/common.go\"  },  {    \"name\": \"Insert\",    \"filename\": \"parsers/docker/buildargs.go\"  },  {    \"name\": \"Modify\",    \"filename\": \"parsers/docker/docker.go\"  },  {    \"name\": \"Modify\",    \"filename\": \"tests/ubuntu/21.04/Dockerfile\"  },  {    \"name\": \"Modify\",    \"filename\": \"tests/ubuntu/clang/Dockerfile\"  }]And of course you can change the default “main” to another branch:$ ./uptodate git /path/to/repo --branch masterand that also pipes into a GitHub action. I don’t want to redundantly reproduce the docs,so if you are interested you can read moreat the user guideor GitHub action pages.Mind you that the library is heavily under develop, so if you have a request for a new updater or want to reporta a bug, please let me know!.OverviewI have loved working on this library. I think it’s the first library in Go whereI’ve been proficient enough to not look everything up that I need - the code has justflowed from my fingers! Mind you I’m still figuring out my own design preferences,and I’m at the stage where I’ll write a new functionality, and then immediately not likemy design, and want to re-write it. But I think that means I’ll eventually get better.But it’s always good to have one or more projects you are passionate about, becauseI don’t personally see a point in being a software engineer if I don’t (yes, I know itmakes a salary, but I require more than that).",
            "content_html": "<p>I recently had an itch to scratch - and that itch was writing a library in Go.We don’t use Go much for my work, so I figured out a compelling reason to start a new personal project -a command line tool written in Go (and matching GitHub action) to help keep things up todate in a repository. Appropriately, I called it <a href=\"https://vsoch.github.io/uptodate/docs/#/\" target=\"_blank\">uptodate</a>!It was hugely inspired from the <a href=\"https://github.com/autamus/binoc\" target=\"_blank\">binoc</a> (short for “binoculars”)library that can also perform specific kinds of updates, but I wanted more of a focus onDocker, and to have total control so I could go wild and crazy with writing Go codewithout worrying about forcing it on the owner, <a href=\"https://github.com/alecbcs\" target=\"_blank\">alecbcs</a>, to merge my wild ideas.</p><p><br /></p><div class=\"padding:20px\"><img src=\"https://vsoch.github.io/uptodate/assets/img/uptodate.png\" /></div><h2 id=\"uptodate\">Uptodate</h2><p>Uptodate is a command line tool in Go and GitHub action that makes it easy to:</p><ol class=\"custom-counter\">  <li> Update FROM statements in Dockerfile to have the latest shas</li>  <li> Update build arguments that are for spack versions, GitHub releases and commits, and container hashes.</li>  <li> Generate a matrix of Docker builds from a single configuration file</li>  <li> Generate a matrix of changed files in a repository.</li>  <li> List Dockerfile in a repository that have been changed.</li></ol><p>With all of the above, you can imagine a workflow that first updates DockerfileFROM statements and build args, and then re-builds and deploys these containers - the assumption being that the underlying dependency such as a GitHub commitor spack version has an update. Uptodate also will take a nested structurethat I call a docker “build hierarchy” and add new folders and Dockerfile whena new tag is detected. A kind of updater in uptodate is naturally called an “updater”and this means for the docker build and docker hierarchy updaters, we can writea yaml configuration file with our preferences for versions to be added, andother metadata. You should check out the <a href=\"https://vsoch.github.io/uptodate/docs/#/user-guide/user-guide\" target=\"_blank\">user guide</a>for detailed usage, or read about <a href=\"https://vsoch.github.io/uptodate/docs/#/user-guide/github-action\" target=\"_blank\">the GitHub action</a></p><h2 id=\"how-does-it-work\">How does it work?</h2><p>I’ll give a brief overview of a few of the commands and then a quick example GitHub workflow,and I’ll recommend that you read the documentation for the latest updates on uptodate, harharhar.The examples below assumed that you’ve <a href=\"https://vsoch.github.io/uptodate/docs/#/user-guide/user-guide?id=install\" target=\"_blank\">installed</a> uptodate and have the binary “uptodate” in your path.</p><h3 id=\"dockerfile\">Dockerfile</h3><p>If you have one or more Dockerfile in your repository you can run uptodate to update digests.For example:</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>$ uptodate dockerfile .</code></pre></div></div><p>will find Dockerfile in the present working directory and subfolders and update.For digests, you might see that:</p><div class=\"language-dockerfile highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">FROM</span><span class=\"s\"> ubuntu:20.04</span></code></pre></div></div><p>is updated to</p><div class=\"language-dockerfile highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">FROM</span><span class=\"s\"> ubuntu:18.04@sha256:9bc830af2bef73276515a29aa896eedfa7bdf4bdbc5c1063b4c457a4bbb8cd79</span></code></pre></div></div><p>Note in the above we still have the digest and the tag, so subsequent updates canfurther update the sha by looking up the container based on the tag.And we can also update build arguments that match a particular format! This one,specifically:</p><div class=\"language-dockerfile highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">ARG</span><span class=\"s\"> uptodate_&lt;build-arg-type&gt;_&lt;build-arg-value&gt;=&lt;default&gt;</span></code></pre></div></div><p>The above flags the build argument for uptodate to look at using the prefix of the libraryname, and then the next string after the underscore is the kind of update, followed byspecific metadata for that updater, and of course the value! A few examples are provided below.</p><h4 id=\"spack-build-arguments\">Spack Build Arguments</h4><p><a href=\"https://github.com/spack/spack\" target=\"_blank\">Spack</a> is a package manager intended for HPC, and it’shuge at the lab where I work. So naturally, it made sense for uptodate to be able tolook up the latest spack versions for some package.To create an argument that matched to a spack package (and its version) you might see:</p><div class=\"language-dockerfile highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">ARG</span><span class=\"s\"> uptodate_spack_ace=6.5.6</span></code></pre></div></div><p>After the updater runs, if it finds a new version 6.5.12, the line will read:</p><div class=\"language-dockerfile highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">ARG</span><span class=\"s\"> uptodate_spack_ace=6.5.12</span></code></pre></div></div><p>This works by using the static API that is deployed alongside the <a href=\"https://spack.github.io/packages/\" target=\"_blank\">Spack Packages</a>repository that I designed earlier this year. So the updater will get the latest versionsas known within the last 24 hours.</p><h4 id=\"github-release-build-argument\">GitHub Release Build Argument</h4><p>If we want an updated version from a GitHub release (let’s say the spack software itself)we might see this:</p><div class=\"language-dockerfile highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">ARG</span><span class=\"s\"> uptodate_github_release_spack__spack=v0.16.1</span></code></pre></div></div><p>The above will look for new releases from spack on GitHub and update as follows:</p><div class=\"language-dockerfile highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">ARG</span><span class=\"s\"> uptodate_github_release_spack__spack=v0.16.2</span></code></pre></div></div><h4 id=\"github-commit-build-argument\">GitHub Commit Build Argument</h4><p>Similarity, if we want more “bleeding edge” changes we can ask for a commitfrom a specific branch, following this pattern:</p><div class=\"language-dockerfile highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">ARG</span><span class=\"s\"> uptodate_github_commit_&lt;org&gt;__&lt;name&gt;__&lt;branch&gt;=&lt;release-tag&gt;</span></code></pre></div></div><p>Here is an example of asking for updates for the develop branch.</p><div class=\"language-dockerfile highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">ARG</span><span class=\"s\"> uptodate_github_commit_spack__spack__develop=NA</span></code></pre></div></div><p>which wouldn’t care about the first “commit” NA as it would update to:</p><div class=\"language-dockerfile highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">ARG</span><span class=\"s\"> uptodate_github_commit_spack__spack__develop=be8e52fbbec8106150680fc628dc72e69e5a20be</span></code></pre></div></div><p>And then to use it in your Dockerfile, you might pop into an environment variable:</p><div class=\"language-dockerfile highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">ENV</span><span class=\"s\"> spack_commit=${uptodate_github_commit_spack__spack__develop}</span></code></pre></div></div><p>See the <a href=\"https://vsoch.github.io/uptodate/docs/#/user-guide/user-guide?id=dockerfile\" target=\"_blank\">docs</a> for more detailed usage and an example for the Dockerfile updater.</p><h3 id=\"docker-build\">Docker Build</h3><p>The second updater that I think is pretty useful is the Docker build updater.This updated will read a config file, an uptodate.yaml, and then follow instructionsfor version regular expressoins and different kinds of builds args to generate a matrix ofbuilds (intended for GitHub actions). For example, let’s say that we start with this configuration file:</p><div class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"na\">dockerbuild</span><span class=\"pi\">:</span>  <span class=\"na\">build_args</span><span class=\"pi\">:</span>    <span class=\"c1\"># This is an example of a manual build arg, versions are required</span>    <span class=\"na\">llvm_version</span><span class=\"pi\">:</span>      <span class=\"c1\"># The key is a shorthand used for naming (required)</span>      <span class=\"na\">key</span><span class=\"pi\">:</span> <span class=\"s\">llvm</span>      <span class=\"na\">versions</span><span class=\"pi\">:</span>       <span class=\"pi\">-</span> <span class=\"s2\">\"</span><span class=\"s\">4.0.0\"</span>       <span class=\"pi\">-</span> <span class=\"s2\">\"</span><span class=\"s\">5.0.1\"</span>       <span class=\"pi\">-</span> <span class=\"s2\">\"</span><span class=\"s\">6.0.0\"</span>    <span class=\"c1\"># This is an example of a spack build arg, the name is the package</span>    <span class=\"na\">abyss_version</span><span class=\"pi\">:</span>      <span class=\"na\">key</span><span class=\"pi\">:</span> <span class=\"s\">abyss</span>      <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">abyss</span>      <span class=\"na\">type</span><span class=\"pi\">:</span> <span class=\"s\">spack</span>    <span class=\"c1\"># This will be parsed by the Dockerfile parser, name is the container name</span>    <span class=\"na\">ubuntu_version</span><span class=\"pi\">:</span>      <span class=\"na\">key</span><span class=\"pi\">:</span> <span class=\"s\">ubuntu</span>      <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">ubuntu</span>      <span class=\"na\">type</span><span class=\"pi\">:</span> <span class=\"s\">container</span>      <span class=\"na\">startat</span><span class=\"pi\">:</span> <span class=\"s2\">\"</span><span class=\"s\">16.04\"</span>      <span class=\"na\">endat</span><span class=\"pi\">:</span> <span class=\"s2\">\"</span><span class=\"s\">20.04\"</span>      <span class=\"na\">filter</span><span class=\"pi\">:</span>         <span class=\"pi\">-</span> <span class=\"s2\">\"</span><span class=\"s\">^[0-9]+[.]04$\"</span>       <span class=\"na\">skips</span><span class=\"pi\">:</span>      <span class=\"pi\">-</span> <span class=\"s2\">\"</span><span class=\"s\">17.04\"</span>      <span class=\"pi\">-</span> <span class=\"s2\">\"</span><span class=\"s\">19.04\"</span></code></pre></div></div><p>You’ll see the primary section of interest is under “dockerbuild” and under thiswe have three build args for a manually defined set of versions, a version froma spack package, and a container. You could run this in a repository rootto look for these config files (and a Dockerfile that they render with inthe same directory or below it) to generate a build matrix.</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>uptodate dockerbuild </code></pre></div></div><p>Or to only include changed uptodate.yaml files:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>uptodate dockerbuild <span class=\"nt\">--changes</span></code></pre></div></div><p>If you provide a registry URI that the containers build to, we can actually checkthese containers to look at current build args (that are saved as labels and thenviewable in the image config by uptodate) to determine if an update is needed.</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>uptodate dockerbuild <span class=\"nt\">--registry</span> ghcr.io/rse-radiuss</code></pre></div></div><p>the container. I think this is one of the neatest features - it was just addedin evenings this last week! Check out an<a href=\"https://crane.ggcr.dev/config/ghcr.io/rse-radiuss/ubuntu:20.04\" target=\"_blank\">example image config</a> that has these labels!This registry URI will also be included in the output to make it easy to buildIn a GitHub action, it might be used like this:</p><div class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"na\">jobs</span><span class=\"pi\">:</span>  <span class=\"na\">generate</span><span class=\"pi\">:</span>    <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">Generate Build Matrix</span>    <span class=\"na\">runs-on</span><span class=\"pi\">:</span> <span class=\"s\">ubuntu-latest</span>    <span class=\"na\">outputs</span><span class=\"pi\">:</span>      <span class=\"na\">dockerbuild_matrix</span><span class=\"pi\">:</span> <span class=\"s\">$</span>      <span class=\"na\">empty_matrix</span><span class=\"pi\">:</span> <span class=\"s\">$</span>    <span class=\"na\">steps</span><span class=\"pi\">:</span>    <span class=\"pi\">-</span> <span class=\"na\">uses</span><span class=\"pi\">:</span> <span class=\"s\">actions/checkout@v2</span>      <span class=\"na\">if</span><span class=\"pi\">:</span> <span class=\"s\">github.event_name == 'pull_request'</span>      <span class=\"na\">with</span><span class=\"pi\">:</span>         <span class=\"na\">fetch-depth</span><span class=\"pi\">:</span> <span class=\"m\">0</span>         <span class=\"na\">ref</span><span class=\"pi\">:</span> <span class=\"s\">$</span>    <span class=\"pi\">-</span> <span class=\"na\">uses</span><span class=\"pi\">:</span> <span class=\"s\">actions/checkout@v2</span>      <span class=\"na\">if</span><span class=\"pi\">:</span> <span class=\"s\">github.event_name != 'pull_request'</span>      <span class=\"na\">with</span><span class=\"pi\">:</span>         <span class=\"na\">fetch-depth</span><span class=\"pi\">:</span> <span class=\"m\">0</span>    <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">Generate Build Matrix</span>      <span class=\"na\">uses</span><span class=\"pi\">:</span> <span class=\"s\">vsoch/uptodate@main</span>      <span class=\"na\">id</span><span class=\"pi\">:</span> <span class=\"s\">dockerbuild</span>      <span class=\"na\">with</span><span class=\"pi\">:</span>         <span class=\"na\">root</span><span class=\"pi\">:</span> <span class=\"s\">.</span>        <span class=\"na\">parser</span><span class=\"pi\">:</span> <span class=\"s\">dockerbuild</span>        <span class=\"na\">flags</span><span class=\"pi\">:</span> <span class=\"s2\">\"</span><span class=\"s\">--registry</span><span class=\"nv\"> </span><span class=\"s\">ghcr.io/myreponame\"</span>    <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">View and Check Build Matrix Result</span>      <span class=\"na\">env</span><span class=\"pi\">:</span>        <span class=\"na\">result</span><span class=\"pi\">:</span> <span class=\"s\">$</span>      <span class=\"na\">run</span><span class=\"pi\">:</span> <span class=\"pi\">|</span>        <span class=\"s\">echo ${result}</span>  <span class=\"na\">build</span><span class=\"pi\">:</span>    <span class=\"na\">needs</span><span class=\"pi\">:</span>      <span class=\"pi\">-</span> <span class=\"s\">generate</span>    <span class=\"na\">runs-on</span><span class=\"pi\">:</span> <span class=\"s\">ubuntu-latest</span>    <span class=\"na\">strategy</span><span class=\"pi\">:</span>      <span class=\"na\">fail-fast</span><span class=\"pi\">:</span> <span class=\"no\">false</span>      <span class=\"na\">matrix</span><span class=\"pi\">:</span>        <span class=\"na\">result</span><span class=\"pi\">:</span> <span class=\"s\">$</span>    <span class=\"na\">if</span><span class=\"pi\">:</span> <span class=\"s\">$</span>    <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s2\">\"</span><span class=\"s\">Build</span><span class=\"nv\"> </span><span class=\"s\">$\"</span>    <span class=\"na\">steps</span><span class=\"pi\">:</span>    <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">Checkout Repository</span>      <span class=\"na\">uses</span><span class=\"pi\">:</span> <span class=\"s\">actions/checkout@v2</span>    <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">Set up Docker Buildx</span>      <span class=\"na\">uses</span><span class=\"pi\">:</span> <span class=\"s\">docker/setup-buildx-action@v1</span>    <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">Build $</span>      <span class=\"na\">id</span><span class=\"pi\">:</span> <span class=\"s\">builder</span>      <span class=\"na\">env</span><span class=\"pi\">:</span>        <span class=\"na\">container</span><span class=\"pi\">:</span> <span class=\"s\">$</span>        <span class=\"na\">prefix</span><span class=\"pi\">:</span> <span class=\"s\">$</span>        <span class=\"na\">filename</span><span class=\"pi\">:</span> <span class=\"s\">$</span>      <span class=\"na\">run</span><span class=\"pi\">:</span> <span class=\"pi\">|</span>        <span class=\"s\">basedir=$(dirname $filename)</span>        <span class=\"s\">cd $basedir</span>        <span class=\"s\">${prefix} -t ${container} .</span></code></pre></div></div><p>Of course you’d want to login to a registry, and then also possibly calculate metrics forthe container, so consider this a very simple example.The build matrix that is being passed between those steps has entries like this:</p><div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">[</span><span class=\"w\">  </span><span class=\"p\">{</span><span class=\"w\">    </span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"ubuntu/clang/uptodate.yaml\"</span><span class=\"p\">,</span><span class=\"w\">    </span><span class=\"nl\">\"container_name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"ghcr.io/rse-radiuss/clang-ubuntu-20.04:llvm-10.0.0\"</span><span class=\"p\">,</span><span class=\"w\">    </span><span class=\"nl\">\"filename\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"ubuntu/clang/Dockerfile\"</span><span class=\"p\">,</span><span class=\"w\">    </span><span class=\"nl\">\"parser\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"dockerbuild\"</span><span class=\"p\">,</span><span class=\"w\">    </span><span class=\"nl\">\"buildargs\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">      </span><span class=\"nl\">\"llvm_version\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"10.0.0\"</span><span class=\"p\">,</span><span class=\"w\">      </span><span class=\"nl\">\"ubuntu_version\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"20.04\"</span><span class=\"w\">    </span><span class=\"p\">},</span><span class=\"w\">    </span><span class=\"nl\">\"command_prefix\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"docker build -f Dockerfile --build-arg llvm_version=10.0.0 --build-arg ubuntu_version=20.04\"</span><span class=\"p\">,</span><span class=\"w\">    </span><span class=\"nl\">\"description\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"ubuntu/clang llvm_version:10.0.0 ubuntu_version:20.04\"</span><span class=\"w\">  </span><span class=\"p\">},</span><span class=\"w\">  </span><span class=\"err\">...</span><span class=\"w\"></span><span class=\"p\">]</span><span class=\"w\"></span></code></pre></div></div><h3 id=\"git-updater\">Git Updater</h3><p>I also like this updater because it easily generates for you a matrix of filesthat are changed, according to git. Running locally it looks like this:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>./uptodate git /path/to/repo              _            _       _         _   _ _ __ | |_ ___   __| | __ _| |_ ___  | | | | <span class=\"s1\">'_ \\| __/ _ \\ / _  |/ _  | __/ _ \\ | |_| | |_) | || (_) | (_| | (_| | ||  __/  \\__,_| .__/ \\__\\___/ \\__,_|\\__,_|\\__\\___|       |_|                          git  ⭐️ Changed Files ⭐️    .github/workflows/build-matrices.yaml: Modify</span></code></pre></div></div><p>And would generate a matrix for a GitHub action too:</p><div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">[</span><span class=\"w\">  </span><span class=\"p\">{</span><span class=\"w\">    </span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Modify\"</span><span class=\"p\">,</span><span class=\"w\">    </span><span class=\"nl\">\"filename\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"cli/dockerbuild.go\"</span><span class=\"w\">  </span><span class=\"p\">},</span><span class=\"w\">  </span><span class=\"p\">{</span><span class=\"w\">    </span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Modify\"</span><span class=\"p\">,</span><span class=\"w\">    </span><span class=\"nl\">\"filename\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"parsers/common.go\"</span><span class=\"w\">  </span><span class=\"p\">},</span><span class=\"w\">  </span><span class=\"p\">{</span><span class=\"w\">    </span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Insert\"</span><span class=\"p\">,</span><span class=\"w\">    </span><span class=\"nl\">\"filename\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"parsers/docker/buildargs.go\"</span><span class=\"w\">  </span><span class=\"p\">},</span><span class=\"w\">  </span><span class=\"p\">{</span><span class=\"w\">    </span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Modify\"</span><span class=\"p\">,</span><span class=\"w\">    </span><span class=\"nl\">\"filename\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"parsers/docker/docker.go\"</span><span class=\"w\">  </span><span class=\"p\">},</span><span class=\"w\">  </span><span class=\"p\">{</span><span class=\"w\">    </span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Modify\"</span><span class=\"p\">,</span><span class=\"w\">    </span><span class=\"nl\">\"filename\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"tests/ubuntu/21.04/Dockerfile\"</span><span class=\"w\">  </span><span class=\"p\">},</span><span class=\"w\">  </span><span class=\"p\">{</span><span class=\"w\">    </span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Modify\"</span><span class=\"p\">,</span><span class=\"w\">    </span><span class=\"nl\">\"filename\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"tests/ubuntu/clang/Dockerfile\"</span><span class=\"w\">  </span><span class=\"p\">}</span><span class=\"w\"></span><span class=\"p\">]</span><span class=\"w\"></span></code></pre></div></div><p>And of course you can change the default “main” to another branch:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>./uptodate git /path/to/repo <span class=\"nt\">--branch</span> master</code></pre></div></div><p>and that also pipes into a GitHub action. I don’t want to redundantly reproduce the docs,so if you are interested you can read moreat the <a href=\"https://vsoch.github.io/uptodate/docs/#/user-guide/user-guide\" target=\"_blank\">user guide</a>or <a href=\"https://vsoch.github.io/uptodate/docs/#/user-guide/github-action\" target=\"_blank\">GitHub action pages</a>.Mind you that the library is heavily under develop, so if you have a request for a new updater or want to reporta a bug, please <a href=\"https://github.com/vsoch/uptodate/issues\" target=\"_blank\">let me know!</a>.</p><h2 id=\"overview\">Overview</h2><p>I have loved working on this library. I think it’s the first library in Go whereI’ve been proficient enough to not look everything up that I need - the code has justflowed from my fingers! Mind you I’m still figuring out my own design preferences,and I’m at the stage where I’ll write a new functionality, and then immediately not likemy design, and want to re-write it. But I think that means I’ll eventually get better.But it’s always good to have one or more projects you are passionate about, becauseI don’t personally see a point in being a software engineer if I don’t (yes, I know itmakes a salary, but I require more than that).</p>",
            "url": "https://hpc.social/personal-blog/2021/uptodate/",
            
            
            
            
            
            "date_published": "2021-09-19T09:30:00-06:00",
            "date_modified": "2021-09-19T09:30:00-06:00",
            
                "author": "Vanessasaurus"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2021/to-compete-your-team-needs-a-specialty/",
            "title": "To Compete, Your Team Needs a Specialty",
            "summary": null,
            "content_text": "And ‘HPC’ or ‘Research Software Development’ isn’t a specialty(Note: This post is adapted from #90 of the Research Computing Teams Newsletter)Quick: what’s your team’s specialty?Your team’s specialty is its reputation for what it’s good at. Not what you think your team is good at; what matters is what specific thing your stakeholders (funders, clients, institutional decision makers) think your specialty is. What they recommend you for to peers, what they recommend funding you for to decision makers.In the post-pandemic world, researchers are used to getting their support remotely from anywhere. To compete, your team will need well-defined specialties; and “HPC” or “research software development” isn’t a specialty.    Stand out from the crowd by having your team choose a specific path and owning it.The pandemic isn’t over, but the end of this phase has begun, and with September (“academic new years”) here, it’s a good time to think about the future. Last October I wrote about what post-pandemic research computing is going to look like, and it’s holding up pretty well. With researchers now very comfortable getting research computing and data support virtually and with budgets under pressure, there is going to be a lot more competition for research computing and data teams. Research collaborations are going to be looking elsewhere more and more often - academic teams at other institutions, or with commercial companies (either commercial cloud vendors for compute, or emerging collaborations between well-known names, like NAG and Azure, for services).This is an opportunity for well run, focussed teams to grow and prosper. But it’s going to take more planning and forethought than decades past, where one could count on having a near monopsony, of being the only available seller of services to local researchers. It’s going to take developing and maintaining a strong reputation for a small set of specialties.“HPC” may sound and feel like a specialty within the community, but to researchers and decision makers it’s incredibly generic and so meaningless. It’s not a technical term, but a term of advocacy and marketing which has been come to mean resources for anything from high throughput batch services to huge tightly coupled simulations to single-node multi-GPU code runs. Even advocates for the term define it as “anything bigger than what a researcher could provide on their own” which is incredibly generic, and so necessarily meaningless. How can your team’s specialty be “anything”? A team is expecting researchers to recommend them for “anything?” There’s a reason why VPRs would be just as happy contracting it out (e.g. see table 2 here).“Services and expertise for quickly analyzing public-health bioinformatics data”, “a platform for firing off and monitoring aerospace CFD calculations”, “a centre of excellence for digital humanities data curation and archiving”: these are examples of specialities - products, services - that researchers and institutional decision makers can see the value of and be willing to put money into, services and products and teams that researchers can recommend to each other. They are areas where a team could build a strong reputation - they could be the group that researchers recommend to collaborators when they chat about research needs.“Research Software Development” at least, to its credit, doesn’t pretend to be a narrow specialty - it’s a broad area which can encompass any area of software development in support of research work. As a result, a team can’t have a specialty in “Research Software Development”; it can have a specialty in “web applications and mobile apps for data collection”, or “GIS analysis tools” or “agent-based simulations for social sciences modelling”. But almost certainly not all three at the same time.Even so, research software development is too specific in one unhelpful sense. It could be that researchers are just looking for your team to write some software for them, hand it over, and be done. But increasingly, researchers are looking not just to be delivered some software, but for a team to host the software, run it, operate it - and/or collect and curate data to be used with the tool, for tests or otherwise. Focusing solely on research software development, as a separate activity from systems operation or data analysis and management, can be overly limiting.Ok, so what does all of this have to do with competition?One of my venial weaknesses is spending too much time on twitter. I’m seeing increasing concern there from research computing teams that cloud vendors or teams using cloud vendors are coming into their institutions and winning or trying to win contracts for projects that “should” have gone to the in-house teams. I’m hearing complaints that the external bids are for amounts of money 2x or more what the in-house team says they could do it for. Incredibly (and almost certainly incorrectly) I’ve even heard 10x.Reader, as hard as it is to believe, those complaining see this as an affront1, and a threat, rather than the enormous opportunity it is.If a contract at your institution is won - or even in serious contention - that is 2x what you estimate you could have provided the services for, that’s not evidence that the external contractor is overcharging. It’s evidence that your team is undercharging, that you could have proposed doing more to support that project and the researchers, and that you’re leaving money on the table. It’s also evidence that you haven’t fully convinced the relevant decision makers that you can provide that service; they don’t see it as being part of your specialty.Clearly your institution found it worthwhile to spend or consider spending that 2x, because they understood that it was worth at least that much to them to have those services. A bid for half that amount having failed or being questioned means that they really didn’t believe the in-house team could do it as well. That’s revealed-preferences data that you can use. (And if I truly believed someone at my institution was seriously considering spending 10x (1000%!) to work with an outside company rather than work with my team, well, that would occasion some serious soul searching.)Cloud providers and other external contractors do have advantages. They have a library of reference architectures they can deploy, so they can pitch (say) CFD solutions to the mech eng department, and bioinformatics pipeline solutions to the biology department. They can pull from a library of testimonials to demonstrate that they can do the work.But so can you. You have access to all the literature to search for how others have deployed such solutions. You have (or should have) testimonials from the people that matter - researchers at that very institution. And you have a network of deep relationships in the institution, relationships based on collaboration on research problems. Those relationships and shared expertise and history of collaboration is something the external contractors have no chance of matching.If you’re in danger of losing out on these sorts of competitions, it’s because you’re not communicating your specialities in a way that matters, in a way that’s convincing, to the people who could pay for your services. They can’t see how your “HPC batch services” connects with “a digital twinning platform for building simulation”. They don’t see “GIS exploration for private social sciences data” as being an obvious of your “Research Software Development” effort - where’s the data part?  If there’s a miscommunication there about what your team can provide, that’s on you and your team, not on the researchers or other decision makers.You have specialities - if you don’t know what they are, ask the researchers who keep coming back. How do they describe what you do? What would they say your speciality is, how do they talk about you to their colleagues? What would you have to demonstrate to them to have them recommend their colleagues to you?Similarly, you already have a million things you don’t do.  You won’t fix a researcher’s printer, you don’t help them do graphic design for their posters, my guess is you don’t help them set up spreadsheets in OneDrive or set up lab webpages.  So it’s not like declaring that there’s computing stuff you do and don’t help researchers with is some completely new thing, previously utterly unknown to your organization.Once you make explicit your specialties, you can start playing to your strengths, and communicating them endlessly. You can make a point of reaching out, having your team talk at conferences in the specialties, and at departmental colloquia. You can be well-regarded enough in your institution for those specialties that external contractors pitching work within your speciality never get in the door. You can start more easily hiring people that are interested in that specialty. A specialty builds on itself, snowballs. You can start steering future work towards that specialty to build on it, and start directing work well outside the specialty to somewhere else - where it does fit inside their specialty.Yeah, that last part is scary. Sticking to this path isn’t easy. It means turning down opportunities that aren’t in or adjacent to your specialities. Especially for new teams, eager to please, this can be scary.But as anywhere in research, your team’s reputation is all that matters. Your team has a reputation, has stuff it does and doesn’t do. Did you choose it, did you shape it, or are you content to just let it happen?Your team can be extremely strong in, specialize in, develop a reputation in, any of a number of things. But not all of the things. Being a manager or leader means choosing.            And affront was taken. There were lots of dark murmurings about slick sales teams trying to fool gullible senior administrators. And, you know, I’m sure it’s comforting for the teams that might lose out on these contracts to think that the vendor mesmerized the simpleton decision makers with their entrancing slide decks, and so hoodwinked them into considering an overpriced contract. But (a) have they never seen a vendor pitch? and (b) it’s self-serving twaddle to imagine that just because someone higher up made a decision to work with someone else they must clearly be dumb. Dismissing out of hand the possibility that there might be valid reasons to direct work elswhere means they’re going to end up making a lot of poor and uninformed decisions. &#8617;      ",
            "content_html": "<h2 id=\"and-hpc-or-research-software-development-isnt-a-specialty\">And ‘HPC’ or ‘Research Software Development’ isn’t a specialty</h2><p>(Note: This post is adapted from <a href=\"https://www.researchcomputingteams.org/newsletter_issues/0090\">#90</a> of the <a href=\"https://www.researchcomputingteams.org\">Research Computing Teams Newsletter</a>)</p><p>Quick: what’s your team’s specialty?</p><p>Your team’s specialty is its reputation for what it’s good at. Not what <em>you</em> think your team is good at; what matters is what specific thing your stakeholders (funders, clients, institutional decision makers) think your specialty is. What they recommend you for to peers, what they recommend funding you for to decision makers.</p><p>In the post-pandemic world, researchers are used to getting their support remotely from anywhere. To compete, your team will need well-defined specialties; and “HPC” or “research software development” isn’t a specialty.</p><figure style=\"width: 45%; float: right;\">  <img alt=\"Standout from the crowd by choosing a specific path.\" src=\"https://www.dursi.ca/assets/imgs/standout_sm.jpg\" />  <figcaption><i>Stand out from the crowd by having your team choose a specific path and owning it.</i></figcaption></figure><p>The pandemic isn’t over, but the end of this phase has begun, and with September (“academic new years”) here, it’s a good time to think about the future. Last October <a href=\"https://www.dursi.ca/post/research-computing-in-the-aftertimes\">I wrote about</a> what post-pandemic research computing is going to look like, and it’s holding up pretty well. With researchers now very comfortable getting research computing and data support virtually and with budgets under pressure, there is going to be a lot more competition for research computing and data teams. Research collaborations are going to be looking elsewhere more and more often - academic teams at other institutions, or with commercial companies (either commercial cloud vendors for compute, or emerging collaborations between well-known names, like <a href=\"https://www.nag.com/news/machine-learning-expertise-new-azure-hpc-ai-collaboration-centre\">NAG and Azure</a>, for services).</p><p>This is an opportunity for well run, focussed teams to grow and prosper. But it’s going to take more planning and forethought than decades past, where one could count on having a near monopsony, of being the only available seller of services to local researchers. It’s going to take developing and maintaining a strong reputation for a small set of specialties.</p><p>“HPC” may sound and feel like a specialty within the community, but to researchers and decision makers it’s incredibly generic and so meaningless. It’s not a technical term, but a term of advocacy and marketing which has been come to mean resources for anything from high throughput batch services to huge tightly coupled simulations to single-node multi-GPU code runs. Even <em>advocates</em> for the term define it as “anything bigger than what a researcher could provide on their own” which is incredibly generic, and so necessarily meaningless. How can your team’s <em>specialty</em> be “anything”? A team is expecting researchers to recommend them for “anything?” There’s a reason why VPRs would be just as happy contracting it out (<em>e.g.</em> see table 2 <a href=\"https://www.srainternational.org/blogs/srai-jra1/2019/12/09/operational-fiscal-management-of-core-facilities\">here</a>).</p><p>“Services and expertise for quickly analyzing public-health bioinformatics data”, “a platform for firing off and monitoring aerospace CFD calculations”, “a centre of excellence for digital humanities data curation and archiving”: these are examples of specialities - products, services - that researchers and institutional decision makers can see the value of and be willing to put money into, services and products and teams that researchers can recommend to each other. They are areas where a team could build a strong reputation - they could be the group that researchers recommend to collaborators when they chat about research needs.</p><p>“Research Software Development” at least, to its credit, doesn’t pretend to be a narrow specialty - it’s a broad area which can encompass any area of software development in support of research work. As a result, a team can’t have a specialty in “Research Software Development”; it can have a specialty in “web applications and mobile apps for data collection”, or “GIS analysis tools” or “agent-based simulations for social sciences modelling”. But almost certainly not all three at the same time.</p><p>Even so, research software development is too specific in one unhelpful sense. It could be that researchers are just looking for your team to write some software for them, hand it over, and be done. But increasingly, researchers are looking not just to be delivered some software, but for a team to host the software, run it, operate it - and/or collect and curate data to be used with the tool, for tests or otherwise. Focusing solely on research software development, as a separate activity from systems operation or data analysis and management, can be overly limiting.</p><p>Ok, so what does all of this have to do with competition?</p><p>One of my venial weaknesses is spending too much time on twitter. I’m seeing increasing concern there from research computing teams that cloud vendors or teams using cloud vendors are coming into their institutions and winning or trying to win contracts for projects that “should” have gone to the in-house teams. I’m hearing complaints that the external bids are for amounts of money 2x or more what the in-house team says they could do it for. Incredibly (and almost certainly incorrectly) I’ve even heard 10x.</p><p>Reader, as hard as it is to believe, those complaining see this as an affront<sup id=\"fnref:1\"><a class=\"footnote\" href=\"https://www.dursi.ca/feed.xml#fn:1\" rel=\"footnote\">1</a></sup>, and a threat, rather than the enormous opportunity it is.</p><p>If a contract at your institution is won - or even in serious contention - that is 2x what you estimate you could have provided the services for, that’s <strong>not</strong> evidence that the external contractor is overcharging. It’s evidence that your team is <em>undercharging</em>, that you could have proposed doing more to support that project and the researchers, and that you’re leaving money on the table. It’s also evidence that you haven’t fully convinced the relevant decision makers that you can provide that service; they don’t see it as being part of your specialty.</p><p>Clearly your institution found it worthwhile to spend or consider spending that 2x, because they understood that it was worth at least that much to them to have those services. A bid for half that amount having failed or being questioned means that they really didn’t believe the in-house team could do it as well. That’s revealed-preferences data that you can use. (And if I truly believed someone at my institution was seriously considering spending 10x (1000%!) to work with an outside company rather than work with my team, well, that would occasion some serious soul searching.)</p><p>Cloud providers and other external contractors do have advantages. They have a library of reference architectures they can deploy, so they can pitch (say) CFD solutions to the mech eng department, and bioinformatics pipeline solutions to the biology department. They can pull from a library of testimonials to demonstrate that they can do the work.</p><p>But so can you. You have access to all the literature to search for how others have deployed such solutions. You have (or should have) testimonials from the people that matter - researchers at that very institution. And you have a network of deep relationships in the institution, relationships based on collaboration on research problems. Those relationships and shared expertise and history of collaboration is something the external contractors have no chance of matching.</p><p>If you’re in danger of losing out on these sorts of competitions, it’s because you’re not communicating your specialities in a way that matters, in a way that’s convincing, to the people who could pay for your services. They can’t see how your “HPC batch services” connects with “a digital twinning platform for building simulation”. They don’t see “GIS exploration for private social sciences data” as being an obvious of your “Research Software Development” effort - where’s the data part?  If there’s a miscommunication there about what your team can provide, that’s on you and your team, not on the researchers or other decision makers.</p><p>You have specialities - if you don’t know what they are, ask the researchers who keep coming back. How do they describe what you do? What would they say your speciality is, how do they talk about you to their colleagues? What would you have to demonstrate to them to have them recommend their colleagues to you?</p><p>Similarly, you already have a million things you <em>don’t</em> do.  You won’t fix a researcher’s printer, you don’t help them do graphic design for their posters, my guess is you don’t help them set up spreadsheets in OneDrive or set up lab webpages.  So it’s not like declaring that there’s computing stuff you do and don’t help researchers with is some completely new thing, previously utterly unknown to your organization.</p><p>Once you make explicit your specialties, you can start playing to your strengths, and communicating them endlessly. You can make a point of reaching out, having your team talk at conferences in the specialties, and at departmental colloquia. You can be well-regarded enough in your institution for those specialties that external contractors pitching work within your speciality never get in the door. You can start more easily hiring people that are interested in that specialty. A specialty builds on itself, snowballs. You can start steering future work towards that specialty to build on it, and start directing work well outside the specialty to somewhere else - where it does fit inside their specialty.</p><p>Yeah, that last part is scary. Sticking to this path isn’t easy. It means turning down opportunities that aren’t in or adjacent to your specialities. Especially for new teams, eager to please, this can be scary.</p><p>But as anywhere in research, your team’s reputation is all that matters. Your team <em>has</em> a reputation, has stuff it does and doesn’t do. Did you choose it, did you shape it, or are you content to just let it happen?</p><p>Your team can be extremely strong in, specialize in, develop a reputation in, any of a number of things. But not all of the things. Being a manager or leader means choosing.</p><hr /><div class=\"footnotes\">  <ol>    <li id=\"fn:1\">      <p>And affront was taken. There were lots of dark murmurings about slick sales teams trying to fool gullible senior administrators. And, you know, I’m sure it’s comforting for the teams that might lose out on these contracts to think that the vendor mesmerized the simpleton decision makers with their entrancing slide decks, and so hoodwinked them into considering an overpriced contract. But (a) have they never <em>seen</em> a vendor pitch? and (b) it’s self-serving twaddle to imagine that just because someone higher up made a decision to work with someone else they must clearly be dumb. Dismissing out of hand the possibility that there might be valid reasons to direct work elswhere means they’re going to end up making a lot of poor and uninformed decisions. <a class=\"reversefootnote\" href=\"https://www.dursi.ca/feed.xml#fnref:1\">&#8617;</a></p>    </li>  </ol></div>",
            "url": "https://hpc.social/personal-blog/2021/to-compete-your-team-needs-a-specialty/",
            
            
            
            
            
            "date_published": "2021-09-11T01:00:00-06:00",
            "date_modified": "2021-09-11T01:00:00-06:00",
            
                "author": "Jonathan Dursi's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2021/crimson-vs-classic-1-nvme-multi-osd-analysis/",
            "title": "Crimson vs Classic 1 NVMe Multi-OSD Analysis",
            "summary": null,
            "content_text": "Spreadsheet looking at Crimson vs Classic performance when scaling multiple OSDs on one NVMe drive.  Done to simulate what we can hopefully expect from multi-reactor down the road.  Includes cycles/OP comparisons as well.",
            "content_html": "<p><a href=\"https://docs.google.com/spreadsheets/d/14HMaGxstvWSjobdyAlTdTG_yqhJh7K71Q5rSaxb1S6M/edit?usp=sharing\">Spreadsheet</a> looking at Crimson vs Classic performance when scaling multiple OSDs on one NVMe drive.  Done to simulate what we can hopefully expect from multi-reactor down the road.  Includes cycles/OP comparisons as well.</p>",
            "url": "https://hpc.social/personal-blog/2021/crimson-vs-classic-1-nvme-multi-osd-analysis/",
            
            
            
            
            
            "date_published": "2021-08-30T01:00:00-06:00",
            "date_modified": "2021-08-30T01:00:00-06:00",
            
                "author": "Mark Nelson's Blog"
            
        }
    
    ]
}
